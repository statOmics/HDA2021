<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Lieven Clement" />


<title>2. Singular Value Decomposition</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
#rmd-source-code {
  display: none;
}
</style>


<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HDDA21</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-chalkboard-teacher"></span>
     
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro.html">1. Introduction</a>
    </li>
    <li>
      <a href="svd.html">2. Singular Value Decomposition</a>
    </li>
    <li>
      <a href="svdGeometricInterpretation.html">2.3. Geometric Interpretation SVD</a>
    </li>
    <li>
      <a href="MDS_linkGramDistanceMatrix.html">2.7. Link MDS and Gram Distance Matrix</a>
    </li>
    <li>
      <a href="prediction.html">3. Prediction with High Dimensional Predictors</a>
    </li>
    <li>
      <a href="sparseSvd.html">4. Sparse Singular Value Decomposition</a>
    </li>
    <li>
      <a href="lda.html">5. Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="lsi.html">6. Large Scale Inference</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-laptop"></span>
     
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Coming soon!</li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/statOmics/HDDA21">
    <span class="fab fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://statomics.github.io/">statOmics</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">2. Singular Value Decomposition</h1>
<h4 class="author">Lieven Clement</h4>
<h4 class="date">statOmics, Ghent University (<a href="https://statomics.github.io" class="uri">https://statomics.github.io</a>)</h4>

</div>


<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">1.1</span> Motivation</h2>
<p>The SVD is one of the most well used and general purpose tools from linear algebra for data processing!</p>
<p>Methodologically</p>
<ul>
<li>Dimension reduction (e.g.¬†images, gene expression data, movie preferences)</li>
<li>Used as a first step in many data reduction and machine learning approaches</li>
<li>Taylor a coordinate system driven by the data</li>
<li>Solve system of linear equations for non-square matrices: e.g.¬†linear regression</li>
<li>Basis for principal component analysis (PCA) and multidimensional scaling (MDS).
<ul>
<li>PCA is one of the most widely used methods to study high dimensional data and to understand them in terms of their dominant patterns and correlations</li>
</ul></li>
</ul>
<p>Applications:</p>
<ul>
<li>At the heart of search engines: Google</li>
<li>Basis of many facial recognition methods: e.g.¬†Facebook</li>
<li>Recommender systems such as Amazon and Netflix</li>
<li>A standard tool for data exploration and dimension reduction in Genomics</li>
</ul>
</div>
<div id="disclaimer" class="section level2">
<h2><span class="header-section-number">1.2</span> Disclaimer</h2>
<p>When you want to run the script you will have to comment out the eval=FALSE statement in some R chunks. Because the SVD takes a while on the faces example we save the svd for later use. So you have to comment the eval=FALSE statement in this chunk when you run the script for the first time.</p>
</div>
<div id="data" class="section level2">
<h2><span class="header-section-number">1.3</span> Data</h2>
<ul>
<li>Extended Yale Face Database B</li>
<li>Cropped and aligned images of 38 individuals under 64 lighting conditions.</li>
<li>Each image is 192 pixels tall and 168 pixels wide.</li>
<li>Each of the facial images in our library will be reshaped into a large vector with 192 √ó 168 = 32 256 elements.</li>
<li>We will use the 64 images of 36 people to build our models</li>
</ul>
<pre class="r"><code>library(pixmap)
library(tidyverse)
library(gridExtra)
library(grid)
library(ggmap)
library(downloader)
library(imager)</code></pre>
<pre class="r"><code>## Download and unzip data
if(!dir.exists(&quot;raw-data&quot;)) dir.create(&quot;raw-data&quot;)
download(
  &quot;https://github.com/statOmics/HDA2020/raw/data/yalefaces_cropped.zip&quot;,
  destfile = &quot;raw-data/yalefaces_cropped.zip&quot;, mode = &quot;wb&quot;, quiet = TRUE
)
unzip (&quot;raw-data/yalefaces_cropped.zip&quot;, exdir = &quot;./raw-data&quot;)

dir &lt;- &quot;./raw-data/CroppedYale&quot;</code></pre>
<pre class="r"><code>people &lt;- list.files(dir)
people2 &lt;- sapply(people,
  function(x) list.files(
    paste0(dir,&quot;/&quot;,x),
    full.names=TRUE
    )
  )

facesList &lt;- lapply(people2, function(x) read.pnm(x))

 grid.arrange(
  grobs=lapply(facesList[1+(0:35)*64],
     function(x) getChannels(x) %&gt;%
        ggimage(.,coord_equal=TRUE)
        ),
  ncol=6)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="method" class="section level2">
<h2><span class="header-section-number">1.4</span> Method</h2>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n\times p\)</span> matrix e.g.</p>
<ul>
<li>gene expression of <span class="math inline">\(p=40 000\)</span> genes for <span class="math inline">\(n=30\)</span> subjects</li>
<li>n = 100 000 000 webpages indexed with p search terms, or</li>
<li><span class="math inline">\(n\)</span> images each stored as a <span class="math inline">\(p=32 256\)</span> vector with the intensity of each pixel</li>
</ul>
<!-- Need "emo" package for emojis, install with -->
<!-- `devtools::install_github("hadley/emo")` -->
<p><strong>Note:</strong> the emoji characters will not be visible in the PDF output.</p>
<p><span class="math display">\[X=
\left[\begin{array}{ccc}
-&amp;\mathbf{x}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{i}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{n}^T &amp;- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
üíá\\\\
üëÆ\\
\\
üë∏\\
\end{array}
\]</span></p>
<p>The data matrix <span class="math inline">\(\mathbf{X}\)</span> can be decomposed with the SVD into 3 matrices:</p>
<p><span class="math display">\[
\mathbf{X}=\mathbf{U}_{n\times n}\boldsymbol{\Delta}_{n\times p}\mathbf{V}^T_{p \times p}
\]</span></p>
<ul>
<li><p>an orthonormal matrix <span class="math inline">\(\mathbf{U}_{n\times n}\)</span> with left singular vectors: <span class="math inline">\(\mathbf{u}_j^T \mathbf{u}_k=1\)</span> if <span class="math inline">\(k=j\)</span> and <span class="math inline">\(\mathbf{u}_j^T \mathbf{u}_k=0\)</span> if <span class="math inline">\(j\neq k\)</span>, i.e. <span class="math display">\[ \mathbf{U}^T\mathbf{U}=\mathbf{I}\]</span></p></li>
<li><p>a matrix <span class="math inline">\(\boldsymbol{\Delta}_{n\times p}\)</span> with only singular values: the singular values <span class="math inline">\(\delta_i\)</span> are the only non-zero elements of the matrix and are on the diagonal element <span class="math inline">\([\boldsymbol{\Delta}]_{ii}\)</span>. They are also organised so that <span class="math inline">\(\delta_1 &gt; \delta_2 &gt; \ldots &gt; \delta_r\)</span>.</p></li>
<li><p>an orthonormal matrix <span class="math inline">\(\mathbf{V}_{p\times p}\)</span> with right singular vectors: <span class="math inline">\(\mathbf{v}_j^T \mathbf{v}_k=1\)</span> if <span class="math inline">\(k=j\)</span> and <span class="math inline">\(\mathbf{v}_j^T \mathbf{v}_k=0\)</span> if <span class="math inline">\(j\neq k\)</span> otherwise, i.e. <span class="math display">\[ \mathbf{V}^T\mathbf{V}=\mathbf{I}\]</span></p></li>
</ul>
<p>Note, that there are only <span class="math inline">\(r\)</span> non-zero singular values, with <span class="math inline">\(r\)</span> the rank of matrix <span class="math inline">\(X\)</span>: <span class="math inline">\(r \leq \text{min}(n,p)\)</span>. So we have <span class="math inline">\(k=1 \ldots r\)</span> non-zero singular values. Hence, we can also rewrite the approximation by restricting us to the rank of matrix <span class="math inline">\(\mathbf{X}\)</span>. Indeed, the n times p matrix <span class="math inline">\(\boldsymbol\Delta\)</span> only contains <span class="math inline">\(r\)</span> non-zero diagonal elements!</p>
<ul>
<li>So <span class="math display">\[
\mathbf{X}=\mathbf{U}_{n\times r}\boldsymbol{\Delta}_{r\times r}\mathbf{V}^T_{p \times r}
\]</span></li>
</ul>
<p><span class="math display">\[
\left[\begin{array}{ccc}
-&amp;\mathbf{x}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{i}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{n}^T &amp;- \\
\end{array}\right]_{n \times p}
=
\left[\begin{array}{ccc}
\mid&amp;&amp;\mid\\
\mathbf{u}_1&amp;\ldots&amp;\mathbf{u}_r\\
\mid&amp;&amp;\mid
\end{array}\right]_{n \times r}
\left[\begin{array}{ccc}
\delta_1\\
&amp;\ddots&amp;\\
&amp;&amp;\delta_r\\
\end{array}\right]_{r \times r}
\left[\begin{array}{ccc}
\mid&amp;&amp;\mid\\
\mathbf{v}_1&amp;\ldots&amp;\mathbf{v}_r\\
\mid&amp;&amp;\mid\\
\end{array}
\right]^T_{p \times r}
\]</span></p>
<p>Also note that <span class="math display">\[
\mathbf{V}^T=\left[\begin{array}{ccc}
-&amp;\mathbf{v}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{v}_{r}^T &amp;- \\
\end{array}\right]_{r \times p}
\]</span></p>
<ul>
<li><p>For high dimensional data <span class="math inline">\(p&gt;&gt;&gt;n\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\text{max}(r)=n\)</span> and</p></li>
<li><p>equivalently for multivariate data with <span class="math inline">\(n&gt;p\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\text{max}(r)=p\)</span></p></li>
</ul>
<p>We can also rewrite the decomposition using the properties of matrix multiplication</p>
<p><span class="math display">\[\begin{eqnarray}
\mathbf{X} &amp;=&amp; \delta_1\left[
\begin{array}{c}
\mid\\
\mathbf{u}_1\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&amp;
\mathbf{v}_1^T&amp;
-
\end{array}
\right]\\\quad\\\quad
\end{array}
+ \ldots +
\delta_r\left[
\begin{array}{c}
\mid\\
\mathbf{u}_r\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&amp;
\mathbf{v}_r^T&amp;
-
\end{array}
\right]\\\quad\\\quad
\end{array}\\
\mathbf{X} &amp;=&amp; \sum_{k=1}^r \delta_k\mathbf{u}_k\mathbf{v}_k^T
\end{eqnarray}\]</span></p>
<ul>
<li><p>Because both <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthonormal all their <span class="math inline">\(r\)</span> vectors are having unit length and they are thus reshaped by the singular values.</p></li>
<li><p>Hence, the singular values determine the importance of the rank one matrices <span class="math inline">\(\delta_k\mathbf{u}_k\mathbf{v}_k^T\)</span> in the reconstruction of the matrix <span class="math inline">\(\mathbf{X}\)</span> and they are ordered so that <span class="math inline">\(\delta_1 &gt; \ldots &gt; \delta_r\)</span>.</p></li>
</ul>
<p>Note, that for symmetric matrices <span class="math inline">\(\mathbf{X}\)</span> <span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\(\mathbf{U} = \mathbf{V}\)</span>.</p>
</div>
<div id="interpretation-of-singular-vectors-face-example" class="section level2">
<h2><span class="header-section-number">1.5</span> Interpretation of singular vectors: face example</h2>
<div id="convert-images-to-vectors" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Convert images to vectors</h3>
<ol style="list-style-type: decimal">
<li>Convert images to vectors and store them as a matrix
<ul>
<li>We use an <code>sapply</code> loop to loop over all faces</li>
<li>We extract the grey intensities from the pictures</li>
<li>We convert the matrix in a long skinny vector (<code>c</code>)</li>
<li>We transpose the resulting matrix from sapply</li>
</ul></li>
</ol>
<pre class="r"><code>allFacesMx &lt;- sapply(facesList,
                     function(x)
                        getChannels(x) %&gt;% c
                     ) %&gt;% t
dim(allFacesMx)</code></pre>
<pre><code>## [1]  2432 32256</code></pre>
<p>Save memory by removing facesList object</p>
<pre><code>rm(facesList)
gc()</code></pre>
<p>Hence we obtain a matrix for n = 2432 images with p = 32256 intensities for each pixel of an image.</p>
<p>Before we do the svd we typically center the data by substracting the average of the columns, i.e.¬†the average face.</p>
<p>We will only work with the first 36 people: <span class="math inline">\(n = 36 \times 64 = 2304\)</span> pictures.</p>
<pre class="r"><code>allFacesCenteredMx &lt;- allFacesMx[1:(36*64),]
meanFace &lt;- colMeans(allFacesCenteredMx)

allFacesMxCentered &lt;- allFacesCenteredMx -
  matrix(1, nrow=nrow(allFacesCenteredMx), ncol=1) %*% matrix(meanFace,nrow=1)</code></pre>
</div>
<div id="visualisation-of-mean-image" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Visualisation of mean image</h3>
<pre class="r"><code>plotFaceVector &lt;- function(faceVector,nrow=192,ncol=168) {
  matrix(faceVector,nrow=nrow,ncol=ncol) %&gt;%
  ggimage()
}

meanFace %&gt;%
  plotFaceVector</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="svd" class="section level3">
<h3><span class="header-section-number">1.5.3</span> SVD</h3>
<div id="perform-svd-in-r" class="section level4">
<h4><span class="header-section-number">1.5.3.1</span> Perform SVD in R</h4>
<ol style="list-style-type: decimal">
<li>We adopt svd on the centered matrix</li>
<li>We cache the result because the calculation takes 10 minutes.</li>
</ol>
<pre class="r"><code>faceSvd &lt;- svd(allFacesMxCentered)</code></pre>
<!-- ```{r, eval=FALSE} -->
<!-- ## Run this code manually to store the SVD for later re-use -->
<!-- saveRDS(faceSvd, file = "faceSvd.rds") -->
<!-- ``` -->
<!-- ```{r, eval=FALSE} -->
<!-- ## Run this code manually to reload the SVD result -->
<!-- faceSvd <- readRDS("faceSvd.rds") -->
<!-- ``` -->
</div>
<div id="svd-1" class="section level4">
<h4><span class="header-section-number">1.5.3.2</span> SVD</h4>
<p>Dimensions of <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{V}\)</span>?</p>
<pre class="r"><code>n &lt;- nrow(allFacesCenteredMx)
p &lt;- ncol(allFacesCenteredMx)
dim(faceSvd$u)</code></pre>
<pre><code>## [1] 2304 2304</code></pre>
<pre class="r"><code>dim(faceSvd$v)</code></pre>
<pre><code>## [1] 32256  2304</code></pre>
<p>Indeed, for the face example <span class="math inline">\(n&lt;p\)</span> so <span class="math inline">\(r=n\)</span></p>
<p>Check orthogonality?</p>
<p>We do not do it for all vectors because it takes too long. First left eigen vector and second left eigenvector. Happens in <span class="math inline">\(\mathbf{U}^T\mathbf{U}\)</span></p>
<pre class="r"><code>t(faceSvd$u[,1])%*%faceSvd$u[,1]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<pre class="r"><code>t(faceSvd$u[,1])%*%faceSvd$u[,2]</code></pre>
<pre><code>##               [,1]
## [1,] -3.794708e-19</code></pre>
<pre class="r"><code>t(faceSvd$u[,2])%*%faceSvd$u[,2]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>So we see that the left eigenvectors are orthonormal.</p>
<p>We check if it also holds for the rows i.e.¬†<span class="math inline">\(\mathbf{U}\mathbf{U}^T\)</span></p>
<pre class="r"><code>t(faceSvd$u[1,])%*%faceSvd$u[1,]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<pre class="r"><code>t(faceSvd$u[2,])%*%faceSvd$u[1,]</code></pre>
<pre><code>##               [,1]
## [1,] -1.691355e-16</code></pre>
<pre class="r"><code>t(faceSvd$u[2,])%*%faceSvd$u[2,]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>We also see that the rows of <span class="math inline">\(\mathbf{U}\)</span> are orthonormal.</p>
<pre class="r"><code>t(faceSvd$v[,1])%*%faceSvd$v[,1]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<pre class="r"><code>t(faceSvd$v[,1])%*%faceSvd$v[,2]</code></pre>
<pre><code>##               [,1]
## [1,] -9.397551e-16</code></pre>
<pre class="r"><code>t(faceSvd$v[,2])%*%faceSvd$v[,2]</code></pre>
<pre><code>##      [,1]
## [1,]    1</code></pre>
<p>So we see that the right eigenvectors are orthonormal.</p>
<pre class="r"><code>t(faceSvd$v[1,])%*%faceSvd$v[1,]</code></pre>
<pre><code>##           [,1]
## [1,] 0.6181993</code></pre>
<pre class="r"><code>t(faceSvd$v[1,])%*%faceSvd$v[2,]</code></pre>
<pre><code>##            [,1]
## [1,] 0.07690404</code></pre>
<pre class="r"><code>t(faceSvd$v[2,])%*%faceSvd$v[2,]</code></pre>
<pre><code>##           [,1]
## [1,] 0.1017995</code></pre>
<p>This, however does not hold for the rows of <span class="math inline">\(\mathbf{V}\)</span>. This is because the matrix <span class="math inline">\(\mathbf{V}\)</span> no longer is a square matrix! <span class="math inline">\(r=n\)</span> and <span class="math inline">\(r&lt;p\)</span>!</p>
</div>
<div id="visualize-right-eigenvectors-mathbfv" class="section level4">
<h4><span class="header-section-number">1.5.3.3</span> Visualize right eigenvectors <span class="math inline">\(\mathbf{V}\)</span></h4>
<pre class="r"><code>grid.arrange(
 grobs=apply(
   faceSvd$v[,1:36],
   2,
   plotFaceVector
   )
 )</code></pre>
<pre><code>## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...
## rescaling mat to [0,1]...</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<ul>
<li><p>Hence, the right singular vectors (in <span class="math inline">\(\mathbf{V}\)</span> of <span class="math inline">\(\mathbf{X}=\mathbf{U}\boldsymbol{\Delta}\mathbf{V}\)</span>) are also faces and we can thus reconstruct the original faces by linear combinations of the eigen faces.</p></li>
<li><p>The first eigen faces are most important to capture overall patterns in the matrix.</p></li>
<li><p>Here it are mainly characteristics and shadows that are important for all faces.</p></li>
<li><p>From eigen face 5 onwards we start to see specific features.</p></li>
<li><p>In this case: <span class="math inline">\(n &lt; p\)</span>, so <span class="math inline">\(r = n\)</span>.</p></li>
</ul>
<p><span class="math display">\[
\mathbf{X}_{n\times p}=\mathbf{U}_{n \times n}\boldsymbol{\Delta}_{n\times n}\mathbf{V}_{p\times n}^T
\]</span></p>
<p><span class="math display">\[
\begin{array}{ccccc}
\left[\begin{array}{ccc}
-&amp;\mathbf{x}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{i}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{n}^T &amp;- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
üíá\\\\
üëÆ\\
\\
üë∏\\
\end{array}
&amp;=&amp;
\begin{array}{c}
\quad\\
\left[\begin{array}{ccc}
\mid&amp;&amp;\mid\\
\mathbf{u}_1&amp;\ldots&amp;\mathbf{u}_n\\
\mid&amp;&amp;\mid
\end{array}\right]_{n \times n}\\
\quad \\
\end{array}
\begin{array}{c}
\quad\\
\left[\begin{array}{ccc}
\delta_1\\
&amp;\ddots&amp;\\
&amp;&amp;\delta_n\\
\end{array}\right]_{n \times n}\\
\quad\\
\end{array}
\begin{array}{c}
\quad
\left[\begin{array}{ccc}
\mid&amp;&amp;\mid\\
\mathbf{v}_1&amp;\ldots&amp;\mathbf{v}_n\\
\mid&amp;&amp;\mid\\
\end{array}
\right]^T_{p \times n}\\
\begin{array}{ccc}
üò®&amp;\quad&amp;üòê
\end{array}
\end{array}
\end{array}
\]</span></p>
<ul>
<li>Or upon transposing the matrix <span class="math inline">\(\mathbf{V}\)</span></li>
</ul>
<p><span class="math display">\[
\begin{array}{ccccc}
\left[\begin{array}{ccc}
-&amp;\mathbf{x}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{i}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{n}^T &amp;- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
üíá\\\\
üëÆ\\
\\
üë∏\\
\end{array}
&amp;=&amp;
\left[\begin{array}{ccc}
\mid&amp;&amp;\mid\\
\mathbf{u}_1&amp;\ldots&amp;\mathbf{u}_n\\
\mid&amp;&amp;\mid
\end{array}\right]_{n \times n}
\left[\begin{array}{ccc}
\delta_1\\
&amp;\ddots&amp;\\
&amp;&amp;\delta_r\\
\end{array}\right]_{n \times n}
\left[\begin{array}{ccc}
-&amp;\mathbf{v}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{v}_{p}^T &amp;- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
üò®\\
\\
üòê
\end{array}
\end{array}
\]</span></p>
</div>
<div id="reconstruction-of-faces-via-linear-combination-of-eigen-faces." class="section level4">
<h4><span class="header-section-number">1.5.3.4</span> Reconstruction of faces via linear combination of eigen faces.</h4>
<p>In left singular vectors <span class="math inline">\(u_{ij}\)</span> we quantify the contribution of the <span class="math inline">\(j^\text{th}\)</span> eigenface in the reconstruction of face <span class="math inline">\(i\)</span> and we rescale the importance of each eigen face by its corresponding eigen value <span class="math inline">\(\delta_j\)</span>.</p>
<p><span class="math display">\[
\left[\begin{array}{ccc}
-&amp;\mathbf{x}_{1}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{i}^T &amp;- \\
\vdots&amp;\vdots&amp;\vdots\\
-&amp;\mathbf{x}_{n}^T &amp;- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
üíá\\\\
üëÆ\\
\\
üë∏\\
\end{array} =
\delta_1\left[
\begin{array}{c}
\mid\\
\mathbf{u}_1\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&amp;
\mathbf{v}_1^T&amp;
-
\end{array}
\right]\\\quad\\\quad
\end{array}
\begin{array}{c}
üò®
\\\quad\\\quad
\end{array}
+ \ldots +
\delta_r\left[
\begin{array}{c}
\mid\\
\mathbf{u}_r\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&amp;
\mathbf{v}_r^T&amp;
-
\end{array}
\right]\\\quad\\\quad
\end{array}
\begin{array}{c}
üòê
\\\quad\\\quad
\end{array}
\]</span></p>
<p>If we truncate the eigen faces say at <span class="math inline">\(k&lt;r\)</span> we can approximate faces using a limited number of eigen faces!</p>
<pre class="r"><code>approximateFace &lt;- function(meanFace,faceSvd,k){
  reconstruct &lt;- (meanFace + faceSvd$u[1,1:k] %*%
    diag(faceSvd$d[1:k]) %*%
  t(faceSvd$v[,1:k]) %&gt;%
  c)
}

approxHlp &lt;- sapply(
      c(25,100,500),
      approximateFace,
      meanFace=meanFace,
      faceSvd=faceSvd)

grid.arrange(
 grobs=apply(
   cbind(
     approxHlp,
     allFacesMxCentered[1,]+meanFace
   ),
   2,
   plotFaceVector
   )
 )</code></pre>
<div class="figure">
<img src="svd_files/figure-html/unnamed-chunk-11-1.png" alt="approximation with 25 (top left),  100 (top right) and 500 (bottom left) eigenfaces and original face (bottom right, or with all eigenfaces)" width="672" />
<p class="caption">
approximation with 25 (top left), 100 (top right) and 500 (bottom left) eigenfaces and original face (bottom right, or with all eigenfaces)
</p>
</div>
</div>
</div>
</div>
</div>
<div id="svd-as-a-matrix-approximation-method" class="section level1">
<h1><span class="header-section-number">2</span> SVD as a Matrix Approximation Method</h1>
<ul>
<li><p>We have seen that we can use the truncted SVD to approximate matrix <span class="math inline">\(\mathbf{X}\)</span> by <span class="math inline">\(\tilde{\mathbf{X}}\)</span>, with <span class="math inline">\(k&lt;r\)</span> and <span class="math display">\[
\tilde{\mathbf{X}}=\mathbf{U}_{n\times k}\boldsymbol{\Delta}_{k\times k}\mathbf{V}_{p \times k}^T
\]</span></p></li>
<li><p>It can be shown that <strong>SVD: optimal approximation</strong></p>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n\times p\)</span> matrix of rank <span class="math inline">\(r\leq \min(n,p)\)</span>, and let <span class="math inline">\(\mathbf{A}\)</span> denote an <span class="math inline">\(n \times p\)</span> matrix of rank <span class="math inline">\(k\leq r\)</span>, with elements denoted by <span class="math inline">\(a_{ij}\)</span>.</p></li>
<li><p>The matrix <span class="math inline">\(\mathbf{A}\)</span> of rank <span class="math inline">\(k\leq r\)</span> that minimises the Frobenius norm <span class="math display">\[
\vert\vert\mathbf{X}-\mathbf{A}\vert\vert^2_\text{fr}=\sum_{i=1}^n\sum_{j=1}^p (x_{ij}-a_{ij})^2
\]</span> is given by the truncated SVD <span class="math display">\[
 \mathbf{X}_k = \sum_{j=1}^k \delta_j \mathbf{u}_j\mathbf{v}_j^T.
\]</span></p></li>
<li><p>The truncated SVD has <span class="math inline">\(k &lt; r\)</span> terms. Hence, generally <span class="math inline">\(\mathbf{X}_k\)</span> does not coincide with <span class="math inline">\(\mathbf{X}\)</span>. It is considered as an approximation.</p></li>
<li><p>Note, that the truncated SVD thus approximates the matrix by minimising a kind of sum of least squared errors between the elements of matrix <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{A}\)</span> and that</p></li>
<li><p>the truncated SVD <span class="math inline">\(\mathbf{X}_k\)</span> is the best rank-k approximation of <span class="math inline">\(\mathbf{X}\)</span> in terms of this Frobenius norm.</p></li>
<li><p>Also, note that upon truncation <span class="math display">\[\mathbf{V}^T_{p\times k} \mathbf{V}_{p\times k} = \mathbf{I}_{k\times k}\]</span> <span class="math display">\[\mathbf{U}^T_{n\times k} \mathbf{U}_{n\times k} = \mathbf{I}_{k\times k}\]</span></p></li>
<li><p>But, that <span class="math display">\[\mathbf{V}_{p\times k} \mathbf{V}_{p\times k}^T \neq \mathbf{I}_{p\times p}!!!\]</span> <span class="math display">\[\mathbf{U}_{n\times k} \mathbf{U}_{n\times k}^T \neq \mathbf{I}_{n\times n}!!!\]</span></p></li>
</ul></li>
</ul>
<hr />
<p>Some <strong>informal statement</strong> about the truncated SVD <span class="math display">\[
     \mathbf{X}_k = \sum_{j=1}^k \delta_j \mathbf{u}_j\mathbf{v}_j^T.
  \]</span></p>
<ul>
<li><p>It can be considered as a weighted sum of matrices <span class="math inline">\(\mathbf{u}_j\mathbf{v}_j^T\)</span>, with weights <span class="math inline">\(\delta_j\)</span>.</p></li>
<li><p>The terms are ordered with decreasing weights <span class="math inline">\(\delta_1\geq \delta_2 \geq \cdots \geq \delta_k &gt;0\)</span>.</p></li>
<li><p>The matrices <span class="math inline">\(\mathbf{u}_j\mathbf{v}_j^T\)</span> are of equal ‚Äúmagnitude‚Äù (constructed from normalised vectors).</p></li>
<li><p>Truncation at <span class="math inline">\(k\)</span> results in <span class="math inline">\(k\)</span> <span class="math inline">\(\delta_j\)</span>‚Äôs, <span class="math inline">\(k\times n\)</span> elements in the <span class="math inline">\(\mathbf{u}_j\)</span> and <span class="math inline">\(k \times p\)</span> elements in the <span class="math inline">\(\mathbf{v}_j\)</span>. Hence a total of <span class="math inline">\(k+kn+kp=k(1+n+p)\)</span> elements (usually much smaller than <span class="math inline">\(np\)</span>). (Note that restrictions apply to <span class="math inline">\(\mathbf{u}_j\)</span> and <span class="math inline">\(\mathbf{v}_j\)</span>; hence even less independent elements).</p></li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> <strong>data compression</strong></p>
<div id="example-1-image-compression" class="section level2">
<h2><span class="header-section-number">2.1</span> Example 1: Image compression</h2>
<div id="painting-mondriaan-composition_no.iii-with-red-blue-yellow-and-black-1929." class="section level3">
<h3><span class="header-section-number">2.1.1</span> Painting Mondriaan: Composition_No.III with red, blue, yellow and black (1929).</h3>
<ul>
<li>Have a look at this painting of Mondriaan (1872 ‚Äì 1944), here shown in black-and-white.</li>
</ul>
<div id="load-the-original-painting" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Load the original painting</h4>
<ol style="list-style-type: decimal">
<li>fetch image from the web</li>
<li>convert into greyscale</li>
<li>plot</li>
<li>save as Matrix</li>
</ol>
<pre class="r"><code>mondriaan &lt;- load.image(&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Piet_Mondrian_-_Composition_No._III%2C_with_red%2C_blue%2C_yellow_and_black%2C_1929.jpg/1920px-Piet_Mondrian_-_Composition_No._III%2C_with_red%2C_blue%2C_yellow_and_black%2C_1929.jpg&quot;)
mondriaan &lt;- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>X &lt;- matrix(as.data.frame(mondriaan)[,3],nrow=nrow(mondriaan),ncol=ncol(mondriaan))</code></pre>
<ul>
<li><p>This picture can be represented as a <span class="math inline">\(1920 \times 1913`\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> with gray scale intensities <span class="math inline">\(\in [0,1]\)</span>. (<span class="math inline">\(\approx 4\times 10^6\)</span> data entries)</p></li>
<li><p>We will here not transform the image in a vector, but will look at the performance of the SVD to compress this image. The SVD can be applied to any matrix!</p></li>
</ul>
</div>
<div id="singular-values" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Singular values</h4>
<pre class="r"><code>monSvd &lt;- svd(X)

p1 &lt;- data.frame(x=1:length(monSvd$d),y=monSvd$d) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

p2 &lt;- data.frame(x=1:10,y=monSvd$d[1:10]) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

grid.arrange(p1,p2,nrow=1)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<ul>
<li>The singular values decay very quickly!</li>
</ul>
</div>
<div id="data-compression" class="section level4">
<h4><span class="header-section-number">2.1.1.3</span> Data compression</h4>
<ul>
<li>We make the plot for a reconstruction with 1 singular vector. This leads to a data compression of <span class="math inline">\(1-\frac{(1+1920+1913)}{1920\times 1913}\)</span> = 99.9%. We only use 1 left singular vector (1920), 1 eigen value, 1 right singular vector (1913).</li>
</ul>
<pre class="r"><code>k &lt;- 1
approxMon &lt;- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])

approxMon[approxMon &lt; 0] &lt;- 0
approxMon[approxMon &gt; 1] &lt;- 1

as.cimg(approxMon) %&gt;%
  plot(.,main=paste0(&quot;Approximation with &quot;,k,&quot; singular vectors&quot;),axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li>We make the plot for a reconstruction with 2 singular vector. This leads to a data compression of <span class="math inline">\(1-\frac{2\times (1+1920+1913)}{1920\times 1913}\)</span> = 99.8%. We only use 2 left singular vectors (2 <span class="math inline">\(\times\)</span> 1920), 2 singular values, 2 right singular vectors (2 <span class="math inline">\(\times\)</span> 1913).</li>
</ul>
<pre class="r"><code>k &lt;- 2
approxMon &lt;- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])

approxMon[approxMon &lt; 0] &lt;- 0
approxMon[approxMon &gt; 1] &lt;- 1

as.cimg(approxMon) %&gt;%
  plot(.,main=paste0(&quot;Approximation with &quot;,k,&quot; singular vectors&quot;),axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1:8))
{
approxMon &lt;- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon &lt; 0] &lt;- 0
approxMon[approxMon &gt; 1] &lt;- 1


approxMon %&gt;%
  as.cimg %&gt;%
  plot(.,main=paste0(k,&quot; singular vectors&quot;),axes=FALSE)
}
plot(as.cimg(X),main=paste0(&quot;Original image&quot;),axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="more-complex-painting-composition-a-piet-mondriaan" class="section level3">
<h3><span class="header-section-number">2.1.2</span> More complex painting: Composition A, Piet Mondriaan</h3>
<div id="load-the-original-painting-1" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> Load the original painting</h4>
<pre class="r"><code>mondriaan &lt;- load.image(&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Composition_A_by_Piet_Mondrian_Galleria_Nazionale_d%27Arte_Moderna_e_Contemporanea.jpg/1920px-Composition_A_by_Piet_Mondrian_Galleria_Nazionale_d%27Arte_Moderna_e_Contemporanea.jpg&quot;)
mondriaan &lt;- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>X &lt;- matrix(as.data.frame(mondriaan)[,3],nrow=dim(mondriaan)[1],ncol=dim(mondriaan)[2])</code></pre>
</div>
<div id="singular-values-1" class="section level4">
<h4><span class="header-section-number">2.1.2.2</span> Singular values</h4>
<pre class="r"><code>monSvd &lt;- svd(X)</code></pre>
<pre class="r"><code>p1 &lt;- data.frame(x=1:length(monSvd$d),y=monSvd$d) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

p2 &lt;- data.frame(x=1:10,y=monSvd$d[1:10]) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

grid.arrange(p1,p2,nrow=1)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<ul>
<li>The singular values decay a bit slower. The painting is a bit more complex. More lines and colors.</li>
</ul>
</div>
<div id="evaluate-data-compression" class="section level4">
<h4><span class="header-section-number">2.1.2.3</span> Evaluate data compression</h4>
<pre class="r"><code>par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1,seq(3,21,3)))
{
approxMon &lt;- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon &lt; 0] &lt;- 0
approxMon[approxMon &gt; 1] &lt;- 1


approxMon %&gt;%
  as.cimg %&gt;%
  plot(.,main=paste0(k,&quot; singular vectors&quot;),axes=FALSE)
}
plot(as.cimg(X),main=paste0(&quot;Original image&quot;),axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>
<div id="self-portret-piet-mondriaan" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Self portret Piet Mondriaan</h3>
<div id="load-the-painting" class="section level4">
<h4><span class="header-section-number">2.1.3.1</span> Load the painting</h4>
<pre class="r"><code>mondriaan &lt;- load.image(&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Mondrian_Zelfportret.jpg/1920px-Mondrian_Zelfportret.jpg&quot;)
mondriaan &lt;- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>X &lt;- matrix(as.data.frame(mondriaan)[,3],nrow=dim(mondriaan)[1],ncol=dim(mondriaan)[2])</code></pre>
</div>
<div id="singular-values-2" class="section level4">
<h4><span class="header-section-number">2.1.3.2</span> Singular values</h4>
<pre class="r"><code>monSvd &lt;- svd(X)</code></pre>
<pre class="r"><code>p1 &lt;- data.frame(x=1:length(monSvd$d),y=monSvd$d) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

p2 &lt;- data.frame(x=1:10,y=monSvd$d[1:10]) %&gt;%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab(&quot;k&quot;) +
  ylab(&quot;singular value&quot;)

grid.arrange(p1,p2,nrow=1)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<ul>
<li>The singular values decay much slower. The painting is more complex.</li>
</ul>
</div>
<div id="evaluate-compression" class="section level4">
<h4><span class="header-section-number">2.1.3.3</span> Evaluate compression</h4>
<pre class="r"><code>par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1,5,10,20,30,40,50,100))
{
approxMon &lt;- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon &lt; 0] &lt;- 0
approxMon[approxMon &gt; 1] &lt;- 1


approxMon %&gt;%
  as.cimg %&gt;%
  plot(.,main=paste0(k,&quot; singular vectors&quot;),axes=FALSE)
}
plot(as.cimg(X),main=paste0(&quot;Original image&quot;),axes=FALSE)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Here we need at least 40 singular vector. This leads to a data compression of <span class="math inline">\(1-\frac{40\times (1+1920+2494)}{1920 \times 2494}\)</span> = 96.3%. We only use 40 left singular vectors (<span class="math inline">\(40 \times 1920\)</span>), 40 singular values, 40 right eigens vector (<span class="math inline">\(40\times 2494\)</span>).</p>
</div>
</div>
</div>
</div>
<div id="geometric-interpretation" class="section level1">
<h1><span class="header-section-number">3</span> Geometric interpretation</h1>
<p>Write the <strong>truncated SVD</strong> as <span class="math display">\[
  \mathbf{X}_k = \mathbf{U}_k \boldsymbol{\Delta}_k \mathbf{V}_k^T = \mathbf{Z}_k \mathbf{V}_k^T
\]</span> with <span class="math display">\[
  \mathbf{Z}_k = \mathbf{U}_k \boldsymbol{\Delta}_k
\]</span> an <span class="math inline">\(n \times k\)</span> matrix.</p>
<p>Each of the <span class="math inline">\(n\)</span> rows of <span class="math inline">\(\mathbf{Z}_k\)</span>, say <span class="math inline">\(\mathbf{z}^T_{k,i}\)</span>, represents a point in a <span class="math inline">\(k\)</span>-dimensional space.</p>
<p>Because of the orthonormality of the singular vectors, we also have <span class="math display">\[\begin{eqnarray*}
  \mathbf{X}_k\mathbf{V}_k &amp;=&amp; \mathbf{Z}_k \mathbf{V}_k^T\mathbf{V}_k \\
  \mathbf{X}_k\mathbf{V}_k &amp;=&amp; \mathbf{Z}_k.
\end{eqnarray*}\]</span></p>
<p>Thus the matrix <span class="math inline">\(\mathbf{V}_k\)</span> is a <strong>transformation matrix</strong> that may be used to transform <span class="math inline">\(\mathbf{X}_k\)</span> into <span class="math inline">\(\mathbf{Z}_k\)</span>, and <span class="math inline">\(\mathbf{Z}_k\)</span> into <span class="math inline">\(\mathbf{X}_k\)</span>.</p>
<hr />
<p>Note that</p>
<ul>
<li><p>The matrix <span class="math inline">\(\mathbf{V}_k\)</span> transforms the <span class="math inline">\(p\)</span>-dimensional <span class="math inline">\(\mathbf{X}_k\)</span> into the <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(\mathbf{Z}_k\)</span>: <span class="math inline">\(\mathbf{Z}_k = \mathbf{X}_k\mathbf{V}_k\)</span>. Note, however, that the matrix <span class="math inline">\(\mathbf{X}_k\)</span> must not necessarily be used for this transformation, because the SVD of the original matrix <span class="math inline">\(\mathbf{X}\)</span> also gives directly <span class="math inline">\(\mathbf{Z}_k = \mathbf{U}_k \boldsymbol{\Delta}_k\)</span>.</p></li>
<li><p>The inverse transformation from the <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(\mathbf{Z}_k\)</span> to the <span class="math inline">\(p\)</span>-dimensional <span class="math inline">\(\mathbf{X}_k\)</span> is given by the transpose of <span class="math inline">\(\mathbf{V}_k\)</span>: <span class="math inline">\(\mathbf{Z}_k \mathbf{V}_k^T=\mathbf{X}_k\)</span>. Often inverse transformations are given by the inverse of a matrix, but thanks to the orthonormality of the columns of <span class="math inline">\(\mathbf{V}_k\)</span>, we get <span class="math inline">\(\mathbf{V}_k^T\mathbf{V}_k=\mathbf{I}\)</span>, and thus <span class="math inline">\(\mathbf{V}_k^T\)</span> acts as an inverse.</p></li>
<li><p>The transformation from the <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(\mathbf{Z}_k\)</span> to the <span class="math inline">\(p\)</span>-dimensional <span class="math inline">\(\mathbf{X}_k\)</span> is transforming points from a low dimensional space (<span class="math inline">\(k\)</span>) to a high dimensional space (<span class="math inline">\(p\)</span>). You may not interpret this as if this transformation adds information; the transformed points in <span class="math inline">\(\mathbf{X}_k\)</span> still live in a <span class="math inline">\(k\)</span>-dimensional subspace of the larger <span class="math inline">\(p\)</span>-dimensional space; the matrix <span class="math inline">\(\mathbf{X}_k\)</span> is only of rank <span class="math inline">\(k\)</span> and thus contains less information than the original data matrix <span class="math inline">\(\mathbf{X}\)</span> (if rank(<span class="math inline">\(\mathbf{X}\)</span>)<span class="math inline">\(=r&gt;k\)</span>).</p></li>
</ul>
<hr />
<p>More importantly, it can be shown that (thanks to orthonormality of <span class="math inline">\(\mathbf{V}\)</span>) <span class="math display">\[
     \mathbf{X}\mathbf{V}_k = \mathbf{Z}_k.
  \]</span> This follows from (w.l.g. rank(<span class="math inline">\(\mathbf{X}\)</span>)=<span class="math inline">\(r\)</span>) <span class="math display">\[\begin{eqnarray*}
    \mathbf{X}\mathbf{V}_k
       &amp;=&amp; \mathbf{UDV}^T\mathbf{V}_k = \mathbf{UD}\begin{pmatrix}
                \mathbf{v}_1^T \\
            \vdots \\
            \mathbf{v}_r^T
               \end{pmatrix}
               \begin{pmatrix}
                 \mathbf{v}_1 \ldots \mathbf{v}_k
               \end{pmatrix} \\
       &amp;=&amp; \mathbf{UDV}^T\mathbf{V}_k = \mathbf{UD}\begin{pmatrix}
                1 &amp; 0 &amp; \ldots &amp; 0 \\
                0 &amp; 1 &amp; \ldots &amp; 0 \\
                \vdots &amp; \vdots &amp; \ddots &amp; 0 \\
                0 &amp; 0 &amp; \ldots &amp; 1 \\
                0 &amp; 0 &amp; \ldots &amp; 0 \\
                \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
                0 &amp; 0 &amp; \ldots &amp; 0
               \end{pmatrix} \
               = \mathbf{U}_k\boldsymbol{\Delta}_k = \mathbf{Z}_k
  \end{eqnarray*}\]</span></p>
<p>The <span class="math inline">\(p \times k\)</span> matrix <span class="math inline">\(\mathbf{V}_k\)</span> acts as a transformation matrix: transforming <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span> dimensional space to <span class="math inline">\(n\)</span> points in a <span class="math inline">\(k\)</span> dimensional space.</p>
<hr />
<p>We take a closer look at <span class="math display">\[
  \mathbf{Z}_k = \mathbf{X}\mathbf{V}_k = \begin{pmatrix}
   \mathbf{x}_1^T \\
   \vdots \\
   \mathbf{x}_n^T
  \end{pmatrix} \begin{pmatrix}
   \mathbf{v}_1 \ldots \mathbf{v}_k
  \end{pmatrix}.
\]</span> The <span class="math inline">\(i\)</span>th row (observation) in <span class="math inline">\(\mathbf{Z}_k\)</span> equals <span class="math display">\[
 \mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k = \left(\mathbf{x}_i^T \mathbf{v}_1 , \mathbf{x}_i^T \mathbf{v}_2 , \ldots , \mathbf{x}_i^T \mathbf{v}_k\right).
\]</span></p>
<p>Hence, <span class="math inline">\(\mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{x}_i\)</span> onto the <span class="math inline">\(k\)</span>-dimensional subspace spanned by the columns of <span class="math inline">\(\mathbf{V}_k\)</span>.</p>
<p>SVD transforms data set to lower dimensional data set: The SVD thus gives a transformation of the <span class="math inline">\(p\)</span> dimensional data to <span class="math inline">\(k\leq r\)</span> dimensional data: <span class="math display">\[
  \mathbf{Z}_k = \mathbf{X}\mathbf{V}_k.
\]</span> This is essentially a dimension reduction.</p>
<hr />
<p>Note that,</p>
<ul>
<li><p>The transformation from <span class="math inline">\(p\)</span>-dimensional <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(\mathbf{Z}_k\)</span> is important. It shows that the <span class="math inline">\(n\)</span> points in the rows of <span class="math inline">\(\mathbf{Z}_k\)</span> are the result of projecting the <span class="math inline">\(n\)</span> points in <span class="math inline">\(\mathbf{X}\)</span> onto the columns of <span class="math inline">\(\mathbf{V}_k\)</span> (i.e.¬†the first <span class="math inline">\(k\)</span> singular vectors of <span class="math inline">\(\mathbf{X}\)</span>). We say that the space of <span class="math inline">\(\mathbf{Z}_k\)</span> is spanned by the column of <span class="math inline">\(\mathbf{V}_k\)</span>.</p></li>
<li><p>The points (rows) in <span class="math inline">\(\mathbf{X}\)</span> live in a <span class="math inline">\(p\)</span>-dimensional space (or rank(<span class="math inline">\(\mathbf{X}\)</span>)<span class="math inline">\(=r\)</span> if <span class="math inline">\(r&lt;p\)</span>) and they are thus projected onto a lower dimensional space. This is in contrast to the projection <span class="math inline">\(\mathbf{X}_k\mathbf{V}_k=\mathbf{Z}_k\)</span>, because the points in <span class="math inline">\(\mathbf{X}_k\)</span> live in a <span class="math inline">\(k\)</span>-dimensional subspace of <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p>Note that with <span class="math inline">\(k&lt;r\)</span> there is no unique transformation to transform <span class="math inline">\(\mathbf{Z}_k\)</span> back to <span class="math inline">\(\mathbf{X}\)</span>. On the previous slide we only established the transformation <span class="math inline">\(\mathbf{Z}_k \mathbf{V}_k^T=\mathbf{X}_k\)</span>. Indeed, starting from <span class="math inline">\(\mathbf{X}\mathbf{V}_k = \mathbf{Z}_k\)</span>, and right-multiplying with <span class="math inline">\(\mathbf{V}_k^T\)</span> does not give the backtransformation, because <span class="math inline">\(\mathbf{V}_k\mathbf{V}_k^T\)</span> is not the identity matrix.</p></li>
</ul>
</div>
<div id="interpretation-of-svd-in-terms-of-correlation-matrices" class="section level1">
<h1><span class="header-section-number">4</span> Interpretation of SVD in terms of correlation matrices</h1>
<p>For a matrix <span class="math inline">\(\mathbf X\)</span> the sample variance covariance matrix estimator is <span class="math inline">\(p\times p\)</span> matrix</p>
<p><span class="math display">\[\begin{eqnarray}
\mathbf{S}&amp;=&amp;\frac{1}{N-1}(\mathbf{X}-\bar{\mathbf{X}})^T (\mathbf{X}-\bar{\mathbf{X}})\\
&amp;=&amp;\frac{1}{N-1}\left[\mathbf{X}^T\mathbf{X} - \bar{\mathbf{X}}^T\bar{\mathbf{X}}\right]
\end{eqnarray}\]</span></p>
<p>So <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> defines up to a constant the variance covariance matrix of <span class="math inline">\(\mathbf{X}\)</span>! When the matrix is column centered <span class="math inline">\(\mathbf{S}=\frac{1}{n-1}\mathbf{X}^T\mathbf{X}\)</span>.</p>
<p>The same holds for the rows of <span class="math inline">\(\mathbf{X}\)</span>! The covariance between the subjects can be estimated as <span class="math display">\[\mathbf{S}=\frac{1}{p-1}\mathbf{X}\mathbf{X}^T\]</span> upon row centering.</p>
<p>Note, that <span class="math display">\[\begin{eqnarray}
\mathbf{X}^T\mathbf{X} &amp;=&amp; \mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T \\
&amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^2 \mathbf{V}^T
\end{eqnarray}\]</span></p>
<p>If we rewrite the expression <span class="math display">\[\begin{eqnarray}
\mathbf{X}^T\mathbf{X}\mathbf{V} &amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^2 \mathbf{V}^T\mathbf{V}\\
&amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^2
\end{eqnarray}\]</span></p>
<p>So, if the data are centered, the SVD can be used to perform a spectral decomposition of the sample covariance matrix where the right singular vectors correspond to the eigen vectors of the covariance matrix and the eigenvalues are the squared singular values!</p>
<p>Similarly the left singular values can be used to estimate the covariance matrix of the rows of <span class="math inline">\(\mathbf{X}\)</span>. So in our notation covariance between subjects.</p>
<p><span class="math display">\[\begin{eqnarray}
\mathbf{X}\mathbf{X}^T &amp;=&amp;\mathbf{U}\boldsymbol{\Delta}^2 \mathbf{U}^T
\end{eqnarray}\]</span></p>
<p>This link is for instance very useful for recommender systems, i.e.¬†to propose movies based on the subjects with whom you correlate. We will also exploit this when we discuss on PCA.</p>
</div>
<div id="svd-and-inverse-of-a-matrix" class="section level1">
<h1><span class="header-section-number">5</span> SVD and inverse of a matrix</h1>
<p>A linear system of equations with <span class="math inline">\(n\)</span> equations and <span class="math inline">\(n\)</span> unknowns <span class="math display">\[
\mathbf{A}_{n\times n} \boldsymbol{\beta} = \mathbf{b}
\]</span> can be solved by <span class="math display">\[
\boldsymbol{\beta} = \mathbf{A}_{n\times n}^{-1} \mathbf{b}
\]</span></p>
<p>A unique solution exists if A is full rank.</p>
<p>Note, that a singular value decomposition of the square matrix <span class="math inline">\(\mathbf{A}=\mathbf{V}\boldsymbol{\Delta}\mathbf{V}^T\)</span> enables the inverse to be written as</p>
<p><span class="math display">\[
\mathbf{A}^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{V}^T
\]</span></p>
<p>indeed <span class="math display">\[
\mathbf{A}^{-1} \mathbf{A} = \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{V}^T = \mathbf{I}
\]</span></p>
<p>Note, that the SVD generalizes this to systems of under (n&lt;p, fat short matrices) and over determined systems (n&gt;p tall skinny matrices):</p>
<p>Let <span class="math display">\[\begin{eqnarray}
\mathbf{A} = \mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T
\end{eqnarray}\]</span></p>
<p>and we want to solve <span class="math display">\[\begin{eqnarray}
\mathbf{A} \boldsymbol{\beta} &amp;=&amp; \mathbf{b}\\
\mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &amp;=&amp; \mathbf{b}\\
\mathbf{U}^T\mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &amp;=&amp; \mathbf{U}^T\mathbf{b}\\
\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &amp;=&amp; \mathbf{U}^T\mathbf{b}\\
\boldsymbol{\Delta}^{-1}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &amp;=&amp; \boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}\\
\mathbf{V}\mathbf{V}^T \boldsymbol{\beta} &amp;=&amp; \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}\\
\boldsymbol{\beta} &amp;=&amp; \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}
\end{eqnarray}\]</span></p>
<p>Note, that for an overdetermined system <span class="math inline">\(n&gt;p\)</span> so <span class="math inline">\(r\leq p\)</span>. Generally, <span class="math inline">\(r=p\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is thus a square matrix so both <span class="math inline">\(\mathbf{V}^T\mathbf{V}=\mathbf{I}\)</span> and <span class="math inline">\(\mathbf{V}\mathbf{V}^T=\mathbf{I}\)</span>. However, <span class="math inline">\(\mathbf{U}^T\mathbf{U}=\mathbf{I}\)</span> but <span class="math inline">\(\mathbf{U}\mathbf{U}^T\neq\mathbf{I}\)</span> because <span class="math inline">\(r&lt;n\)</span>.</p>
<p><span class="math inline">\(\mathbf{A}^\dagger=\mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\)</span> is also referred to as the pseudo inverse and it enables us to solve under and overdetermined systems of equations.</p>
<p>Note, that for</p>
<ul>
<li>underdetermined systems there typically does not exist a unique solution</li>
<li>for overdetermined systems usually there does not exist an exact solution. We will focus on the latter in the next section where we explore the link between linear regression and SVD.</li>
</ul>
</div>
<div id="linear-regression-and-svd" class="section level1">
<h1><span class="header-section-number">6</span> Linear regression and SVD</h1>
<p>Suppose we have the linear regression problem with <span class="math inline">\(n&gt;p\)</span>:</p>
<p><span class="math display">\[\mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span></p>
<p><span class="math inline">\(\mathbf{X}\)</span> is a tall skinny matrix with <span class="math inline">\(n &gt;&gt; p\)</span>.</p>
<p>We know that <span class="math display">\[
\hat{\boldsymbol{\beta}}=\left(\mathbf{X}^T \mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}
\]</span></p>
<p>If we replace <span class="math inline">\(\mathbf{X}\)</span> by its SVD</p>
<p><span class="math display">\[\begin{eqnarray}
\hat{\boldsymbol{\beta}}&amp;=&amp;\left(\mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T\right)^{-1}\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
\hat{\boldsymbol{\beta}}&amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
\hat{\boldsymbol{\beta}}&amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{Y}
\end{eqnarray}\]</span></p>
<p>So the SVD also solves the linear regression problem by using the pseudoinverse! If we now think about the fit:</p>
<p><span class="math display">\[\begin{eqnarray}
\hat{\mathbf{Y}}&amp;=&amp;\mathbf{X}\hat{\boldsymbol{\beta}}\\
&amp;=&amp;\mathbf{U}\boldsymbol{\Delta}^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
&amp;=&amp;\mathbf{U}\mathbf{U}^T\mathbf{Y}
\end{eqnarray}\]</span></p>
<ul>
<li><p>For an overdetermined system <span class="math inline">\(\mathbf{U}\mathbf{U}^T\)</span> is not equal to the unity matrix <span class="math inline">\(\mathbf{I}\)</span> (for an overdetermined system only <span class="math inline">\(\mathbf{U}^T\mathbf{U}=\mathbf{I}\)</span> because <span class="math inline">\(n&gt;r\)</span>).</p></li>
<li><p>So <span class="math inline">\(\hat{\mathbf{Y}}\neq \mathbf{Y}\)</span>. Hence, we typically do not have an exact solution.</p></li>
<li><p>Note, that <span class="math inline">\(\mathbf{U}\mathbf{U}^T\)</span> spans the same space as the columns of <span class="math inline">\(\mathbf{X}\)</span>, and will define the same <span class="math inline">\(p\)</span>-dimensional plane in the <span class="math inline">\(n\)</span> dimensional space <span class="math inline">\(\mathcal{R}^n\)</span>, e.g.¬†cfr <span class="math inline">\(\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\)</span>. So it projects <span class="math inline">\(\mathbf{Y}\)</span> in the column space of <span class="math inline">\(\mathbf{X}\)</span> and the errors will be orthogonal onto this plane.</p></li>
</ul>
<div id="example-prostate-dataset" class="section level2">
<h2><span class="header-section-number">6.1</span> Example prostate dataset</h2>
<div id="fit-with-lm" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Fit with lm</h3>
<pre class="r"><code>prostate &lt;- read_csv(
  &quot;https://raw.githubusercontent.com/GTPB/PSLS20/master/data/prostate.csv&quot;,
  col_types = cols()
)
lm1 &lt;- lm(lpsa ~ lcavol + lweight + svi, prostate)</code></pre>
</div>
<div id="fit-with-svd" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Fit with SVD</h3>
<pre class="r"><code>X &lt;- prostate[,c(1:2,5)]
X[,3] &lt;- as.double(X[,3]!=&quot;healthy&quot;)
X &lt;- cbind(Intercept=1,X)

svdX &lt;- svd(X)
betaSvd &lt;- svdX$v %*% diag(1/svdX$d) %*% t(svdX$u) %*% prostate$lpsa

cbind(lm1$coef,betaSvd)</code></pre>
<pre><code>##                   [,1]       [,2]
## (Intercept) -0.2680724 -0.2680724
## lcavol       0.5516386  0.5516386
## lweight      0.5085359  0.5085359
## sviinvasion  0.6661583  0.6661583</code></pre>
</div>
</div>
</div>
<div id="svd-and-multi-dimensional-scaling-mds" class="section level1">
<h1><span class="header-section-number">7</span> SVD and Multi-Dimensional Scaling (MDS)</h1>
<div id="example" class="section level2">
<h2><span class="header-section-number">7.1</span> Example</h2>
<p>In this section we will use a dataset on food consumption in the UK. The data originate from the UKs ‚ÄòDepartment for Environment, Food and Rural Affairs‚Äô (DEFRA), showing the consumption in grams (per person, per week) of 17 different types of foodstuff measured and averaged in the four countries of the United Kingdom in 1997. We would like to explore the data and interpret how the food patterns of the different countries differ.</p>
<pre class="r"><code>uk &lt;- read_csv(
  &quot;https://raw.githubusercontent.com/statOmics/HDA2020/data/ukFoods.csv&quot;,
  col_types = cols()
)</code></pre>
<pre><code>## New names:
## * `` -&gt; ...1</code></pre>
<pre class="r"><code>knitr::kable(uk, caption = &quot;The full UK foods data table&quot;)</code></pre>
<table>
<caption>The full UK foods data table</caption>
<thead>
<tr class="header">
<th align="left">‚Ä¶1</th>
<th align="right">England</th>
<th align="right">Wales</th>
<th align="right">Scotland</th>
<th align="right">N.Ireland</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cheese</td>
<td align="right">105</td>
<td align="right">103</td>
<td align="right">103</td>
<td align="right">66</td>
</tr>
<tr class="even">
<td align="left">Carcass_meat</td>
<td align="right">245</td>
<td align="right">227</td>
<td align="right">242</td>
<td align="right">267</td>
</tr>
<tr class="odd">
<td align="left">Other_meat</td>
<td align="right">685</td>
<td align="right">803</td>
<td align="right">750</td>
<td align="right">586</td>
</tr>
<tr class="even">
<td align="left">Fish</td>
<td align="right">147</td>
<td align="right">160</td>
<td align="right">122</td>
<td align="right">93</td>
</tr>
<tr class="odd">
<td align="left">Fats_and_oils</td>
<td align="right">193</td>
<td align="right">235</td>
<td align="right">184</td>
<td align="right">209</td>
</tr>
<tr class="even">
<td align="left">Sugars</td>
<td align="right">156</td>
<td align="right">175</td>
<td align="right">147</td>
<td align="right">139</td>
</tr>
<tr class="odd">
<td align="left">Fresh_potatoes</td>
<td align="right">720</td>
<td align="right">874</td>
<td align="right">566</td>
<td align="right">1033</td>
</tr>
<tr class="even">
<td align="left">Fresh_Veg</td>
<td align="right">253</td>
<td align="right">265</td>
<td align="right">171</td>
<td align="right">143</td>
</tr>
<tr class="odd">
<td align="left">Other_Veg</td>
<td align="right">488</td>
<td align="right">570</td>
<td align="right">418</td>
<td align="right">355</td>
</tr>
<tr class="even">
<td align="left">Processed_potatoes</td>
<td align="right">198</td>
<td align="right">203</td>
<td align="right">220</td>
<td align="right">187</td>
</tr>
<tr class="odd">
<td align="left">Processed_Veg</td>
<td align="right">360</td>
<td align="right">365</td>
<td align="right">337</td>
<td align="right">334</td>
</tr>
<tr class="even">
<td align="left">Fresh_fruit</td>
<td align="right">1102</td>
<td align="right">1137</td>
<td align="right">957</td>
<td align="right">674</td>
</tr>
<tr class="odd">
<td align="left">Cereals</td>
<td align="right">1472</td>
<td align="right">1582</td>
<td align="right">1462</td>
<td align="right">1494</td>
</tr>
<tr class="even">
<td align="left">Beverages</td>
<td align="right">57</td>
<td align="right">73</td>
<td align="right">53</td>
<td align="right">47</td>
</tr>
<tr class="odd">
<td align="left">Soft_drinks</td>
<td align="right">1374</td>
<td align="right">1256</td>
<td align="right">1572</td>
<td align="right">1506</td>
</tr>
<tr class="even">
<td align="left">Alcoholic_drinks</td>
<td align="right">375</td>
<td align="right">475</td>
<td align="right">458</td>
<td align="right">135</td>
</tr>
<tr class="odd">
<td align="left">Confectionery</td>
<td align="right">54</td>
<td align="right">64</td>
<td align="right">62</td>
<td align="right">41</td>
</tr>
</tbody>
</table>
<p>Note, that here the matrix is displayed with the p variables in the rows and the n experimental units (countries) in the columns. This is often done for high dimensional data where <span class="math inline">\(p&gt;&gt;n\)</span> because this makes it easier to look at to the raw data table. Note, that the svd calculates left and right singular vectors so it will also provide the correct solution, we just should look to the other set of singular vectors in order to get to the correct interpretation.</p>
</div>
<div id="motivation-1" class="section level2">
<h2><span class="header-section-number">7.2</span> Motivation</h2>
<p>The objective of Multidimensional Scaling (MDS) is to find a low-dimensional representation, say <span class="math inline">\(k\)</span>-dimensional, of <span class="math inline">\(n\)</span> data points such that the distances between the <span class="math inline">\(n\)</span> points in the <span class="math inline">\(k\)</span>-dimensional space is a good approximation of a given squared distance matrix, say <span class="math inline">\(\mathbf{D}_X\)</span>.</p>
<ul>
<li><p>The squared distance matrix <span class="math inline">\(\mathbf{D}_X\)</span> may be given without knowledge of the original observations (not even the dimensionality), or it may be computed from a given set of <span class="math inline">\(n\)</span> <span class="math inline">\(p\)</span>-dimensional data points.</p></li>
<li><p>Note that the distances between points in a <span class="math inline">\(k\)</span>-dimensional subspace coincide with the distances between these points in the larger <span class="math inline">\(p\)</span>-dimensional space.</p></li>
</ul>
<p>Use of MDS:</p>
<ul>
<li><p>A high dimensional data matrix <span class="math inline">\(\mathbf{X}\)</span> is given, and one wants to get a visual representation (in 2 or 3 dimensions) of the observations. In this graph each point represents an observation (row of data matrix). From this graph one wants points close to one another to be similar, and observations far away from one another to be dissimilar. Thus the distances between the <span class="math inline">\(n\)</span> points in the original <span class="math inline">\(p\)</span>-dimensional space should be well preserved in the 2 or 3 dimensional space.</p></li>
<li><p>In some applications the researcher only has knowledge of the similarity (or dissimilarity) between observations. For example, food products can be evaluated by a taste panel and a dissimilarity matrix can be completed. This dissimilarity matrix shows for each pair of food products their dissimilarity (numerical values provided by taste panel, but not objectively quantified). Given this dissimilarity matrix, a 2 or 3 dimensional graph could be helpful if the distances between the points (food products) in this graph are monotonically related to the dissimilarities provided by the taste panel. Food products close to one another in this graph, taste similarly.</p></li>
<li><p>Here we discuss the ‚Äúmetric‚Äù MDS, which actually requires the Euclidean distances between observations. However, the method works also well if the matrix <span class="math inline">\(\mathbf{D}_X\)</span> contains dissimilarities rather than squared Euclidean distances.</p></li>
</ul>
<hr />
<p>In this chapter we assume that the data matrix <span class="math inline">\(\mathbf{X}\)</span> is column-centered (i.e.¬†each column has mean zero).</p>
<p>Centering can be accomplished by multiplying the original data matrix <span class="math inline">\(\mathbf{X}\)</span> with the <span class="math inline">\(n \times n\)</span> <span class="math inline">\(\mathbf{centering\ matrix}\)</span> <span class="math display">\[
   \mathbf{H} = \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1}^T ,
 \]</span> in which <span class="math inline">\(\mathbf{1}\)</span> is an <span class="math inline">\(n\)</span>-vector with all entries equal to 1. Hence, <span class="math inline">\(\mathbf{H}\mathbf{X}\)</span> is the column-centered data matrix.</p>
<p>We will assume that <span class="math inline">\(\mathbf{X}\)</span> is already column-centered, and therefore <span class="math inline">\(\mathbf{H}\mathbf{X}=\mathbf{X}\)</span>.</p>
<p>(Note, that the matrix <span class="math inline">\(\mathbf{1}\mathbf{1}^T\)</span> is an <span class="math inline">\(n\times n\)</span> matrix with all entries set to 1. )</p>
<hr />
<p>We will need the following interesting relationship between the <strong>Gram matrix</strong> <span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> and the distance matrix.</p>
<ul>
<li>For an <span class="math inline">\(n\times p\)</span> data matrix <span class="math inline">\(\mathbf{X}\)</span>, the matrix <span class="math inline">\(\mathbf{D}_X\)</span> with squared distances has elements <span class="math display">\[
    (\mathbf{x}_{i}-\mathbf{x}_{j})^T(\mathbf{x}_{i}-\mathbf{x}_{j}) = \Vert \mathbf{x}_i\Vert^2 - 2 \mathbf{x}^T_i\mathbf{x}_j + \Vert \mathbf{x}_j\Vert.^2
 \]</span></li>
</ul>
<p>-The <span class="math inline">\(n\times n\)</span> squared distance matrix can then be written as <span class="math display">\[
     \mathbf{D}_X = \mathbf{N} - 2\mathbf{X}\mathbf{X}^T + \mathbf{N}^T ,
   \]</span> with <span class="math inline">\(\mathbf{N}\)</span> the <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(i\)</span>th row filled with <span class="math inline">\(\Vert \mathbf{x}_i\Vert^2\)</span>.</p>
<ul>
<li><p>Note that the elements of <span class="math inline">\(\mathbf{N}\)</span> can also be found on the diagonal of <span class="math inline">\(\mathbf{XX}^T\)</span>.</p></li>
<li><p>Given the structure of the <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{N}\)</span> matrices, it is easy to verify that <span class="math display">\[
    -\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T.
 \]</span> This gives an important relation between the distance matrix and the Gram matrix. We will use the notation <span class="math inline">\(\mathbf{G}_X = -\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H}\)</span>.</p></li>
<li><p>The <span class="math inline">\(n \times n\)</span> Gram matrix <span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> equals <span class="math display">\[
  \begin{pmatrix}
\mathbf{x}_1^T \\
\vdots  \\
\mathbf{x}_n^T
  \end{pmatrix}
  \begin{pmatrix}
 \mathbf{x}_1 \ldots \mathbf{x}_n
  \end{pmatrix}
\]</span> and has thus on its <span class="math inline">\((i,j)\)</span>th position the inner product <span class="math display">\[
\mathbf{x}_i^T\mathbf{x}_j= \Vert \mathbf{x}_i\Vert\Vert \mathbf{x}_j\Vert \cos&lt;\mathbf{x}_i,\mathbf{x}_j&gt; .
\]</span></p></li>
<li><p>On the diagonal we find <span class="math inline">\(\mathbf{x}_i^T\mathbf{x}_i=\Vert \mathbf{x}_i\Vert^2\)</span>, the squared norm of the <span class="math inline">\(i\)</span>th observation.</p></li>
<li><p>The relation <span class="math inline">\(-\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T\)</span> tells us that the distance matrix and the Gram matrix contain the same information. The distances, however, are in most situations easier to interpret.</p></li>
</ul>
<hr />
</div>
<div id="link-with-the-svd" class="section level2">
<h2><span class="header-section-number">7.3</span> Link with the SVD</h2>
<p>Note, that we have shown that we can rewrite <span class="math inline">\(\mathbf{X}\mathbf{X}^T\)</span> using the SVD:</p>
<p><span class="math display">\[
\mathbf{X}\mathbf{X}^T = \mathbf{U}\boldsymbol{\Delta}^2\mathbf{U}^T
\]</span></p>
<p>Because the truncated SVD of <span class="math inline">\(\mathbf{X}\)</span> minimises the Frobenius norm <span class="math inline">\(\Vert \mathbf{X}-\mathbf{X}_k\Vert_F^2\)</span>, it this has an important consequence:</p>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{D}_X\)</span> denote the <span class="math inline">\(n \times n\)</span> matrix with the squared Euclidian distances between the <span class="math inline">\(n\)</span> data points <span class="math inline">\(\mathbf{x}_i\)</span> in the original <span class="math inline">\(p\)</span>-dimensional space.</p></li>
<li><p>Let <span class="math inline">\(\mathbf{D}_{Zk}\)</span> denote the <span class="math inline">\(n \times n\)</span> matrix with the squared Euclidian distances between the <span class="math inline">\(n\)</span> transformed data points <span class="math inline">\(\mathbf{z}_{k,i}\)</span> in the reduced <span class="math inline">\(k\)</span>-dimensional space.</p></li>
</ul>
<p>Then, it can be shown that the truncated SVD also minimises <span class="math display">\[
   \Vert \mathbf{D}_X - \mathbf{D}_{Zk} \Vert_F^2.
 \]</span></p>
<p>Or in other words: The n points <span class="math inline">\(\mathbf{z}_{k,i}\)</span> in the k-dimensional subspace spanned by the columns of <span class="math inline">\(\mathbf{V}_k\)</span> are the best approximation of the <span class="math inline">\(n\)</span> original points <span class="math inline">\(\mathbf{x}_i\)</span> in the original p-dimensional space in the sense that the Euclidean distances between the n original points are best approximated by the Euclidean distances between the n transformed points, among all possible k-dimensional linear subspaces.</p>
<hr />
<ul>
<li><p>If <span class="math inline">\(\mathbf{X}\)</span> is known, we can readily obtain the k-dimensional projection by the SVD of <span class="math inline">\(\mathbf{X}\)</span></p></li>
<li><p>If we only know the distance matrix <span class="math inline">\(\mathbf{D}\)</span></p>
<ol style="list-style-type: decimal">
<li>We can readily obtain the Gram matrix <span class="math display">\[-\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T\]</span></li>
<li>The truncated SVD of the squared and symmetric Gram matrix <span class="math display">\[\mathbf{U}_k\boldsymbol{\Delta}_k^\prime\mathbf{U}_k^T\]</span></li>
<li>from which also can obtain <span class="math display">\[\mathbf{Z}_k=\mathbf{U}_k\boldsymbol{\Delta}_k,\]</span> with <span class="math inline">\(\boldsymbol{\Delta}_k= \mathbf{\Delta}_k^{\prime \frac{1}{2}}\)</span></li>
</ol></li>
</ul>
</div>
<div id="example-1" class="section level2">
<h2><span class="header-section-number">7.4</span> Example</h2>
<pre class="r"><code>X &lt;- as.matrix(t(uk[,-1]))
n &lt;- nrow(X)
H &lt;- diag(n) - matrix(1/n,nrow=n,ncol=n)
X &lt;- H%*%X
svdUk &lt;- svd(X)

k &lt;- 2
Uk &lt;- svdUk$u[,1:k]
Dk &lt;- diag(svdUk$d[1:k])
Zk &lt;- Uk%*%Dk
rownames(Zk) &lt;- colnames(uk)[-1]
colnames(Zk) &lt;- paste0(&quot;Z&quot;,1:k)
Zk</code></pre>
<pre><code>##                   Z1          Z2
## England   -144.99315    2.532999
## Wales     -240.52915  224.646925
## Scotland   -91.86934 -286.081786
## N.Ireland  477.39164   58.901862</code></pre>
<pre class="r"><code>Zk %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(x=Z1,y=Z2,label=rownames(Zk))) +
  geom_point(size = 3) +
  geom_text(nudge_x = 50)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<ul>
<li><p>The graph suggests that Wales and England are quite similar in terms of food consumption, and North Ireland seem to have a very different food consumption pattern.</p></li>
<li><p>Also note that England is close to the origin, meaning that the food consumption pattern is close to the average pattern in the UK.</p></li>
<li><p>A few questions remain:</p>
<ul>
<li>How well can the 17-dimensional data be represented in a 2-dimensional subspace?</li>
<li>How can we interpret the distances (differences) between the data points in terms of the original 17 variables?</li>
</ul></li>
</ul>
</div>
<div id="the-biplot" class="section level2">
<h2><span class="header-section-number">7.5</span> The biplot</h2>
<p>The biplot is a single 2-dimensional graph which displays the information in both <span class="math inline">\(\mathbf{Z}_2=\mathbf{U}_2\boldsymbol{\Delta}_2\)</span> and <span class="math inline">\(\mathbf{V}_2\)</span>.</p>
<p>The plot of <span class="math inline">\(\mathbf{Z}_2\)</span> has been discussed previously (e.g.¬†best approximation of distances).</p>
<p>The name ‚Äúbi‚Äùplot refers to the plotting of two parts of the SVD (<span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(\mathbf{V}\)</span>) in a single graph.</p>
<p>From the geometrical interpretation of the SVD we know that <span class="math inline">\(\mathbf{Z}_2\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{X}\)</span> on <span class="math inline">\(\mathbf{V}_2\)</span> the basis spanned by the first two singular values. Thus for the <span class="math inline">\(i^\text{th}\)</span> individual we get <span class="math display">\[
\mathbf{z}_{2,i}= \mathbf{x}_i \left[\begin{array}{cc}\mathbf{v}_1&amp;\mathbf{v}_2\end{array}\right]
\]</span></p>
<p>We also know that we can approximate <span class="math inline">\(\mathbf{X}\)</span> by <span class="math inline">\(\mathbf{X}_2\)</span> which equals:</p>
<p><span class="math display">\[\begin{align}
  \mathbf{X}_2 &amp;= \mathbf{Z_2}\mathbf{V}_2^T\\
  &amp;= \mathbf{Z_2}\left[\begin{array}{c} \mathbf{v}_1^T\\
      \mathbf{v}_2^T\end{array}\right]
  = \mathbf{Z_2}
    \left[
      \begin{array}{ccc}\tilde{\mathbf{v}}_{2,1} \ldots \tilde{\mathbf{v}}_{2,p}\end{array}
    \right]
\end{align}\]</span></p>
<p>with <span class="math inline">\(\mathbf{v}_1\)</span> and <span class="math inline">\(\mathbf{v}_2\)</span> the first two right singular vector and <span class="math inline">\(\tilde{\mathbf{v}}_j\)</span> the j<span class="math inline">\(^\text{th}\)</span> column of the matrix <span class="math inline">\(\mathbf{V}_2^T\)</span>.</p>
<ul>
<li>This basically shows us that the orthogonal projection of <span class="math inline">\(\mathbf{z}_{2,i}\)</span> for subject/experimental unit i on <span class="math inline">\(\tilde{\mathbf{v}}_{2,j}\)</span> gives us the approximation of the value for j<span class="math inline">\(^\text{th}\)</span> variable that was observed for the <span class="math inline">\(i^\text{th}\)</span> experimental unit, i.e.¬†<span class="math inline">\(x_{2,ij}\)</span>. <span class="math display">\[x_{2,ij}=\mathbf{z}_{2,i}^T\tilde{\mathbf{v}}_{2,j}\]</span></li>
</ul>
<div id="uk-example" class="section level3">
<h3><span class="header-section-number">7.5.1</span> UK example</h3>
<div id="biplot" class="section level4">
<h4><span class="header-section-number">7.5.1.1</span> Biplot</h4>
<pre class="r"><code>Vk &lt;- svdUk$v[,1:k]
rownames(Vk) &lt;- uk[[1]]
colnames(Vk) &lt;- colnames(Zk)
scaleFactor &lt;- mean(svdUk$d[1:k])

diyBiplot &lt;- ggplot() +
  geom_point(
    data=Zk %&gt;% as.data.frame,
    aes(x=Z1,y=Z2),
    size = 3) +
  geom_text(
    data=Zk %&gt;% as.data.frame,
    aes(x=Z1,y=Z2,label=rownames(Zk)),
    nudge_x = 50) +
  geom_segment(
    data=Vk %&gt;% as.data.frame,
    aes(x=0, y=0, xend=Z1*scaleFactor, yend=Z2*scaleFactor),
    arrow=arrow(length=unit(0.4,&quot;cm&quot;)),
    alpha=0.25) +
  geom_text(
    data=Vk %&gt;% as.data.frame,
    aes(x=Z1*scaleFactor, y=Z2*scaleFactor, label=rownames(Vk)),
    alpha=0.5,
    size=3)

diyBiplot</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>biplot(Zk,Vk)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<p>The R code shows two ways of constructing a biplot: the first method starts from the SVD of <span class="math inline">\(\mathbf{X}\)</span> and plots the p vectors <span class="math inline">\(\tilde{\mathbf{v}}_{2,j}\)</span> as arrows. Note that we used a scaling factor to give the arrows a convenient length in the graph (not too small, not too large); it does not affect the interpretation. The second way of obtaining the biplot is simply by using the R built-in function biplot.</p>
</div>
<div id="illustration-of-projection" class="section level4">
<h4><span class="header-section-number">7.5.1.2</span> Illustration of projection</h4>
<p>We project <span class="math inline">\(\mathbf{z}_{2,i}\)</span> for N.Ireland on Fresh_potatoes.</p>
<p><img src="svd_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>rownames(X) &lt;- rownames(Zk)
colnames(X) &lt;- rownames(Vk)
X[,&quot;Fresh_potatoes&quot;]</code></pre>
<pre><code>##   England     Wales  Scotland N.Ireland 
##    -78.25     75.75   -232.25    234.75</code></pre>
<pre class="r"><code>Zk[&quot;N.Ireland&quot;,]%*%Vk[&quot;Fresh_potatoes&quot;,]</code></pre>
<pre><code>##          [,1]
## [1,] 233.7418</code></pre>
<p>We observe that N. Ireland has a consumption in Fresh_potatoes that is 234.75 above the average and this is well approximated by the projection 233.74.</p>
<p>Also note that</p>
<ul>
<li><p>The origin corresponds to the sample average of the 17-dimensional observations in the data matrix</p></li>
<li><p>England is close to the origin and thus one could say that the food consumption in England is as the average in the UK</p></li>
<li><p>Projecting the <span class="math inline">\(\mathbf{Z}_2\)</span> coordinates for North Ireland orthogonally onto the vector <span class="math inline">\(\tilde{\mathbf{v}}_{2,j}\)</span> of fresh potatoes, we get a large and positive <span class="math inline">\(x_{2,ij}\)</span>. Hence, people in N. Ireland tend to eat more fresh potatoes than on average in the UK.</p></li>
<li><p>We can do the same for the other vectors.</p></li>
</ul>
</div>
</div>
<div id="further-interpretation-of-the-biplot" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Further interpretation of the Biplot</h3>
<p>From <span class="math inline">\(\mathbf{x}_{k,i}^T = \mathbf{z}_{k,i}^T\mathbf{V}_k^T\)</span> we learnt that the original data can be (approximately) reconstructed from the <span class="math inline">\(\mathbf{z}_{k,i}\)</span>.</p>
<p>We now show how the dimensions of <span class="math inline">\(\mathbf{Z}_k\)</span> (columns or variables) can be interpreted. Remember, <span class="math display">\[
  \mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k = \left(\mathbf{x}_i^T \mathbf{v}_1 , \mathbf{x}_i^T \mathbf{v}_2 , \ldots , \mathbf{x}_i^T \mathbf{v}_k\right).
 \]</span></p>
<p>The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbf{Z}_k\)</span> can be written as <span class="math display">\[
   \mathbf{z}_{k,j} = \mathbf{X}\mathbf{v}_j,
 \]</span></p>
<p>An individual element of <span class="math inline">\(\mathbf{Z}_k\)</span> can be written as <span class="math display">\[
   z_{k,ij} = \mathbf{x}_i^T\mathbf{v}_j,
 \]</span></p>
<p>which is a linear combination of the observations on the <span class="math inline">\(p\)</span> variables in <span class="math inline">\(\mathbf{x}_i\)</span>, with coefficients given by the <span class="math inline">\(p\)</span> elements in <span class="math inline">\(\mathbf{v}_j\)</span>. Understanding the elements in <span class="math inline">\(\mathbf{v}_j\)</span> will give us insight into the interpretation of the <span class="math inline">\(j\)</span>th dimension of <span class="math inline">\(\mathbf{Z}_k\)</span>.</p>
</div>
</div>
<div id="illustration-uk-food" class="section level2">
<h2><span class="header-section-number">7.6</span> Illustration Uk food</h2>
<pre class="r"><code>p1 &lt;- ggplot() +
  geom_bar(
    aes( x=rownames(Vk), y=Vk[,1]),
    stat=&quot;identity&quot;) +
  xlab(&quot;V1&quot;) +
  ylab(&quot;contribution&quot;) +
  coord_flip()

p2 &lt;-
ggplot() +
  geom_bar(
    aes( x=rownames(Vk), y=Vk[,2]),
    stat=&quot;identity&quot;) +
  xlab(&quot;V2&quot;) +
  ylab(&quot;contribution&quot;) +
  coord_flip()

grid.arrange(p1,p2,ncol=2)</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>In giving an interpretation to the elements in <span class="math inline">\(\mathbf{v}_1\)</span> and <span class="math inline">\(\mathbf{v}_2\)</span>, we ignore the elements close to zero (this is a subjective decision; later we see more objective methods).</p>
<p>For <span class="math inline">\(\mathbf{v}_1\)</span> (1st dimension of <span class="math inline">\(\mathbf{Z}_2\)</span>):</p>
<ul>
<li><p>contrast of soft drinks and fresh potatoes versus fresh fruit and alcoholic drinks</p></li>
<li><p>a large value of <span class="math inline">\(z_{i1}\)</span> can result from eating many fresh potatoes and drinking a lot of soft drinks, but eating only few fresh fruit and drinking not much alcoholic drinks</p></li>
<li><p>a small value of <span class="math inline">\(z_{i1}\)</span> can result from eating few fresh potatoes and drinking only few soft drinks, but eating a lot of fresh fruit and drinking much alcoholic drinks</p></li>
</ul>
<p>For <span class="math inline">\(\mathbf{v}_2\)</span> (2nd dimension of <span class="math inline">\(\mathbf{Z}_2\)</span>):</p>
<ul>
<li><p>contrast of soft drinks versus fresh potatoes</p></li>
<li><p>a large value of <span class="math inline">\(z_{i2}\)</span> can result from eating many fresh potatoes, but drinking not much soft drinks</p></li>
<li><p>a small value of <span class="math inline">\(z_{i2}\)</span> can result from eating few fresh potatoes, but drinking much soft drinks .</p></li>
</ul>
<hr />
<p>The elements in v1 and v2 are also shown in the biplot.</p>
<pre class="r"><code>grid.arrange(p1, p2, diyBiplot, ncol=2, layout_matrix = rbind(c(1,2),c(3,3)))</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>From the graph we see e.g.¬†that N. Ireland has a high score for the first dimension. Now that we can give an interpretation to the dimension, we conclude that in Northern Ireland people eat relatively much fresh potatoes and drink many soft drinks, but they do not drink much alcoholic drinks and eat not much fresh fruit. We had already come to these findings by projecting N. Ireland on the vectors of these four food products. This comes to no surprise: both interpretations arise from the same data set and its SVD, and so no contradictory results should arise. Other conclusions can be found in a similar fashion.</p>
<p>We have derived the interpretation of the first dimension from the barplot of the elements of <span class="math inline">\(\mathbf{v}_1\)</span>. However, there is actually no need to make a separate plot to read the elements of <span class="math inline">\(\mathbf{v}_1\)</span>; they can also be read from projecting the vectors in the biplot onto the first dimension (i.e.¬†basis vector of the first dimension). For example, fresh potatoes is in the 7th column of <span class="math inline">\(\mathbf{X}\)</span> and thus in the biplot its vector is <span class="math inline">\(\tilde{\mathbf{v}}_{2,7}\)</span>; it is a 2-dimensional vector (2-dimensional biplot is shown). In the space of the biplot (i.e.¬†the column space of <span class="math inline">\(\mathbf{Z}_2\)</span>), the first basis vector is given by <span class="math inline">\((1,0)\)</span>. Projecting <span class="math inline">\(\tilde{\mathbf{v}}_{2,7}^T\)</span> orthogonally onto <span class="math inline">\((1,0)\)</span> gives <span class="math display">\[
  \tilde{\mathbf{v}}_{2,7}^T  \begin{pmatrix} 1 \\ 0 \end{pmatrix} = (v_{71} , v_{72}) \begin{pmatrix} 1 \\ 0 \end{pmatrix} = v_{71}
\]</span> which is the seventh element of the first right-singular vector of <span class="math inline">\(\mathbf{X}\)</span>, which is thus the bar of fresh potatoes in the barplot of the first dimension.</p>
</div>
</div>
<div id="svd-and-principal-component-analysis-pca" class="section level1">
<h1><span class="header-section-number">8</span> SVD and principal component analysis (PCA)</h1>
<ul>
<li><p>PCA is basically a SVD</p></li>
<li><p>PCA adds another layer of interpretation</p></li>
<li><p>PCA comes with its own terminology</p></li>
<li><p>One of the most widely used algorithms for dimension reduction and data exploration of multivariate and high dimensional data.</p></li>
<li><p>It is motivated from the decomposition of the Variance covariance matrix of the data</p></li>
</ul>
<div id="variance-covariance-matrix" class="section level2">
<h2><span class="header-section-number">8.1</span> Variance covariance matrix</h2>
<ul>
<li>For a given centered data matrix <span class="math inline">\(\mathbf{X}\)</span> the <span class="math inline">\(p\times p\)</span> covariance matrix can be estimated by <span class="math display">\[
\boldsymbol{\Sigma}_X = \frac{1}{n-1}\mathbf{X}^T\mathbf{X} =\frac{1}{n-1}\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T,
  \]</span> i.e.¬†the <span class="math inline">\((i,j)\)</span>th element is given by (column means are zero) <span class="math display">\[
\frac{1}{n-1}\sum_{m=1}^n x_{mi}x_{mj}=\frac{1}{n-1}\sum_{m=1}^n (x_{mi}-\bar{x}_i)(x_{mj}-\bar{x}_j).
  \]</span></li>
</ul>
<p>Note, that when we forget to write the factor <span class="math inline">\(1/(n-1)\)</span> all the derivations still hold. It is only a proportionality factor and it does not affect the interpretation.</p>
<hr />
</div>
<div id="conventional-derivation-of-pca" class="section level2">
<h2><span class="header-section-number">8.2</span> Conventional derivation of PCA</h2>
<p>PCA is usually introduced as follows.</p>
<p>Let <span class="math display">\[
  y_i=\mathbf{x}_i^T\mathbf{a}
\]</span> with <span class="math inline">\(\mathbf{a}\)</span> a <span class="math inline">\(p\)</span>-vector of constants. Hence <span class="math inline">\(y_i\)</span> is a linear combination (or transformation) of <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>PCA aims at finding <span class="math inline">\(\mathbf{a}\)</span> such that the sample variance among the <span class="math inline">\(n\)</span> <span class="math inline">\(y_i\)</span>‚Äôs is maximal, with <span class="math display">\[\begin{eqnarray*}
  \frac{1}{n-1}\sum_{i=1}^n (y_i - \bar{y})^2 =  \frac{1}{n-1}\sum_{i=1}^n y_i ^2 \\
    &amp;=&amp; \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i^T\mathbf{a})^2 = \frac{1}{n-1} \mathbf{a}^T \left(\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T\right) \mathbf{a} \\
    &amp;=&amp; \frac{1}{n-1} \mathbf{a}^T\mathbf{X}^T\mathbf{X}\mathbf{a} = \mathbf{a}^T\boldsymbol{\Sigma}_X\mathbf{a}
\end{eqnarray*}\]</span> in which <span class="math inline">\(\boldsymbol{\Sigma}_X= \frac{1}{n-1}\mathbf{X}^T\mathbf{X}\)</span> is the sample covariance matrix of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div id="problem-for-optimisation" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Problem for optimisation</h3>
<p>Finding <span class="math inline">\(\mathbf{a}\)</span> that maximises <span class="math display">\[
  \text{var}[y]  = \mathbf{a}^T\boldsymbol{\Sigma}_X\mathbf{a}
\]</span> has a trivial solution (set all elements of <span class="math inline">\(\mathbf{a}\)</span> equal to <span class="math inline">\(\infty\)</span>). To avoid the trivial solution, the solution must satisfy a restriction, e.g. <span class="math display">\[
  \Vert \mathbf{a}\Vert^2 = \mathbf{a}^T\mathbf{a}=1.
\]</span></p>
<p>The solution of the constrained maximisation problem may be formulated as <span class="math display">\[
  \mathbf{a} = \text{ArgMax}_{b:\Vert b\Vert^2=1} \mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}.
\]</span> By introducing a Lagrange multiplier <span class="math inline">\(\lambda\)</span>, we get an unconstrained maximisation problem, <span class="math display">\[
  \mathbf{a} = \text{ArgMax}_{b} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right).
\]</span></p>
<hr />
<p>Note that,</p>
<ul>
<li><p>If the constraint is linear, then the constrained optimisation problem (here: maximisation) may be replaced by an unconstrained optimisation problem of the same criterion but with an ``penalty term‚Äô‚Äô added. This method is due to Lagrange. The penalty term vanishes when the constraint is satisfied.</p></li>
<li><p>If the constraint is satisfied, <span class="math inline">\(\mathbf{b}^T\mathbf{b}=\Vert \mathbf{b}\Vert^2=1\)</span> and thus <span class="math inline">\(\mathbf{b}^T\mathbf{b}-1=0\)</span> and the ``penalty‚Äô‚Äô term in the unconstrained criterion vanishes.</p></li>
<li><p>The Lagrange multiplier <span class="math inline">\(\lambda\)</span> is to be considered as an extra parameter that may have to be estimated from the data.</p></li>
</ul>
<hr />
<p>The solution of <span class="math display">\[
  \mathbf{a} = \text{ArgMax}_{b} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right)
\]</span> is obtained by differentiating <span class="math inline">\(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\)</span> w.r.t. <span class="math inline">\(\mathbf{b}\)</span>, equating it to zero and solving for <span class="math inline">\(\mathbf{b}\)</span>. <span class="math display">\[\begin{eqnarray*}
 \frac{\partial}{\partial \mathbf{b}} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right)
    &amp;=&amp; 0 \\
    2\boldsymbol{\Sigma}_X\mathbf{b} -2\lambda\mathbf{b}
    &amp;=&amp; 0.
\end{eqnarray*}\]</span></p>
<p>Hence, we need the solution of <span class="math display">\[
  \boldsymbol{\Sigma}_X\mathbf{b} = \lambda\mathbf{b}.
\]</span></p>
<p>This equation has <span class="math inline">\(r\)</span> solutions:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{b}=\mathbf{e}_j\)</span>: the <span class="math inline">\(j\)</span>th eigenvector of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span></p></li>
<li><p><span class="math inline">\(\lambda=\lambda_j\)</span>: the <span class="math inline">\(j\)</span>th eigenvalue of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span>.</p></li>
</ul>
<p>The eigenvectors are orthonormal, i.e. <span class="math inline">\(\mathbf{e}_i^T\mathbf{e}_j=1\)</span> if <span class="math inline">\(i=j\)</span> and <span class="math inline">\(\mathbf{e}_i^T\mathbf{e}_j=0\)</span> otherwise.</p>
<hr />
<p>Consider the following calculations, with <span class="math inline">\(\mathbf{b}=\mathbf{e}_j\)</span>, the <span class="math inline">\(j\)</span>the eigenvector of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span>. <span class="math display">\[\begin{eqnarray*}
  \text{var}[y]
    &amp;=&amp; \mathbf{e}_j^T\boldsymbol{\Sigma}_X\mathbf{e}_j \\
    &amp;=&amp; \mathbf{e}_j^T(\boldsymbol{\Sigma}_X\mathbf{e}_j) \\
    &amp;=&amp; \mathbf{e}_j^T \left(\lambda_j \mathbf{e}_j\right) \\
    &amp;=&amp; \lambda_j\mathbf{e}_j^T\mathbf{e}_j \\
    &amp;=&amp; \lambda_j.
\end{eqnarray*}\]</span> By convention the eigenvectors/eigenvalues are ordered so that <span class="math display">\[
  \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_r.
\]</span> Hence, the first eigenvector <span class="math inline">\(\mathbf{e}_1\)</span> gives the largest variance <span class="math inline">\(\lambda_1\)</span>. Hence, this is the solution we were looking for.</p>
<hr />
<p>We now switch notation and from now onwards we denote <span class="math inline">\(y\)</span> by <span class="math inline">\(z\)</span>.</p>
<p>The observations on the first principal component (PC) are then given by <span class="math display">\[
    z_{i1} = \mathbf{x}_i^T\mathbf{e}_1.
\]</span></p>
<p>In PCA terminology they are referred to as the <strong>scores</strong> of the first PC. The elements in the eigenvector <span class="math inline">\(\mathbf{e}_i\)</span> that make up the transformation are known as the <strong>loadings</strong> of the first PC.</p>
<p>The first PC is thus a new variable (construct) that has the largest variance among all linear transformations of the original variables.</p>
<p>Variability between observations is considered as informative to understand differences between the observations.</p>
<hr />
<p>The equation <span class="math display">\[
  \boldsymbol{\Sigma}_X\mathbf{b} = \lambda\mathbf{b}
\]</span> has <span class="math inline">\(r=\text{rank}(\boldsymbol{\Sigma}_X)\)</span> solutions. The eigenvectors are orthonormal, i.e. <span class="math display">\[
  \forall i\neq j: \mathbf{e}_i^T \mathbf{e}_j = 0 \text{ and } i=j: \mathbf{e}_i^T \mathbf{e}_j = 1
\]</span> Hence (for <span class="math inline">\(i\neq j\)</span>) <span class="math display">\[
  \text{cov}\left[\mathbf{x}^T\mathbf{e}_i,\mathbf{x}^T\mathbf{e}_j\right] = \mathbf{e}_i^T\text{var}[\mathbf{x}]\mathbf{e}_j = \mathbf{e}_i^T\boldsymbol{\Sigma}_X\mathbf{e}_j = \mathbf{e}_i^T\left(\lambda_j\mathbf{e}_j\right) = \lambda_j \mathbf{e}_i^T\mathbf{e}_j = 0
\]</span></p>
<p>If <span class="math inline">\(z_{j}=\mathbf{x}_i^T\mathbf{e}_j\)</span> denotes the <span class="math inline">\(j\)</span>th PC, then <span class="math display">\[
  \text{cov}\left[z_i,z_j\right]=\text{cov}\left[\mathbf{x}^T\mathbf{e}_i,\mathbf{x}^T\mathbf{e}_j\right] =0 \text{ if } i\neq j
\]</span> <span class="math display">\[
  \text{var}\left[z_j\right] = \lambda_j .
\]</span> We say that the <span class="math inline">\(j\)</span>th PC maximises the variance among all linear transformations such that it is uncorrelated with the previous PCs.</p>
<hr />
</div>
<div id="interpretation-of-pca" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Interpretation of PCA</h3>
<p>A PCA is a transformation of the original <span class="math inline">\(p\)</span> variables to <span class="math inline">\(r\)</span> PCs such that</p>
<ul>
<li><p>the first PC has largest variance, equal to first eigenvalue of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span></p></li>
<li><p>the next PCs have decreasing variances (decreasing information content)</p></li>
<li><p>all PCs are mutually uncorrelated (no information-overlap).</p></li>
</ul>
<hr />
</div>
</div>
<div id="link-with-svd" class="section level2">
<h2><span class="header-section-number">8.3</span> Link with SVD</h2>
<p>If we write the eigen value decomposition in matrix form: <span class="math display">\[
\boldsymbol{\Sigma}_X\mathbf{
  B} = \mathbf{B}\boldsymbol{\Lambda}.
\]</span></p>
<p>with <span class="math inline">\(\boldsymbol{\Lambda}\)</span> is a diagonal matrix with diagonal elements <span class="math inline">\([\lambda_i]_{ii}\)</span>.</p>
<p>We recognise an expression that we have seen when discussing the link between the SVD and the sample covariance matrix</p>
<p><span class="math display">\[\begin{eqnarray}
\mathbf{X}^T\mathbf{X}\mathbf{V} &amp;=&amp;\mathbf{V}\boldsymbol{\Delta}^2
\end{eqnarray}\]</span></p>
<ul>
<li>So, the SVD can be used to solve the spectral decomposition (eigen value eigen vector) problem and the eigen vectors coincide with the right singular vectors and the eigen values are the singular values squared!</li>
</ul>
</div>
<div id="conservation-of-variance" class="section level2">
<h2><span class="header-section-number">8.4</span> Conservation of variance</h2>
<ul>
<li><p>The total variance in a data set <span class="math inline">\(\mathbf{X}\)</span> is given by the sum of the variances of the variables, <span class="math display">\[
  \sigma^2_{\text{tot}} = \sum_{j=1}^p \text{var}\left[X_j\right] = \text{trace}(\boldsymbol{\Sigma}_X)
\]</span></p></li>
<li><p>For a symmetric matrix it holds that <span class="math display">\[
 \text{trace}(\boldsymbol{\Sigma}_X) = \sum_{j=1}^r \lambda_r.
  \]</span></p></li>
<li><p>Since <span class="math inline">\(\lambda_r=\text{var}\left[Z_j\right]\)</span>, we find <span class="math display">\[
 \sigma^2_{\text{tot}} = \sum_{j=1}^p \text{var}\left[X_j\right]= \sum_{j=1}^r \text{var}\left[Z_j\right].
  \]</span></p></li>
</ul>
<p><span class="math inline">\(\longrightarrow\)</span> Thus no information is lost by the PCA transformation from the original <span class="math inline">\(p\)</span>-dimensional space to the <span class="math inline">\(r\)</span>-dimensional PCA space.</p>
<hr />
</div>
<div id="choosing-the-number-of-dimensions" class="section level2">
<h2><span class="header-section-number">8.5</span> Choosing the number of dimensions</h2>
<p>For PCA the reasoning is usually based on the relative variance of a PC, <span class="math display">\[
   \frac{\text{var}\left[Z_j\right]}{\sigma^2_{\text{tot}}} = \frac{\lambda_j}{\sum_{i=1}^r \lambda_i} =\frac{\delta_j^2}{\sum_{i=1}^r \delta_i^2} .
\]</span> If the first <span class="math inline">\(k\)</span> PCs are selected for further use, then they represent <span class="math display">\[
  100 \times  \frac{\sum_{j=1}^k \lambda_j}{\sum_{i=1}^r \lambda_i} \%
  = 100 \times  \frac{\sum_{j=1}^k \delta^2_j}{\sum_{i=1}^r \delta_i^2} \%
\]</span> of the total variance (or information) of the original data set <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div id="uk-example-1" class="section level3">
<h3><span class="header-section-number">8.5.1</span> UK example</h3>
<p>A scree plot is often used to look at the eigenvalues. Here it is shown for the UK consumption data.</p>
<pre class="r"><code>n &lt;- nrow(X)
r &lt;- ncol(svdUk$v)
totVar &lt;- sum(svdUk$d^2)/(n-1)
vars &lt;- data.frame(comp=1:4,var=svdUk$d^2/(n-1)) %&gt;%
  mutate(propVar=var/totVar,cumVar=cumsum(var/totVar))

pVar1 &lt;- vars %&gt;%
  ggplot(aes(x=comp:r,y=var)) +
  geom_point() +
  geom_line() +
  xlab(&quot;Component&quot;) +
  ylab(&quot;Variance&quot;)

pVar2 &lt;- vars %&gt;%
  ggplot(aes(x=comp:r,y=propVar)) +
  geom_point() +
  geom_line() +
  xlab(&quot;Component&quot;) +
  ylab(&quot;Proportion of Total Variance&quot;)

pVar3 &lt;- vars %&gt;%
  ggplot(aes(x=comp:r,y=cumVar)) +
  geom_point() +
  geom_line() +
  xlab(&quot;Component&quot;) +
  ylab(&quot;Cum. prop. of tot. var.&quot;)

grid.arrange(pVar1, pVar2, pVar3, nrow=1)</code></pre>
<pre><code>## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used

## Warning in comp:r: numerical expression has 4 elements: only the first used</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>From these graphs we may conclude that with using k = 2 dimensions, more than 95% of the total variance is retained.</p>
<hr />
</div>
<div id="choosing-the-number-of-dimensions-svd" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Choosing the number of dimensions SVD</h3>
<p>More generally, scree plots are also informative for the SVD.</p>
<p>The motivation comes from <span class="math display">\[
    \text{Min}_{A: \text{rank}(A)=k} \Vert \mathbf{X} - \mathbf{A}\Vert_F^2 = \Vert \mathbf{X} - \mathbf{X}_k\Vert_F^2 = \sum_{j=k+1}^r\delta_{j}^2 = \sum_{j=k+1}^r\lambda_{j}.
  \]</span> This is known as the <strong>approximation error</strong> of the matrix <span class="math inline">\(\mathbf{X}_k\)</span>.</p>
<p>Hence, <span class="math display">\[
    \frac{\sum_{j=1}^k\delta_j^2}{\sum_{j=1}^r \delta_j^2}  =\frac{\sum_{j=1}^k\lambda_{j}}{\sum_{j=1}^r\lambda_{j}}
  \]</span> still makes sense as a relative quality measure for the SVD in general (including matrix approximation and MDS).</p>
</div>
<div id="rules-of-thumb-to-select-number-of-dimensions" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Rules of thumb to select number of dimensions</h3>
<p>Select k such that</p>
<ul>
<li><p>at least 80% of the total variance is retained in the PC space, or, equivalently, at most 20% relative approximation error</p></li>
<li><p>adding more dimensions does not add more information (in a relative sense); this can often be visually detected as the ‚Äúknee‚Äù or ‚Äúelbow‚Äù in the scree plot</p></li>
<li><p>none of the dimensions has a variance smaller than the variance of one of the p original variables (this rule is typically only used when the variables are first standardised - see later).</p></li>
</ul>
</div>
</div>
<div id="covariance-vs-correlation-matrix" class="section level2">
<h2><span class="header-section-number">8.6</span> Covariance vs Correlation Matrix</h2>
<div id="direction-of-largest-variability" class="section level3">
<h3><span class="header-section-number">8.6.1</span> Direction of largest variability</h3>
<p>Now that we know how the PCs are constructed, we can give an additional interpretation to the first eigenvector / right-singular vector of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span>.</p>
<ul>
<li><p>The first eigenvector of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span> is the direction in the original <span class="math inline">\(p\)</span>-dimensional space of the largest variability.</p></li>
<li><p>The second eigenvector of <span class="math inline">\(\boldsymbol{\Sigma}_X\)</span> is the direction in the original <span class="math inline">\(p\)</span>-dimensional space of the second largest variability, among all directions orthogonal to the first eigenvector</p></li>
</ul>
<div id="iris-example" class="section level4">
<h4><span class="header-section-number">8.6.1.1</span> Iris example</h4>
<p>The iris dataset in R contains information on leaves of iris flowers from different species. Here, we will focus on the setosa species and on the sepal length and sepal width.</p>
<pre class="r"><code>irisSetosa &lt;- iris %&gt;%
  filter(Species == &quot;setosa&quot;) %&gt;%
  dplyr::select(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;)

nIris &lt;- nrow(irisSetosa)
hIris &lt;- diag(nIris) - matrix(1/nIris, nIris, nIris)
irisX &lt;- irisSetosa %&gt;%
  as.matrix
irisX &lt;- hIris %*% irisX

irisSvd &lt;- svd(irisX)

pIris &lt;- irisX %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(x=Sepal.Length,y=Sepal.Width)) +
  geom_point()

pIris &lt;- pIris +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = -irisSvd$v[1,1]*irisSvd$d[1]/sqrt(nIris-1),
      yend = -irisSvd$v[2,1]*irisSvd$d[1]/sqrt(nIris-1)
      ),
      arrow = arrow(length=unit(0.4,&quot;cm&quot;))
    ) +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd$v[1,2]*irisSvd$d[2]/sqrt(nIris-1),
      yend = irisSvd$v[2,2]*irisSvd$d[2]/sqrt(nIris-1)
      ),
    arrow = arrow(length=unit(0.4,&quot;cm&quot;))
    ) +
  geom_text(
    aes(
      x = -irisSvd$v[1,1]*irisSvd$d[1]/sqrt(nIris-1)*1.2,
      y = -irisSvd$v[2,1]*irisSvd$d[1]/sqrt(nIris-1)*1.2,
      label=&quot;e1&quot;
      )
    ) +
  geom_text(
    aes(
      x = irisSvd$v[1,2]*irisSvd$d[2]/sqrt(nIris-1)*1.2,
      y = irisSvd$v[2,2]*irisSvd$d[2]/sqrt(nIris-1)*1.2,
      label=&quot;e2&quot;
      )
    ) +
  xlim(-1, 1) +
  ylim(-1,1)

pIris</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
</div>
<div id="covariance-versus-correlation-matrix" class="section level3">
<h3><span class="header-section-number">8.6.2</span> Covariance versus correlation matrix</h3>
<p>So far we have worked with</p>
<ul>
<li><p>the column-centered data matrix <span class="math inline">\(\mathbf{X}\)</span></p></li>
<li><p>the SVD of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_X\propto \mathbf{X}^T\mathbf{X}\)</span> (the covariance matrix).</p></li>
</ul>
<p>In some situations it is better to start from the standardised variables:</p>
<ul>
<li>substract from each element in <span class="math inline">\(\mathbf{X}\)</span> the column-mean</li>
<li>divide each centered element in <span class="math inline">\(\mathbf{X}\)</span> by the column-specific standard deviation</li>
</ul>
<p>Thus each element <span class="math inline">\(x_{ij}\)</span> is replaced with <span class="math display">\[
     \frac{x_{ij}-\bar{x}_j}{s_j} ,
  \]</span> with <span class="math inline">\(\bar{x}_j\)</span> and <span class="math inline">\(s_j\)</span> the column mean and column standard deviation.</p>
<ul>
<li>In matrix notation, the standardisation, starting from the centered matrix <span class="math inline">\(\mathbf{X}\)</span> is computed as <span class="math display">\[
  \mathbf{X}\mathbf{S}^{\prime-1}
  \]</span> where <span class="math inline">\(\mathbf{S}^\prime\)</span> is a diagonal matrix with the column-specific standard deviations.</li>
</ul>
<hr />
<div id="iris-example-1" class="section level4">
<h4><span class="header-section-number">8.6.2.1</span> Iris Example</h4>
<pre class="r"><code>var(irisX)</code></pre>
<pre><code>##              Sepal.Length Sepal.Width
## Sepal.Length   0.12424898  0.09921633
## Sepal.Width    0.09921633  0.14368980</code></pre>
<pre class="r"><code>Sprime &lt;- diag(sqrt(diag(var(irisX))))
Xs &lt;- irisX%*%solve(Sprime)
var(Xs)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 1.0000000 0.7425467
## [2,] 0.7425467 1.0000000</code></pre>
<p>Much faster to use the scale function. By default center=TRUE and scale=TRUE.</p>
<pre class="r"><code>Xs &lt;- irisSetosa %&gt;% scale
var(Xs)</code></pre>
<pre><code>##              Sepal.Length Sepal.Width
## Sepal.Length    1.0000000   0.7425467
## Sepal.Width     0.7425467   1.0000000</code></pre>
<p>Show problem of different units:</p>
<ul>
<li>Sepal length in cm</li>
<li>Sepal width in mm</li>
</ul>
<pre class="r"><code>irisX2 &lt;- irisX
irisX2[,2] &lt;- irisX2[,2]*10

pIris2 &lt;- irisX2 %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(x=Sepal.Length,y=Sepal.Width)) +
  geom_point()

irisSvd2 &lt;- svd(irisX2)

pIris2 &lt;- pIris2 +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd2$v[1,1]*irisSvd2$d[1]/sqrt(nIris-1),
      yend = irisSvd2$v[2,1]*irisSvd2$d[1]/sqrt(nIris-1)
      ),
      arrow = arrow(length=unit(0.4,&quot;cm&quot;))
    ) +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd2$v[1,2]*irisSvd2$d[2]/sqrt(nIris-1),
      yend = irisSvd2$v[2,2]*irisSvd2$d[2]/sqrt(nIris-1)
      ),
    arrow = arrow(length=unit(0.4,&quot;cm&quot;))
    ) +
  geom_text(
    aes(
      x = irisSvd2$v[1,1]*irisSvd2$d[1]/sqrt(nIris-1)*1.2,
      y = irisSvd2$v[2,1]*irisSvd2$d[1]/sqrt(nIris-1)*1.2,
      label=&quot;e1&quot;
      )
    ) +
  geom_text(
    aes(
      x = irisSvd2$v[1,2]*irisSvd2$d[2]/sqrt(nIris-1)*1.2,
      y = irisSvd2$v[2,2]*irisSvd2$d[2]/sqrt(nIris-1)*1.2,
      label=&quot;e2&quot;
      )
    ) +
  xlim(-10, 10) +
  ylim(-10,10)

grid.arrange(pIris, pIris2)</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).

## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Top: original matrix ‚Äì Bottom: second column of <span class="math inline">\(\mathbf{X}\)</span> multiplied by 10 (e.g.¬†moving from cm to mm).</p>
<p>Directions of maximal variability are affected by the units of the variables.</p>
<hr />
</div>
</div>
<div id="recommendations" class="section level3">
<h3><span class="header-section-number">8.6.3</span> Recommendations</h3>
<p>When to use correlation and when covariance?</p>
<ul>
<li><p>use correlation when columns of <span class="math inline">\(\mathbf{X}\)</span> are expressed in different units</p></li>
<li><p>use covariance when columns of <span class="math inline">\(\mathbf{X}\)</span> are expressed in the same units.</p></li>
</ul>
<p>There may be exceptions.</p>
<hr />
</div>
</div>
<div id="pca-and-the-multivariate-normal-distribution" class="section level2">
<h2><span class="header-section-number">8.7</span> PCA and the Multivariate Normal Distribution</h2>
<p>The density function of a multivariate normal distribution (MVN) is given by <span class="math display">\[
  f(\mathbf{x}) = (2\pi)^{-p/2}\vert \boldsymbol{\Sigma}\vert^{-1/2} \exp\left( -\frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})\right) ,
\]</span> where</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{\mu}\)</span> is the multivariate mean vector (<span class="math inline">\(p\)</span>-dimensional). The <span class="math inline">\(j\)</span>th element is <span class="math inline">\(\mu_j = \text{E}\left[X_j\right]\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the <span class="math inline">\(p \times p\)</span> covariance matrix. The <span class="math inline">\((i,j)\)</span>th element is <span class="math inline">\(\sigma_{ij}=\text{cov}\left[X_i, X_j\right]\)</span>.</p></li>
</ul>
<hr />
<p>To get a better understanding of the MVN we focus on the exponential which has the factor <span class="math display">\[
    (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})
\]</span> This factor</p>
<ul>
<li><p>is the only factor in the density function that depends on <span class="math inline">\(\mathbf{x}\)</span></p></li>
<li><p>is a <span class="math inline">\(\mathbf{quadratic}\)</span> form</p></li>
<li><p>is constant in points <span class="math inline">\(\mathbf{x}\)</span> with constant density <span class="math inline">\(f(\mathbf{x})\)</span>.</p></li>
</ul>
<hr />
<p>Consider <span class="math inline">\(p=2\)</span> (bivariate normal). Then, all <span class="math inline">\(\mathbf{x} \in \mathbb{R}^2\)</span> for which <span class="math display">\[
    (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) = \text{constant } c^2
\]</span> lie on an ellipse with center <span class="math inline">\(\boldsymbol{\mu}\)</span>.</p>
<p>These ellipses are known as <strong>constant density ellipses</strong>.</p>
<div id="iris-example-2" class="section level3">
<h3><span class="header-section-number">8.7.1</span> iris example</h3>
<pre class="r"><code>pIris +
  stat_ellipse() +
  stat_ellipse(level=.68) +
  stat_ellipse(level=.1)</code></pre>
<pre><code>## Warning: Removed 1 rows containing non-finite values (stat_ellipse).

## Warning: Removed 1 rows containing non-finite values (stat_ellipse).

## Warning: Removed 1 rows containing non-finite values (stat_ellipse).</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<hr />
<p>Now plug in the SVD of <span class="math inline">\(\boldsymbol{\Sigma}=\mathbf{VDV}^T\)</span>, <span class="math display">\[\begin{eqnarray*}
   (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) &amp;=&amp; c^2\\
   (\mathbf{x}-\boldsymbol{\mu})^T \mathbf{VD}^{-1}\mathbf{V}^T (\mathbf{x}-\boldsymbol{\mu}) &amp;=&amp; c^2
\end{eqnarray*}\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{x}-\boldsymbol{\mu}\)</span> is the centered <span class="math inline">\(\mathbf{x}\)</span>. Without loss of generality, take <span class="math inline">\(\boldsymbol{\mu}=\mathbf{0}\)</span>. Hence, <span class="math display">\[\begin{eqnarray*}
   \mathbf{x}^T \mathbf{VD}^{-1}\mathbf{V}^T \mathbf{x} &amp;=&amp; c^2    \\
   \left(\mathbf{x}^T\mathbf{V}\right) \mathbf{D}^{-1} \left(\mathbf{x}^T\mathbf{V}\right)^T &amp;=&amp; c^2 \\
   \sum_{j=1}^p (\mathbf{x}^T\mathbf{v}_j)^2  / \delta_j &amp;=&amp; c^2\\
   \sum_{j=1}^p (z_j)^2  / (c^2\delta_j) &amp;=&amp; 1.
\end{eqnarray*}\]</span> The last equation is the equation of an ellipse with axes parallel to the basis of <span class="math inline">\((z_1,\ldots, z_p)\)</span> and with half axis lengths <span class="math inline">\(c\sqrt{\lambda_j}=c\delta_j\)</span> with <span class="math inline">\(\lambda_j\)</span> the <span class="math inline">\(j\)</span>th eigenvalue of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<hr />
<pre class="r"><code>pIris +
  stat_ellipse() +
  stat_ellipse(level=.68) +
  stat_ellipse(level=.1)</code></pre>
<pre><code>## Warning: Removed 1 rows containing non-finite values (stat_ellipse).

## Warning: Removed 1 rows containing non-finite values (stat_ellipse).

## Warning: Removed 1 rows containing non-finite values (stat_ellipse).</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>This graphs shows <span class="math inline">\(n=50\)</span> data points on <span class="math inline">\(p=2\)</span> dimensions. The two ellipses are constant density ellipses for three different values of <span class="math inline">\(c\)</span> (i.e.¬†three different constant densities). The inner ellipse corresponds to the largest constant density and the outer to the smallest constant density. The arrows show the two eigenvectors / singular vectors and they are scaled according to the sqrt of the eigen values and they form the axes of the constant density ellipses. It is clear that the first axis is pointing to the larger variance.</p>
</div>
</div>
<div id="biplot-1" class="section level2">
<h2><span class="header-section-number">8.8</span> Biplot</h2>
<ul>
<li>Because of the close connection between PCA and the SVD, the biplot as discussed before is still meaningful, with the first axis pointing into the direction of largest variance.</li>
</ul>
</div>
<div id="ovarian-cancer-example" class="section level2">
<h2><span class="header-section-number">8.9</span> Ovarian Cancer Example</h2>
<p>The ovarian cancer data set consists of proteomics data for 216 patients, 121 of whom have ovarian cancer, and 95 of whom do not. For each subject, the expression of 4000 spectral features is assessed. The first 121 rows consist of data for the cancer patients.</p>
<div id="importing-the-data" class="section level3">
<h3><span class="header-section-number">8.9.1</span> Importing the data</h3>
<pre class="r"><code>ovarian &lt;- read_csv(
  &quot;https://raw.githubusercontent.com/statOmics/HDA2020/data/ovarian.csv&quot;,
  col_names = FALSE,
  col_types = cols()
)
grid.arrange(
  qplot(1:4000,
    ovarian[1,] %&gt;% unlist,
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;cancer&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[2,] %&gt;% unlist,
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;cancer&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,] %&gt;% unlist,
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;Normal&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,] %&gt;% unlist,
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;Normal&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  ncol=1
  )</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Centering</p>
<pre class="r"><code>ovarian &lt;-  scale(ovarian, scale=FALSE)
grid.arrange(
  qplot(1:4000,
    ovarian[1,],
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;cancer&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[2,],
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;cancer&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,],
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;Normal&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[201,],
    geom=&quot;line&quot;,
    ylab=&quot;centered intensity&quot;,
    xlab=&quot;&quot;,main=&quot;Normal&quot;,
    ylim=range(ovarian[c(1,2,200,201),])),
  ncol=1
  )</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
</div>
<div id="svd-analysis" class="section level3">
<h3><span class="header-section-number">8.9.2</span> SVD Analysis</h3>
<pre class="r"><code>svdOvarian &lt;- svd(ovarian)

nOvarian &lt;- nrow(ovarian)
r &lt;- ncol(svdOvarian$v)

totVar &lt;- sum(svdOvarian$d^2)/(nOvarian-1)
vars &lt;- data.frame(comp=1:r,var=svdOvarian$d^2/(nOvarian-1)) %&gt;%
  mutate(propVar=var/totVar,cumVar=cumsum(var/totVar))

pVar2 &lt;- vars %&gt;%
  ggplot(aes(x=comp:r,y=propVar)) +
  geom_point() +
  geom_line() +
  xlab(&quot;Component&quot;) +
  ylab(&quot;Proportion of Total Variance&quot;)

pVar3 &lt;- vars %&gt;%
  ggplot(aes(x=comp:r,y=cumVar)) +
  geom_point() +
  geom_line() +
  xlab(&quot;Component&quot;) +
  ylab(&quot;Cum. prop. of tot. var.&quot;)

grid.arrange(pVar2, pVar3, nrow=1)</code></pre>
<pre><code>## Warning in comp:r: numerical expression has 216 elements: only the first used

## Warning in comp:r: numerical expression has 216 elements: only the first used

## Warning in comp:r: numerical expression has 216 elements: only the first used

## Warning in comp:r: numerical expression has 216 elements: only the first used

## Warning in comp:r: numerical expression has 216 elements: only the first used

## Warning in comp:r: numerical expression has 216 elements: only the first used</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>We see that we can explain a lot of the variability using a few PC‚Äôs!</p>
<pre class="r"><code>Zk &lt;- svdOvarian$u[,1:6]%*%diag(svdOvarian$d[1:6])
colnames(Zk) &lt;- paste0(&quot;Z&quot;,1:6)
Vk &lt;- svdOvarian$v[,1:6]
colnames(Vk) &lt;- paste0(&quot;V&quot;,1:6)
reduced &lt;- data.frame(Zk,cancer=c(rep(1,121),rep(2,95)))

pOv1 &lt;- reduced %&gt;%
  ggplot(aes(x=Z1,y=Z2,col=cancer)) +
  geom_point()  +
  theme(legend.position = &quot;none&quot;)

pOv2 &lt;- Vk %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(x=1:4000,y=V1)) +
  geom_line() +
  xlab(&quot;&quot;)

pOv3 &lt;- Vk %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(x=1:4000,y=V2)) +
  geom_line() +
  xlab(&quot;&quot;)

grid.arrange(pOv1,pOv2,pOv3,layout_matrix = rbind(c(1,3),c(2,NA)))</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Reconstruction of profiles:</p>
<pre class="r"><code>i &lt;- 1
pOv4 &lt;- qplot(1:4000,
  ovarian[i,],
  geom=&quot;line&quot;,
  ylab=&quot;centered intensity&quot;,
  xlab=&quot;&quot;,main=&quot;Cancer&quot;,
  ylim=range(ovarian[c(1,2,200,201),]))

pOv5 &lt;- qplot(1:4000,
  Vk[,1:2] %*%Zk[i,1:2],
  geom=&quot;line&quot;,
  ylab=&quot;centered intensity&quot;,
  xlab=&quot;&quot;,main=&quot;Reconstructed&quot;,
  ylim=range(ovarian[c(1,2,200,201),]))



grid.arrange(
  pOv1 +
    annotate(&quot;point&quot;, x = reduced[i,1], y = reduced[i,2], colour = &quot;red&quot;,cex=3),
  pOv2,
  pOv3,
  pOv4,
  pOv5,
  layout_matrix = rbind(c(1,2,3),c(4,5,NA)))</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<pre class="r"><code>i &lt;- 3
pOv4 &lt;- qplot(1:4000,
  ovarian[i,],
  geom=&quot;line&quot;,
  ylab=&quot;centered intensity&quot;,
  xlab=&quot;&quot;,main=&quot;Cancer&quot;,
  ylim=range(ovarian[c(1,2,200,201),]))

pOv5 &lt;- qplot(1:4000,
  Vk[,1:2] %*%Zk[i,1:2],
  geom=&quot;line&quot;,
  ylab=&quot;centered intensity&quot;,
  xlab=&quot;&quot;,main=&quot;Reconstructed&quot;,
  ylim=range(ovarian[c(1,2,200,201),]))



grid.arrange(
  pOv1 +
    annotate(&quot;point&quot;, x = reduced[i,1], y = reduced[i,2], colour = &quot;red&quot;,cex=3),
  pOv2,
  pOv3,
  pOv4,
  pOv5,
  layout_matrix = rbind(c(1,2,3),c(4,5,NA)))</code></pre>
<p><img src="svd_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="acknowledgement" class="section level1 unnumbered">
<h1>Acknowledgement</h1>
<ul>
<li>Olivier Thas for sharing his materials of Analysis of High Dimensional Data 2019-2020, which I used as the starting point for this chapter.</li>
</ul>
</div>
<div id="session-info" class="section level1 unnumbered">
<h1>Session info</h1>
<details>
<p><summary>Session info</summary></p>
<pre><code>## [1] &quot;2021-10-13 14:45:21 UTC&quot;</code></pre>
<pre><code>## ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
##  setting  value                       
##  version  R version 4.1.1 (2021-08-10)
##  os       macOS Catalina 10.15.7      
##  system   x86_64, darwin17.0          
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  ctype    en_US.UTF-8                 
##  tz       UTC                         
##  date     2021-10-13                  
## 
## ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
##  ! package     * version    date       lib source                          
##  P assertthat    0.2.1      2019-03-21 [?] CRAN (R 4.1.0)                  
##  P backports     1.2.1      2020-12-09 [?] CRAN (R 4.1.0)                  
##  P bit           4.0.4      2020-08-04 [?] CRAN (R 4.1.0)                  
##  P bit64         4.0.5      2020-08-30 [?] CRAN (R 4.1.0)                  
##  P bitops        1.0-7      2021-04-24 [?] standard (@1.0-7)               
##  P bmp           0.3        2017-09-11 [?] CRAN (R 4.1.0)                  
##  P broom         0.7.9      2021-07-27 [?] CRAN (R 4.1.0)                  
##  P bslib         0.3.0      2021-09-02 [?] CRAN (R 4.1.1)                  
##  P cellranger    1.1.0      2016-07-27 [?] CRAN (R 4.1.0)                  
##  P cli           3.0.1      2021-07-17 [?] CRAN (R 4.1.0)                  
##  P colorspace    2.0-2      2021-06-24 [?] CRAN (R 4.1.0)                  
##  P crayon        1.4.1      2021-02-08 [?] CRAN (R 4.1.0)                  
##  P curl          4.3.2      2021-06-23 [?] CRAN (R 4.1.0)                  
##  P DBI           1.1.1      2021-01-15 [?] CRAN (R 4.1.0)                  
##  P dbplyr        2.1.1      2021-04-06 [?] CRAN (R 4.1.0)                  
##  P digest        0.6.28     2021-09-23 [?] CRAN (R 4.1.0)                  
##  P downloader  * 0.4        2015-07-09 [?] CRAN (R 4.1.0)                  
##  P dplyr       * 1.0.7      2021-06-18 [?] CRAN (R 4.1.0)                  
##  P ellipsis      0.3.2      2021-04-29 [?] standard (@0.3.2)               
##  P emo           0.0.0.9000 2021-10-13 [?] Github (hadley/emo@3f03b11)     
##  P evaluate      0.14       2019-05-28 [?] CRAN (R 4.1.0)                  
##  P fansi         0.5.0      2021-05-25 [?] CRAN (R 4.1.0)                  
##  P farver        2.1.0      2021-02-28 [?] CRAN (R 4.1.0)                  
##  P fastmap       1.1.0      2021-01-25 [?] CRAN (R 4.1.0)                  
##  P forcats     * 0.5.1      2021-01-27 [?] CRAN (R 4.1.0)                  
##  P fs            1.5.0      2020-07-31 [?] CRAN (R 4.1.0)                  
##  P generics      0.1.0      2020-10-31 [?] CRAN (R 4.1.0)                  
##  P ggmap       * 3.0.0      2019-02-05 [?] CRAN (R 4.1.0)                  
##  P ggplot2     * 3.3.5      2021-06-25 [?] CRAN (R 4.1.0)                  
##  P glue          1.4.2      2020-08-27 [?] CRAN (R 4.1.0)                  
##  P gridExtra   * 2.3        2017-09-09 [?] CRAN (R 4.1.0)                  
##  P gtable        0.3.0      2019-03-25 [?] CRAN (R 4.1.0)                  
##  P haven         2.4.3      2021-08-04 [?] CRAN (R 4.1.0)                  
##  P highr         0.9        2021-04-16 [?] CRAN (R 4.1.0)                  
##  P hms           1.1.1      2021-09-26 [?] CRAN (R 4.1.0)                  
##  P htmltools     0.5.2      2021-08-25 [?] CRAN (R 4.1.0)                  
##  P httr          1.4.2      2020-07-20 [?] CRAN (R 4.1.0)                  
##  P igraph        1.2.6      2020-10-06 [?] CRAN (R 4.1.0)                  
##  P imager      * 0.42.10    2021-06-10 [?] CRAN (R 4.1.0)                  
##  P jpeg          0.1-9      2021-07-24 [?] CRAN (R 4.1.0)                  
##  P jquerylib     0.1.4      2021-04-26 [?] CRAN (R 4.1.0)                  
##  P jsonlite      1.7.2      2020-12-09 [?] CRAN (R 4.1.0)                  
##  P knitr         1.33       2021-04-24 [?] CRAN (R 4.1.0)                  
##  P labeling      0.4.2      2020-10-20 [?] CRAN (R 4.1.0)                  
##  P lattice       0.20-45    2021-09-22 [?] CRAN (R 4.1.0)                  
##  P lifecycle     1.0.1      2021-09-24 [?] CRAN (R 4.1.0)                  
##  P lubridate     1.7.10     2021-02-26 [?] CRAN (R 4.1.0)                  
##  P magrittr    * 2.0.1      2020-11-17 [?] CRAN (R 4.1.0)                  
##  P MASS          7.3-54     2021-05-03 [?] CRAN (R 4.1.1)                  
##  P modelr        0.1.8      2020-05-19 [?] CRAN (R 4.1.0)                  
##  P munsell       0.5.0      2018-06-12 [?] CRAN (R 4.1.0)                  
##  P pillar        1.6.3      2021-09-26 [?] CRAN (R 4.1.0)                  
##  P pixmap      * 0.4-12     2021-01-29 [?] CRAN (R 4.1.0)                  
##  P pkgconfig     2.0.3      2021-10-13 [?] Github (r-lib/pkgconfig@b81ae03)
##  P plyr          1.8.6      2020-03-03 [?] CRAN (R 4.1.0)                  
##  P png           0.1-7      2013-12-03 [?] CRAN (R 4.1.0)                  
##  P purrr       * 0.3.4      2020-04-17 [?] CRAN (R 4.1.0)                  
##  P R6            2.5.1      2021-08-19 [?] CRAN (R 4.1.0)                  
##  P Rcpp          1.0.7      2021-07-07 [?] CRAN (R 4.1.0)                  
##  P readbitmap    0.1.5      2018-06-27 [?] CRAN (R 4.1.0)                  
##  P readr       * 2.0.1      2021-08-10 [?] CRAN (R 4.1.1)                  
##  P readxl        1.3.1      2019-03-13 [?] CRAN (R 4.1.0)                  
##  P renv          0.14.0     2021-07-21 [?] CRAN (R 4.1.0)                  
##  P reprex        2.0.1      2021-08-05 [?] CRAN (R 4.1.0)                  
##  P RgoogleMaps   1.4.5.3    2020-02-12 [?] CRAN (R 4.1.0)                  
##  P rjson         0.2.20     2018-06-08 [?] CRAN (R 4.1.0)                  
##  P rlang         0.4.11     2021-04-30 [?] CRAN (R 4.1.0)                  
##  P rmarkdown     2.10       2021-08-06 [?] CRAN (R 4.1.1)                  
##  P rstudioapi    0.13       2020-11-12 [?] CRAN (R 4.1.0)                  
##  P rvest         1.0.1      2021-07-26 [?] CRAN (R 4.1.0)                  
##  P sass          0.4.0      2021-05-12 [?] CRAN (R 4.1.0)                  
##  P scales        1.1.1      2020-05-11 [?] CRAN (R 4.1.0)                  
##  P sessioninfo   1.1.1      2018-11-05 [?] CRAN (R 4.1.0)                  
##  P sp            1.4-5      2021-01-10 [?] CRAN (R 4.1.0)                  
##  P stringi       1.7.4      2021-08-25 [?] CRAN (R 4.1.1)                  
##  P stringr     * 1.4.0      2019-02-10 [?] CRAN (R 4.1.0)                  
##  P tibble      * 3.1.5      2021-09-30 [?] CRAN (R 4.1.0)                  
##  P tidyr       * 1.1.4      2021-09-27 [?] CRAN (R 4.1.0)                  
##  P tidyselect    1.1.1      2021-04-30 [?] standard (@1.1.1)               
##  P tidyverse   * 1.3.1      2021-04-15 [?] CRAN (R 4.1.0)                  
##  P tiff          0.1-8      2021-03-31 [?] CRAN (R 4.1.0)                  
##  P tzdb          0.1.2      2021-07-20 [?] CRAN (R 4.1.0)                  
##  P utf8          1.2.2      2021-07-24 [?] CRAN (R 4.1.0)                  
##  P vctrs         0.3.8      2021-04-29 [?] standard (@0.3.8)               
##  P vroom         1.5.4      2021-08-05 [?] CRAN (R 4.1.1)                  
##  P withr         2.4.2      2021-04-18 [?] CRAN (R 4.1.0)                  
##  P xfun          0.25       2021-08-06 [?] CRAN (R 4.1.1)                  
##  P xml2          1.3.2      2020-04-23 [?] CRAN (R 4.1.0)                  
##  P yaml          2.2.1      2020-02-01 [?] CRAN (R 4.1.0)                  
## 
## [1] /Users/runner/work/HDDA21/HDDA21/renv/library/R-4.1/x86_64-apple-darwin17.0
## [2] /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/RtmpxLydmZ/renv-system-library
## 
##  P ‚îÄ‚îÄ Loaded and on-disk path mismatch.</code></pre>
</details>
</div>

<div id="rmd-source-code">---
title: "2. Singular Value Decomposition"
author: "Lieven Clement"
date: "statOmics, Ghent University (https://statomics.github.io)"
output:
  pdf_document:
    toc: true
    number_sections: true
    latex_engine: xelatex
---

# Introduction

## Motivation

The SVD is one of the most well used and general purpose tools from linear algebra for data processing!

Methodologically

- Dimension reduction (e.g. images, gene expression data, movie preferences)
- Used as a first step in many data reduction and machine learning approaches
- Taylor a coordinate system driven by the data
- Solve system of linear equations for non-square matrices: e.g. linear regression
- Basis for principal component analysis (PCA) and multidimensional scaling (MDS).
  - PCA is one of the most widely used methods to study high dimensional data and to understand them in terms of their  dominant patterns and correlations

Applications:

- At the heart of search engines: Google
- Basis of many facial recognition methods: e.g. Facebook
- Recommender systems such as Amazon and Netflix
- A standard tool for data exploration and dimension reduction in Genomics

## Disclaimer

When you want to run the script you will have to comment out the eval=FALSE statement in some R chunks. Because the SVD takes a while on the faces example we save the svd for later use. So you have to comment the eval=FALSE statement in this chunk when you run the script for the first time.

## Data

- Extended Yale Face Database B
- Cropped and aligned images of 38 individuals under 64 lighting conditions.
- Each image is 192 pixels tall and 168 pixels wide.
- Each of the facial images in our library will be reshaped into a large vector with 192 × 168 = 32 256 elements.
- We will use the 64 images of 36 people to build our models

```{r libraries, message=FALSE, warning=FALSE, silent=TRUE}
library(pixmap)
library(tidyverse)
library(gridExtra)
library(grid)
library(ggmap)
library(downloader)
library(imager)
```

```{r download-data, warning=FALSE, message=FALSE}
## Download and unzip data
if(!dir.exists("raw-data")) dir.create("raw-data")
download(
  "https://github.com/statOmics/HDA2020/raw/data/yalefaces_cropped.zip",
  destfile = "raw-data/yalefaces_cropped.zip", mode = "wb", quiet = TRUE
)
unzip ("raw-data/yalefaces_cropped.zip", exdir = "./raw-data")

dir <- "./raw-data/CroppedYale"
```

```{r, warning=FALSE}
people <- list.files(dir)
people2 <- sapply(people,
  function(x) list.files(
    paste0(dir,"/",x),
    full.names=TRUE
    )
  )

facesList <- lapply(people2, function(x) read.pnm(x))

 grid.arrange(
  grobs=lapply(facesList[1+(0:35)*64],
     function(x) getChannels(x) %>%
        ggimage(.,coord_equal=TRUE)
        ),
  ncol=6)
```


## Method

Let $\mathbf{X}$ be an $n\times p$ matrix e.g.

- gene expression of $p=40 000$ genes for $n=30$ subjects
- n = 100 000 000 webpages indexed with p search terms, or
- $n$ images each stored as a $p=32 256$ vector with the intensity of each pixel

<!-- Need "emo" package for emojis, install with -->
<!-- `devtools::install_github("hadley/emo")` -->

__Note:__ the emoji characters will not be visible in the PDF output.

\[X=
\left[\begin{array}{ccc}
-&\mathbf{x}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{i}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{n}^T &- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
`r set.seed(1);emo::ji("beauty")`\\\\
`r set.seed(1);emo::ji("cop")`\\
\\
`r set.seed(1);emo::ji("blonde")`\\
\end{array}
\]

The data matrix  $\mathbf{X}$ can be decomposed with the SVD into 3 matrices:

\[
\mathbf{X}=\mathbf{U}_{n\times n}\boldsymbol{\Delta}_{n\times p}\mathbf{V}^T_{p \times p}
\]

- an orthonormal matrix $\mathbf{U}_{n\times n}$ with left singular vectors: $\mathbf{u}_j^T \mathbf{u}_k=1$ if $k=j$ and $\mathbf{u}_j^T \mathbf{u}_k=0$ if $j\neq k$, i.e.
\[ \mathbf{U}^T\mathbf{U}=\mathbf{I}\]

- a matrix $\boldsymbol{\Delta}_{n\times p}$ with only singular values: the singular values $\delta_i$ are the only non-zero elements of the matrix and are on the diagonal element $[\boldsymbol{\Delta}]_{ii}$. They are also organised so that $\delta_1 > \delta_2 > \ldots > \delta_r$.

- an orthonormal matrix $\mathbf{V}_{p\times p}$ with right singular vectors: $\mathbf{v}_j^T \mathbf{v}_k=1$ if $k=j$ and $\mathbf{v}_j^T \mathbf{v}_k=0$ if $j\neq k$ otherwise, i.e.
\[ \mathbf{V}^T\mathbf{V}=\mathbf{I}\]


Note, that there are only $r$ non-zero singular values, with $r$ the rank of matrix $X$: $r \leq \text{min}(n,p)$. So we have $k=1 \ldots r$ non-zero singular values. Hence, we can also rewrite the approximation by restricting us to the rank of matrix $\mathbf{X}$. Indeed, the n times p matrix $\boldsymbol\Delta$ only contains $r$ non-zero diagonal elements!

- So
\[
\mathbf{X}=\mathbf{U}_{n\times r}\boldsymbol{\Delta}_{r\times r}\mathbf{V}^T_{p \times r}
\]


\[
\left[\begin{array}{ccc}
-&\mathbf{x}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{i}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{n}^T &- \\
\end{array}\right]_{n \times p}
=
\left[\begin{array}{ccc}
\mid&&\mid\\
\mathbf{u}_1&\ldots&\mathbf{u}_r\\
\mid&&\mid
\end{array}\right]_{n \times r}
\left[\begin{array}{ccc}
\delta_1\\
&\ddots&\\
&&\delta_r\\
\end{array}\right]_{r \times r}
\left[\begin{array}{ccc}
\mid&&\mid\\
\mathbf{v}_1&\ldots&\mathbf{v}_r\\
\mid&&\mid\\
\end{array}
\right]^T_{p \times r}
\]

Also note that
\[
\mathbf{V}^T=\left[\begin{array}{ccc}
-&\mathbf{v}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{v}_{r}^T &- \\
\end{array}\right]_{r \times p}
\]


- For high dimensional data $p>>>n$ $\rightarrow$ $\text{max}(r)=n$ and

- equivalently for multivariate data with $n>p$ $\rightarrow$ $\text{max}(r)=p$


We can also rewrite the decomposition using the properties of matrix multiplication

\begin{eqnarray}
\mathbf{X} &=& \delta_1\left[
\begin{array}{c}
\mid\\
\mathbf{u}_1\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&
\mathbf{v}_1^T&
-
\end{array}
\right]\\\quad\\\quad
\end{array}
+ \ldots +
\delta_r\left[
\begin{array}{c}
\mid\\
\mathbf{u}_r\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&
\mathbf{v}_r^T&
-
\end{array}
\right]\\\quad\\\quad
\end{array}\\
\mathbf{X} &=& \sum_{k=1}^r \delta_k\mathbf{u}_k\mathbf{v}_k^T
\end{eqnarray}

- Because both $\mathbf{U}$ and $\mathbf{V}$ are orthonormal all their $r$ vectors are having unit length and they are thus reshaped by the singular values.

- Hence, the singular values determine the importance of the rank one matrices $\delta_k\mathbf{u}_k\mathbf{v}_k^T$ in the reconstruction of the matrix $\mathbf{X}$ and they are ordered so that $\delta_1 > \ldots > \delta_r$.

Note, that for symmetric matrices $\mathbf{X}$ $\longrightarrow$ $\mathbf{U} = \mathbf{V}$.

## Interpretation of singular vectors: face example

### Convert images to vectors
1. Convert images to vectors and store them as a matrix
    - We use an `sapply` loop to loop over all faces
    - We extract the grey intensities from the pictures
    - We convert the matrix in a long skinny vector (`c`)
    - We transpose the resulting matrix from sapply
```{r}
allFacesMx <- sapply(facesList,
                     function(x)
                        getChannels(x) %>% c
                     ) %>% t
dim(allFacesMx)
```

Save memory by removing facesList object
```
rm(facesList)
gc()
```

Hence we obtain a matrix for n = `r nrow(allFacesMx)` images with p = `r ncol(allFacesMx)` intensities for each pixel of an image.

Before we do the svd we typically center the data by substracting the average of the columns, i.e. the average face.

We will only work with the first 36 people: $n = 36 \times 64 = `r 36*64`$ pictures.

```{r}
allFacesCenteredMx <- allFacesMx[1:(36*64),]
meanFace <- colMeans(allFacesCenteredMx)

allFacesMxCentered <- allFacesCenteredMx -
  matrix(1, nrow=nrow(allFacesCenteredMx), ncol=1) %*% matrix(meanFace,nrow=1)
```

### Visualisation of mean image
```{r}
plotFaceVector <- function(faceVector,nrow=192,ncol=168) {
  matrix(faceVector,nrow=nrow,ncol=ncol) %>%
  ggimage()
}

meanFace %>%
  plotFaceVector
```

### SVD

#### Perform SVD in R

1. We adopt svd on the centered matrix
2. We cache the result because the calculation takes 10 minutes.

```{r run-SVD, cache=TRUE}
faceSvd <- svd(allFacesMxCentered)
```

<!-- ```{r, eval=FALSE} -->
<!-- ## Run this code manually to store the SVD for later re-use -->
<!-- saveRDS(faceSvd, file = "faceSvd.rds") -->
<!-- ``` -->

<!-- ```{r, eval=FALSE} -->
<!-- ## Run this code manually to reload the SVD result -->
<!-- faceSvd <- readRDS("faceSvd.rds") -->
<!-- ``` -->



#### SVD

Dimensions of $\mathbf{U}$, $\mathbf{V}$?

```{r}
n <- nrow(allFacesCenteredMx)
p <- ncol(allFacesCenteredMx)
dim(faceSvd$u)
dim(faceSvd$v)
```

Indeed, for the face example $n<p$ so $r=n$

Check orthogonality?

We do not do it for all vectors because it takes too long.
First left eigen vector and second left eigenvector.
Happens in $\mathbf{U}^T\mathbf{U}$
```{r}
t(faceSvd$u[,1])%*%faceSvd$u[,1]
t(faceSvd$u[,1])%*%faceSvd$u[,2]
t(faceSvd$u[,2])%*%faceSvd$u[,2]
```
So we see that the left eigenvectors are orthonormal.

We check if it also holds for the rows i.e. $\mathbf{U}\mathbf{U}^T$
```{r}
t(faceSvd$u[1,])%*%faceSvd$u[1,]
t(faceSvd$u[2,])%*%faceSvd$u[1,]
t(faceSvd$u[2,])%*%faceSvd$u[2,]
```
We also see that the rows of $\mathbf{U}$ are orthonormal.

```{r}
t(faceSvd$v[,1])%*%faceSvd$v[,1]
t(faceSvd$v[,1])%*%faceSvd$v[,2]
t(faceSvd$v[,2])%*%faceSvd$v[,2]
```
So we see that the right eigenvectors are orthonormal.

```{r}
t(faceSvd$v[1,])%*%faceSvd$v[1,]
t(faceSvd$v[1,])%*%faceSvd$v[2,]
t(faceSvd$v[2,])%*%faceSvd$v[2,]
```

This, however does not hold for the rows of $\mathbf{V}$.
This is because the matrix $\mathbf{V}$ no longer is a square matrix! $r=n$ and $r<p$!

#### Visualize right eigenvectors $\mathbf{V}$
```{r}
grid.arrange(
 grobs=apply(
   faceSvd$v[,1:36],
   2,
   plotFaceVector
   )
 )
```

- Hence, the right singular vectors (in $\mathbf{V}$ of $\mathbf{X}=\mathbf{U}\boldsymbol{\Delta}\mathbf{V}$) are also faces and we can thus reconstruct the original faces by linear combinations of the eigen faces.

- The first eigen faces are most important to capture overall patterns in the matrix.

- Here it are mainly characteristics and shadows that are important for all faces.

- From eigen face 5 onwards we start to see specific features.

- In this case: $n < p$, so $r = n$.

\[
\mathbf{X}_{n\times p}=\mathbf{U}_{n \times n}\boldsymbol{\Delta}_{n\times n}\mathbf{V}_{p\times n}^T
\]

\[
\begin{array}{ccccc}
\left[\begin{array}{ccc}
-&\mathbf{x}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{i}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{n}^T &- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
`r set.seed(1);emo::ji("beauty")`\\\\
`r set.seed(1);emo::ji("cop")`\\
\\
`r set.seed(1);emo::ji("blonde")`\\
\end{array}
&=&
\begin{array}{c}
\quad\\
\left[\begin{array}{ccc}
\mid&&\mid\\
\mathbf{u}_1&\ldots&\mathbf{u}_n\\
\mid&&\mid
\end{array}\right]_{n \times n}\\
\quad \\
\end{array}
\begin{array}{c}
\quad\\
\left[\begin{array}{ccc}
\delta_1\\
&\ddots&\\
&&\delta_n\\
\end{array}\right]_{n \times n}\\
\quad\\
\end{array}
\begin{array}{c}
\quad
\left[\begin{array}{ccc}
\mid&&\mid\\
\mathbf{v}_1&\ldots&\mathbf{v}_n\\
\mid&&\mid\\
\end{array}
\right]^T_{p \times n}\\
\begin{array}{ccc}
`r set.seed(1);emo::ji("fear")`&\quad&`r set.seed(1);emo::ji("meh")`
\end{array}
\end{array}
\end{array}
\]

- Or upon transposing the matrix $\mathbf{V}$

\[
\begin{array}{ccccc}
\left[\begin{array}{ccc}
-&\mathbf{x}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{i}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{n}^T &- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
`r set.seed(1);emo::ji("beauty")`\\\\
`r set.seed(1);emo::ji("cop")`\\
\\
`r set.seed(1);emo::ji("blonde")`\\
\end{array}
&=&
\left[\begin{array}{ccc}
\mid&&\mid\\
\mathbf{u}_1&\ldots&\mathbf{u}_n\\
\mid&&\mid
\end{array}\right]_{n \times n}
\left[\begin{array}{ccc}
\delta_1\\
&\ddots&\\
&&\delta_r\\
\end{array}\right]_{n \times n}
\left[\begin{array}{ccc}
-&\mathbf{v}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{v}_{p}^T &- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
`r set.seed(1);emo::ji("fear")`\\
\\
`r set.seed(1);emo::ji("meh")`
\end{array}
\end{array}
\]

#### Reconstruction of faces via linear combination of eigen faces.

In left singular vectors $u_{ij}$ we quantify the contribution of the $j^\text{th}$ eigenface in the reconstruction of face $i$ and we rescale the importance of each eigen face by its corresponding eigen value $\delta_j$.

\[
\left[\begin{array}{ccc}
-&\mathbf{x}_{1}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{i}^T &- \\
\vdots&\vdots&\vdots\\
-&\mathbf{x}_{n}^T &- \\
\end{array}\right]_{n \times p}
\begin{array}{c}
`r set.seed(1);emo::ji("beauty")`\\\\
`r set.seed(1);emo::ji("cop")`\\
\\
`r set.seed(1);emo::ji("blonde")`\\
\end{array} =
\delta_1\left[
\begin{array}{c}
\mid\\
\mathbf{u}_1\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&
\mathbf{v}_1^T&
-
\end{array}
\right]\\\quad\\\quad
\end{array}
\begin{array}{c}
`r set.seed(1);emo::ji("fear")`
\\\quad\\\quad
\end{array}
+ \ldots +
\delta_r\left[
\begin{array}{c}
\mid\\
\mathbf{u}_r\\
\mid
\end{array}
\right]
\begin{array}{c}
\left[
\begin{array}{ccc}
-&
\mathbf{v}_r^T&
-
\end{array}
\right]\\\quad\\\quad
\end{array}
\begin{array}{c}
`r set.seed(1);emo::ji("meh")`
\\\quad\\\quad
\end{array}
\]

If we truncate the eigen faces say at $k<r$ we can approximate faces using a limited number of eigen faces!

```{r fig.cap='approximation with 25 (top left),  100 (top right) and 500 (bottom left) eigenfaces and original face (bottom right, or with all eigenfaces)'}
approximateFace <- function(meanFace,faceSvd,k){
  reconstruct <- (meanFace + faceSvd$u[1,1:k] %*%
    diag(faceSvd$d[1:k]) %*%
  t(faceSvd$v[,1:k]) %>%
  c)
}

approxHlp <- sapply(
      c(25,100,500),
      approximateFace,
      meanFace=meanFace,
      faceSvd=faceSvd)

grid.arrange(
 grobs=apply(
   cbind(
     approxHlp,
     allFacesMxCentered[1,]+meanFace
   ),
   2,
   plotFaceVector
   )
 )

```

# SVD as a Matrix Approximation Method

- We have seen that we can use the truncted SVD to approximate matrix $\mathbf{X}$ by $\tilde{\mathbf{X}}$, with $k<r$ and
\[
\tilde{\mathbf{X}}=\mathbf{U}_{n\times k}\boldsymbol{\Delta}_{k\times k}\mathbf{V}_{p \times k}^T
\]
- It can be shown that **SVD: optimal approximation**

  - Let $\mathbf{X}$ be an $n\times p$ matrix of rank $r\leq \min(n,p)$, and let $\mathbf{A}$ denote an $n \times p$ matrix of rank $k\leq r$, with elements denoted by $a_{ij}$.

  - The matrix $\mathbf{A}$ of rank $k\leq r$ that minimises the Frobenius norm
  \[
    \vert\vert\mathbf{X}-\mathbf{A}\vert\vert^2_\text{fr}=\sum_{i=1}^n\sum_{j=1}^p (x_{ij}-a_{ij})^2
  \]
  is given by the truncated SVD
  \[
     \mathbf{X}_k = \sum_{j=1}^k \delta_j \mathbf{u}_j\mathbf{v}_j^T.
  \]

  - The truncated SVD has $k < r$ terms. Hence, generally $\mathbf{X}_k$ does not coincide with $\mathbf{X}$. It is considered as an approximation.

  - Note, that the truncated SVD thus approximates the matrix by minimising a kind of sum of least squared errors between the elements of matrix $\mathbf{X}$ and $\mathbf{A}$ and that

  - the truncated SVD $\mathbf{X}_k$ is the best rank-k approximation of $\mathbf{X}$ in terms of this
Frobenius norm.

  - Also, note that upon truncation
  \[\mathbf{V}^T_{p\times k} \mathbf{V}_{p\times k} = \mathbf{I}_{k\times k}\]
  \[\mathbf{U}^T_{n\times k} \mathbf{U}_{n\times k} = \mathbf{I}_{k\times k}\]

  - But, that
\[\mathbf{V}_{p\times k} \mathbf{V}_{p\times k}^T \neq \mathbf{I}_{p\times p}!!!\]
\[\mathbf{U}_{n\times k} \mathbf{U}_{n\times k}^T \neq \mathbf{I}_{n\times n}!!!\]

---


Some **informal statement** about the truncated SVD
  \[
     \mathbf{X}_k = \sum_{j=1}^k \delta_j \mathbf{u}_j\mathbf{v}_j^T.
  \]

 - It can be considered as a weighted sum of matrices $\mathbf{u}_j\mathbf{v}_j^T$, with weights $\delta_j$.


 - The terms are ordered with decreasing weights $\delta_1\geq \delta_2 \geq \cdots \geq \delta_k >0$.


 - The matrices $\mathbf{u}_j\mathbf{v}_j^T$ are of equal "magnitude" (constructed from normalised vectors).

 - Truncation at $k$ results in $k$ $\delta_j$'s, $k\times n$ elements in the $\mathbf{u}_j$ and $k \times p$ elements in the $\mathbf{v}_j$. Hence a total of $k+kn+kp=k(1+n+p)$ elements (usually much smaller than $np$). (Note that restrictions apply to $\mathbf{u}_j$ and $\mathbf{v}_j$; hence even less independent elements).

 $\longrightarrow$ **data compression**

## Example 1: Image compression

### Painting Mondriaan: Composition_No.III with red, blue, yellow and black (1929).

- Have a look at this painting of Mondriaan (1872 -- 1944), here shown in black-and-white.

#### Load the original painting

1. fetch image from the web
2. convert into greyscale
3. plot
4. save as Matrix

```{r}
mondriaan <- load.image("https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Piet_Mondrian_-_Composition_No._III%2C_with_red%2C_blue%2C_yellow_and_black%2C_1929.jpg/1920px-Piet_Mondrian_-_Composition_No._III%2C_with_red%2C_blue%2C_yellow_and_black%2C_1929.jpg")
mondriaan <- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)
X <- matrix(as.data.frame(mondriaan)[,3],nrow=nrow(mondriaan),ncol=ncol(mondriaan))
```

- This picture can be represented as a $`r nrow(mondriaan)` \times `r ncol(mondriaan)``$ matrix $\mathbf{X}$ with gray scale intensities $\in [0,1]$. ($\approx 4\times 10^6$ data entries)

- We will here not transform the image in a vector, but will look at the performance of the SVD to compress this image. The SVD can be applied to any matrix!


#### Singular values

```{r}
monSvd <- svd(X)

p1 <- data.frame(x=1:length(monSvd$d),y=monSvd$d) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

p2 <- data.frame(x=1:10,y=monSvd$d[1:10]) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

grid.arrange(p1,p2,nrow=1)
```

- The singular values decay very quickly!

#### Data compression

- We make the plot for a reconstruction with 1 singular vector. This leads to a data compression of $1-\frac{(1+`r nrow(mondriaan)`+`r ncol(mondriaan)`)}{`r nrow(mondriaan) `\times `r ncol(mondriaan)`}$ = `r 100 - round((1+nrow(mondriaan)+ncol(mondriaan))/(nrow(mondriaan)*ncol(mondriaan))*100,1)`%. We only use 1 left singular vector (`r nrow(mondriaan)`), 1 eigen value, 1 right singular vector (`r ncol(mondriaan)`).


```{r}
k <- 1
approxMon <- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])

approxMon[approxMon < 0] <- 0
approxMon[approxMon > 1] <- 1

as.cimg(approxMon) %>%
  plot(.,main=paste0("Approximation with ",k," singular vectors"),axes=FALSE)
```

- We make the plot for a reconstruction with 2 singular vector. This leads to a data compression of $1-\frac{2\times (1+`r nrow(mondriaan)`+`r ncol(mondriaan)`)}{`r nrow(mondriaan) `\times `r ncol(mondriaan)`}$ = `r 100 - round(2*(1+nrow(mondriaan)+ncol(mondriaan))/(nrow(mondriaan)*ncol(mondriaan))*100,1)`%. We only use 2 left singular vectors
(2 $\times$ `r nrow(mondriaan)`), 2 singular values, 2 right singular vectors (2 $\times$ `r ncol(mondriaan)`).

```{r}
k <- 2
approxMon <- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])

approxMon[approxMon < 0] <- 0
approxMon[approxMon > 1] <- 1

as.cimg(approxMon) %>%
  plot(.,main=paste0("Approximation with ",k," singular vectors"),axes=FALSE)
```


```{r}
par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1:8))
{
approxMon <- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon < 0] <- 0
approxMon[approxMon > 1] <- 1


approxMon %>%
  as.cimg %>%
  plot(.,main=paste0(k," singular vectors"),axes=FALSE)
}
plot(as.cimg(X),main=paste0("Original image"),axes=FALSE)
```

### More complex painting: Composition A, Piet Mondriaan

#### Load the original painting

```{r}
mondriaan <- load.image("https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Composition_A_by_Piet_Mondrian_Galleria_Nazionale_d%27Arte_Moderna_e_Contemporanea.jpg/1920px-Composition_A_by_Piet_Mondrian_Galleria_Nazionale_d%27Arte_Moderna_e_Contemporanea.jpg")
mondriaan <- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)
X <- matrix(as.data.frame(mondriaan)[,3],nrow=dim(mondriaan)[1],ncol=dim(mondriaan)[2])
```


#### Singular values

```{r, cache=TRUE}
monSvd <- svd(X)
```

```{r}
p1 <- data.frame(x=1:length(monSvd$d),y=monSvd$d) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

p2 <- data.frame(x=1:10,y=monSvd$d[1:10]) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

grid.arrange(p1,p2,nrow=1)
```

- The singular values decay a bit slower. The painting is a bit more complex. More lines and colors.

#### Evaluate data compression

```{r}
par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1,seq(3,21,3)))
{
approxMon <- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon < 0] <- 0
approxMon[approxMon > 1] <- 1


approxMon %>%
  as.cimg %>%
  plot(.,main=paste0(k," singular vectors"),axes=FALSE)
}
plot(as.cimg(X),main=paste0("Original image"),axes=FALSE)
```

### Self portret Piet Mondriaan

#### Load the painting

```{r}
mondriaan <- load.image("https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Mondrian_Zelfportret.jpg/1920px-Mondrian_Zelfportret.jpg")
mondriaan <- grayscale(mondriaan)
plot(mondriaan,axes=FALSE)
X <- matrix(as.data.frame(mondriaan)[,3],nrow=dim(mondriaan)[1],ncol=dim(mondriaan)[2])
```

#### Singular values

```{r, cache=TRUE}
monSvd <- svd(X)
```

```{r}
p1 <- data.frame(x=1:length(monSvd$d),y=monSvd$d) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

p2 <- data.frame(x=1:10,y=monSvd$d[1:10]) %>%
  ggplot(aes(x=x,y=y)) +
  geom_point() +
  xlab("k") +
  ylab("singular value")

grid.arrange(p1,p2,nrow=1)
```

- The singular values decay much slower. The painting is more complex.

#### Evaluate compression

```{r}
par (mfrow=c(3,3))
par(mar=c(1,2,1,1))
for (k in c(1,5,10,20,30,40,50,100))
{
approxMon <- monSvd$u[,1:k] %*%
  diag(monSvd$d[1:k],ncol=k) %*%
  t(monSvd$v[,1:k])


approxMon[approxMon < 0] <- 0
approxMon[approxMon > 1] <- 1


approxMon %>%
  as.cimg %>%
  plot(.,main=paste0(k," singular vectors"),axes=FALSE)
}
plot(as.cimg(X),main=paste0("Original image"),axes=FALSE)
```



Here we need at least 40 singular vector. This leads to a data compression of $1-\frac{40\times (1+`r nrow(mondriaan)`+`r ncol(mondriaan)`)}{`r nrow(mondriaan)` \times `r ncol(mondriaan)`}$ = `r 100 - round(40*(1+nrow(mondriaan)+ncol(mondriaan))/(nrow(mondriaan)*ncol(mondriaan))*100,1)`%. We only use 40 left singular vectors ($40 \times `r nrow(mondriaan)`$), 40 singular values, 40 right eigens vector ($40\times `r ncol(mondriaan)`$).

# Geometric interpretation

Write the **truncated SVD** as
\[
  \mathbf{X}_k = \mathbf{U}_k \boldsymbol{\Delta}_k \mathbf{V}_k^T = \mathbf{Z}_k \mathbf{V}_k^T
\]
with
\[
  \mathbf{Z}_k = \mathbf{U}_k \boldsymbol{\Delta}_k
\]
an $n \times k$ matrix.

Each of the $n$ rows of $\mathbf{Z}_k$, say $\mathbf{z}^T_{k,i}$, represents a point in a $k$-dimensional space.


Because of the orthonormality of the singular vectors, we also have
\begin{eqnarray*}
  \mathbf{X}_k\mathbf{V}_k &=& \mathbf{Z}_k \mathbf{V}_k^T\mathbf{V}_k \\
  \mathbf{X}_k\mathbf{V}_k &=& \mathbf{Z}_k.
\end{eqnarray*}

Thus the matrix $\mathbf{V}_k$ is a **transformation matrix** that may be used to transform $\mathbf{X}_k$ into $\mathbf{Z}_k$, and $\mathbf{Z}_k$ into $\mathbf{X}_k$.

---

Note that

- The matrix $\mathbf{V}_k$ transforms the $p$-dimensional $\mathbf{X}_k$ into the $k$-dimensional $\mathbf{Z}_k$: $\mathbf{Z}_k = \mathbf{X}_k\mathbf{V}_k$. Note, however, that the matrix $\mathbf{X}_k$ must not necessarily be used for this transformation, because the SVD of the original matrix $\mathbf{X}$ also gives directly $\mathbf{Z}_k = \mathbf{U}_k \boldsymbol{\Delta}_k$.

- The inverse transformation from the  $k$-dimensional $\mathbf{Z}_k$ to the $p$-dimensional $\mathbf{X}_k$ is given by the transpose of $\mathbf{V}_k$: $\mathbf{Z}_k \mathbf{V}_k^T=\mathbf{X}_k$. Often inverse transformations are given by the inverse of a matrix, but thanks to the orthonormality of the columns of $\mathbf{V}_k$, we get $\mathbf{V}_k^T\mathbf{V}_k=\mathbf{I}$, and thus $\mathbf{V}_k^T$ acts as an inverse.

- The transformation from the  $k$-dimensional $\mathbf{Z}_k$ to the $p$-dimensional $\mathbf{X}_k$ is transforming points from a low dimensional space ($k$) to a high dimensional space ($p$). You may not interpret this as if this transformation adds information; the transformed points in $\mathbf{X}_k$ still live in a $k$-dimensional subspace of the larger $p$-dimensional space; the matrix $\mathbf{X}_k$ is only of rank $k$ and thus contains less information than the original data matrix $\mathbf{X}$ (if rank($\mathbf{X}$)$=r>k$).

---

More importantly, it can be shown that (thanks to orthonormality of $\mathbf{V}$)
  \[
     \mathbf{X}\mathbf{V}_k = \mathbf{Z}_k.
  \]
  This follows from (w.l.g. rank($\mathbf{X}$)=$r$)
  \begin{eqnarray*}
    \mathbf{X}\mathbf{V}_k
       &=& \mathbf{UDV}^T\mathbf{V}_k = \mathbf{UD}\begin{pmatrix}
       	        \mathbf{v}_1^T \\
	        \vdots \\
	        \mathbf{v}_r^T
               \end{pmatrix}
               \begin{pmatrix}
                 \mathbf{v}_1 \ldots \mathbf{v}_k
               \end{pmatrix} \\
       &=& \mathbf{UDV}^T\mathbf{V}_k = \mathbf{UD}\begin{pmatrix}
                1 & 0 & \ldots & 0 \\
                0 & 1 & \ldots & 0 \\
                \vdots & \vdots & \ddots & 0 \\
                0 & 0 & \ldots & 1 \\
                0 & 0 & \ldots & 0 \\
                \vdots & \vdots & \vdots & \vdots \\
                0 & 0 & \ldots & 0
               \end{pmatrix} \
               = \mathbf{U}_k\boldsymbol{\Delta}_k = \mathbf{Z}_k
  \end{eqnarray*}

  The $p \times k$ matrix $\mathbf{V}_k$ acts as a transformation matrix: transforming $n$ points in a $p$ dimensional space to $n$ points in a $k$ dimensional space.

---

We take a closer look at
\[
  \mathbf{Z}_k = \mathbf{X}\mathbf{V}_k = \begin{pmatrix}
   \mathbf{x}_1^T \\
   \vdots \\
   \mathbf{x}_n^T
  \end{pmatrix} \begin{pmatrix}
   \mathbf{v}_1 \ldots \mathbf{v}_k
  \end{pmatrix}.
\]
The $i$th row (observation) in $\mathbf{Z}_k$ equals
\[
 \mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k = \left(\mathbf{x}_i^T \mathbf{v}_1 , \mathbf{x}_i^T \mathbf{v}_2 , \ldots , \mathbf{x}_i^T \mathbf{v}_k\right).
\]

Hence, $\mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k$ is the orthogonal projection of $\mathbf{x}_i$ onto the $k$-dimensional subspace spanned by the columns of $\mathbf{V}_k$.

SVD transforms data set to lower dimensional data set:
The SVD thus gives a transformation of the $p$ dimensional data to $k\leq r$ dimensional data:
\[
  \mathbf{Z}_k = \mathbf{X}\mathbf{V}_k.
\]
This is essentially a dimension reduction.

----

Note that,

- The transformation from $p$-dimensional $\mathbf{X}$ to $k$-dimensional $\mathbf{Z}_k$ is important. It shows that the $n$ points in the rows of $\mathbf{Z}_k$ are the result of projecting the $n$ points in $\mathbf{X}$ onto the columns of $\mathbf{V}_k$ (i.e. the first $k$ singular vectors of $\mathbf{X}$). We say that the space of $\mathbf{Z}_k$ is spanned by the column of $\mathbf{V}_k$.

- The points (rows) in $\mathbf{X}$ live in a $p$-dimensional space (or rank($\mathbf{X}$)$=r$ if $r<p$) and they are thus projected onto a lower dimensional space. This is in contrast to the projection $\mathbf{X}_k\mathbf{V}_k=\mathbf{Z}_k$, because the points in $\mathbf{X}_k$ live in a $k$-dimensional subspace of $\mathbf{X}$.

- Note that with $k<r$ there is no unique transformation to transform $\mathbf{Z}_k$ back to $\mathbf{X}$. On the previous slide we only established the transformation $\mathbf{Z}_k \mathbf{V}_k^T=\mathbf{X}_k$. Indeed, starting from $\mathbf{X}\mathbf{V}_k = \mathbf{Z}_k$, and right-multiplying with $\mathbf{V}_k^T$ does not give the backtransformation, because $\mathbf{V}_k\mathbf{V}_k^T$ is not the identity matrix.


# Interpretation of SVD in terms of correlation matrices

For a matrix $\mathbf X$ the sample variance covariance matrix estimator is $p\times p$ matrix

\begin{eqnarray}
\mathbf{S}&=&\frac{1}{N-1}(\mathbf{X}-\bar{\mathbf{X}})^T (\mathbf{X}-\bar{\mathbf{X}})\\
&=&\frac{1}{N-1}\left[\mathbf{X}^T\mathbf{X} - \bar{\mathbf{X}}^T\bar{\mathbf{X}}\right]
\end{eqnarray}

So $\mathbf{X}^T\mathbf{X}$ defines up to a constant the variance covariance matrix of $\mathbf{X}$! When the matrix is column centered $\mathbf{S}=\frac{1}{n-1}\mathbf{X}^T\mathbf{X}$.

The same holds for the rows of $\mathbf{X}$! The covariance between the subjects can be estimated as \[\mathbf{S}=\frac{1}{p-1}\mathbf{X}\mathbf{X}^T\] upon row centering.

Note, that
\begin{eqnarray}
\mathbf{X}^T\mathbf{X} &=& \mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T \\
&=&\mathbf{V}\boldsymbol{\Delta}^2 \mathbf{V}^T
\end{eqnarray}

If we rewrite the expression
\begin{eqnarray}
\mathbf{X}^T\mathbf{X}\mathbf{V} &=&\mathbf{V}\boldsymbol{\Delta}^2 \mathbf{V}^T\mathbf{V}\\
&=&\mathbf{V}\boldsymbol{\Delta}^2
\end{eqnarray}

So, if the data are centered, the SVD can be used to perform a spectral decomposition of the sample covariance matrix where the right singular vectors correspond to the eigen vectors of the covariance matrix and the eigenvalues are the squared singular values!

Similarly the left singular values can be used to estimate the covariance matrix of the rows of $\mathbf{X}$. So in our notation covariance between subjects.

\begin{eqnarray}
\mathbf{X}\mathbf{X}^T &=&\mathbf{U}\boldsymbol{\Delta}^2 \mathbf{U}^T
\end{eqnarray}

This link is for instance very useful for recommender systems, i.e. to propose movies based on the subjects with whom you correlate. We will also exploit this when we discuss on PCA.

# SVD and inverse of a matrix

A linear system of equations with $n$ equations and $n$ unknowns
\[
\mathbf{A}_{n\times n} \boldsymbol{\beta} = \mathbf{b}
\]
can be solved by
\[
\boldsymbol{\beta} = \mathbf{A}_{n\times n}^{-1} \mathbf{b}
\]

A unique solution exists if A is full rank.

Note, that a singular value decomposition of the square matrix $\mathbf{A}=\mathbf{V}\boldsymbol{\Delta}\mathbf{V}^T$ enables the inverse to be written as

\[
\mathbf{A}^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{V}^T
\]

indeed
\[
\mathbf{A}^{-1} \mathbf{A} = \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{V}^T = \mathbf{I}
\]

Note, that the SVD generalizes this to systems of under (n<p, fat short matrices)
 and over determined systems (n>p tall skinny matrices):

Let
\begin{eqnarray}
\mathbf{A} = \mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T
\end{eqnarray}

and we want to solve
\begin{eqnarray}
\mathbf{A} \boldsymbol{\beta} &=& \mathbf{b}\\
\mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &=& \mathbf{b}\\
\mathbf{U}^T\mathbf{U}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &=& \mathbf{U}^T\mathbf{b}\\
\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &=& \mathbf{U}^T\mathbf{b}\\
\boldsymbol{\Delta}^{-1}\boldsymbol{\Delta}\mathbf{V}^T \boldsymbol{\beta} &=& \boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}\\
\mathbf{V}\mathbf{V}^T \boldsymbol{\beta} &=& \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}\\
\boldsymbol{\beta} &=& \mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{b}
\end{eqnarray}

Note, that for an overdetermined system $n>p$ so $r\leq p$.
Generally, $r=p$ and $\mathbf{V}$ is thus a square matrix so both $\mathbf{V}^T\mathbf{V}=\mathbf{I}$ and  $\mathbf{V}\mathbf{V}^T=\mathbf{I}$. However, $\mathbf{U}^T\mathbf{U}=\mathbf{I}$ but  $\mathbf{U}\mathbf{U}^T\neq\mathbf{I}$ because $r<n$.

$\mathbf{A}^\dagger=\mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T$ is also referred to as the pseudo inverse and it enables us to solve under and overdetermined systems of equations.

Note, that for

- underdetermined systems there typically does not exist a unique solution
- for overdetermined systems usually there does not exist an exact solution. We will focus on the latter in the next section where we explore the link between linear regression and SVD.

# Linear regression and SVD

Suppose we have the linear regression problem with $n>p$:

\[\mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]

$\mathbf{X}$ is a tall skinny matrix with $n >> p$.

We know that
\[
\hat{\boldsymbol{\beta}}=\left(\mathbf{X}^T \mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}
\]

If we replace $\mathbf{X}$ by its SVD

\begin{eqnarray}
\hat{\boldsymbol{\beta}}&=&\left(\mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T\right)^{-1}\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
\hat{\boldsymbol{\beta}}&=&\mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
\hat{\boldsymbol{\beta}}&=&\mathbf{V}\boldsymbol{\Delta}^{-1}\mathbf{U}^T\mathbf{Y}
\end{eqnarray}

So the SVD also solves the linear regression problem by using the pseudoinverse!
If we now think about the fit:

\begin{eqnarray}
\hat{\mathbf{Y}}&=&\mathbf{X}\hat{\boldsymbol{\beta}}\\
&=&\mathbf{U}\boldsymbol{\Delta}^{-1}\mathbf{V}^T\mathbf{V}\boldsymbol{\Delta}\mathbf{U}^T\mathbf{Y}\\
&=&\mathbf{U}\mathbf{U}^T\mathbf{Y}
\end{eqnarray}

- For an overdetermined system $\mathbf{U}\mathbf{U}^T$ is not equal to the unity matrix $\mathbf{I}$ (for an overdetermined system only $\mathbf{U}^T\mathbf{U}=\mathbf{I}$ because $n>r$).

- So $\hat{\mathbf{Y}}\neq \mathbf{Y}$. Hence, we typically do not have an exact solution.

- Note, that $\mathbf{U}\mathbf{U}^T$ spans the same space as the columns of $\mathbf{X}$, and will define the same $p$-dimensional plane in the $n$ dimensional space $\mathcal{R}^n$, e.g. cfr $\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T$. So it projects $\mathbf{Y}$ in the column space of $\mathbf{X}$ and the errors will be orthogonal onto this plane.

## Example prostate dataset
### Fit with lm

```{r}
prostate <- read_csv(
  "https://raw.githubusercontent.com/GTPB/PSLS20/master/data/prostate.csv",
  col_types = cols()
)
lm1 <- lm(lpsa ~ lcavol + lweight + svi, prostate)
```

### Fit with SVD

```{r}
X <- prostate[,c(1:2,5)]
X[,3] <- as.double(X[,3]!="healthy")
X <- cbind(Intercept=1,X)

svdX <- svd(X)
betaSvd <- svdX$v %*% diag(1/svdX$d) %*% t(svdX$u) %*% prostate$lpsa

cbind(lm1$coef,betaSvd)
```

# SVD and Multi-Dimensional Scaling (MDS)

## Example

In this section we will use a dataset on food consumption in the UK. The data originate from the UKs ‘Department for Environment, Food and Rural Affairs’ (DEFRA), showing the consumption in grams (per person, per week) of 17 different types of foodstuff measured and averaged in the four countries of the United Kingdom in 1997. We would like to explore the data and interpret how the food patterns of the different countries differ.

```{r}
uk <- read_csv(
  "https://raw.githubusercontent.com/statOmics/HDA2020/data/ukFoods.csv",
  col_types = cols()
)
knitr::kable(uk, caption = "The full UK foods data table")
```

Note, that here the matrix is displayed with the p variables in the rows and the n experimental units (countries) in the columns.
This is often done for high dimensional data where $p>>n$ because this makes it easier to look at to the raw data table.
Note, that the svd calculates left and right singular vectors so it will also provide the correct solution, we just should look to the other set of singular vectors in order to get to the correct interpretation.

## Motivation

The objective of Multidimensional Scaling (MDS) is to find a low-dimensional representation, say $k$-dimensional, of $n$ data points such that the distances between the $n$ points in the $k$-dimensional space is a good approximation of a given squared distance matrix, say $\mathbf{D}_X$.

- The squared distance matrix $\mathbf{D}_X$ may be given without knowledge of the original observations (not even the dimensionality), or it may be computed from a given set of $n$ $p$-dimensional data points.

- Note that the distances between points in a $k$-dimensional subspace coincide with the distances between these points in the larger $p$-dimensional space.

Use of MDS:

- A high dimensional data matrix $\mathbf{X}$ is given, and one wants to get a visual representation (in 2 or 3 dimensions) of the observations. In this graph each point represents an observation (row of data matrix). From this graph one wants points close to one another to be similar, and observations far away from one another to be dissimilar. Thus the distances between the $n$ points in the original $p$-dimensional space should be well preserved in the 2 or 3 dimensional space.

- In some applications the researcher only has knowledge of the similarity (or dissimilarity) between observations. For example, food products can be evaluated by a taste panel and a dissimilarity matrix can be completed. This dissimilarity matrix shows for each pair of food products their dissimilarity (numerical values provided by taste panel, but not objectively quantified). Given this dissimilarity matrix, a 2 or 3 dimensional graph could be helpful if the distances between the points (food products) in this graph are monotonically related to the dissimilarities provided by the taste panel. Food products close to one another in this graph, taste similarly.

- Here we discuss the "metric" MDS, which actually requires the Euclidean distances between observations. However, the method works also well if the matrix $\mathbf{D}_X$ contains dissimilarities rather than squared Euclidean distances.


---

In this chapter we assume that the data matrix $\mathbf{X}$ is column-centered (i.e. each column has mean zero).

Centering can be accomplished by multiplying the original data matrix $\mathbf{X}$ with the $n \times n$ $\mathbf{centering\ matrix}$
 \[
   \mathbf{H} = \mathbf{I} - \frac{1}{n} \mathbf{1}\mathbf{1}^T ,
 \]
 in which $\mathbf{1}$ is an $n$-vector with all entries equal to 1.
 Hence, $\mathbf{H}\mathbf{X}$ is the column-centered data matrix.

We will assume that $\mathbf{X}$ is already column-centered, and therefore $\mathbf{H}\mathbf{X}=\mathbf{X}$.

(Note, that the matrix $\mathbf{1}\mathbf{1}^T$ is an $n\times n$ matrix with all entries set to 1.
)

---

We will need the following interesting relationship between the **Gram matrix** $\mathbf{X}\mathbf{X}^T$ and the distance matrix.

- For an $n\times p$ data matrix $\mathbf{X}$, the matrix $\mathbf{D}_X$ with squared distances has elements
   \[
      (\mathbf{x}_{i}-\mathbf{x}_{j})^T(\mathbf{x}_{i}-\mathbf{x}_{j}) = \Vert \mathbf{x}_i\Vert^2 - 2 \mathbf{x}^T_i\mathbf{x}_j + \Vert \mathbf{x}_j\Vert.^2
   \]


-The $n\times n$ squared distance matrix can then be written as
   \[
     \mathbf{D}_X = \mathbf{N} - 2\mathbf{X}\mathbf{X}^T + \mathbf{N}^T ,
   \]
   with $\mathbf{N}$ the $n \times n$ matrix with $i$th row filled with $\Vert \mathbf{x}_i\Vert^2$.

- Note that the elements of $\mathbf{N}$ can also be found on the diagonal of $\mathbf{XX}^T$.

- Given the structure of the $\mathbf{H}$ and $\mathbf{N}$ matrices, it is easy to verify that
   \[
      -\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T.
   \] This gives an important relation between the distance matrix and the Gram matrix. We will use the notation $\mathbf{G}_X = -\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H}$.

-  The $n \times n$ Gram matrix $\mathbf{X}\mathbf{X}^T$  equals
\[
  \begin{pmatrix}
   \mathbf{x}_1^T \\
   \vdots  \\
   \mathbf{x}_n^T
  \end{pmatrix}
  \begin{pmatrix}
    \mathbf{x}_1 \ldots \mathbf{x}_n
  \end{pmatrix}
\]
and
has thus on its $(i,j)$th position the inner product
\[
   \mathbf{x}_i^T\mathbf{x}_j= \Vert \mathbf{x}_i\Vert\Vert \mathbf{x}_j\Vert \cos<\mathbf{x}_i,\mathbf{x}_j> .
\]

- On the diagonal we find $\mathbf{x}_i^T\mathbf{x}_i=\Vert \mathbf{x}_i\Vert^2$, the squared norm of the $i$th observation.

- The relation $-\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T$ tells us that the distance matrix and the Gram matrix contain the same information. The distances, however, are in most situations easier to interpret.

---

## Link with the SVD

Note, that we have shown that we can rewrite $\mathbf{X}\mathbf{X}^T$ using the SVD:

\[
\mathbf{X}\mathbf{X}^T = \mathbf{U}\boldsymbol{\Delta}^2\mathbf{U}^T
\]

Because the truncated SVD of $\mathbf{X}$ minimises the Frobenius norm $\Vert \mathbf{X}-\mathbf{X}_k\Vert_F^2$, it this has an important consequence:

- Let $\mathbf{D}_X$ denote the $n \times n$ matrix with the squared Euclidian distances between the $n$ data points $\mathbf{x}_i$ in the original $p$-dimensional space.

- Let $\mathbf{D}_{Zk}$ denote the $n \times n$ matrix with the squared Euclidian distances between the $n$ transformed data points $\mathbf{z}_{k,i}$ in the reduced $k$-dimensional space.

 Then, it can be shown that the truncated SVD also minimises
 \[
   \Vert \mathbf{D}_X - \mathbf{D}_{Zk} \Vert_F^2.
 \]

Or in other words: The n points $\mathbf{z}_{k,i}$ in the k-dimensional subspace spanned by the columns of $\mathbf{V}_k$ are the best approximation of the $n$ original points $\mathbf{x}_i$ in the original p-dimensional space in the sense that the Euclidean distances between the n original points are best approximated by the Euclidean distances between the n transformed points, among all possible k-dimensional linear subspaces.

---

- If $\mathbf{X}$ is known, we can readily obtain the k-dimensional projection by the SVD of $\mathbf{X}$

- If we only know the distance matrix $\mathbf{D}$

    1. We can readily obtain the Gram matrix
    \[-\frac{1}{2}\mathbf{H}\mathbf{D}_X\mathbf{H} = \mathbf{X}\mathbf{X}^T\]
    2. The truncated SVD of the squared and symmetric Gram matrix
    \[\mathbf{U}_k\boldsymbol{\Delta}_k^\prime\mathbf{U}_k^T\]
    3. from which also can obtain
    \[\mathbf{Z}_k=\mathbf{U}_k\boldsymbol{\Delta}_k,\] with $\boldsymbol{\Delta}_k= \mathbf{\Delta}_k^{\prime \frac{1}{2}}$

## Example

```{r}
X <- as.matrix(t(uk[,-1]))
n <- nrow(X)
H <- diag(n) - matrix(1/n,nrow=n,ncol=n)
X <- H%*%X
svdUk <- svd(X)

k <- 2
Uk <- svdUk$u[,1:k]
Dk <- diag(svdUk$d[1:k])
Zk <- Uk%*%Dk
rownames(Zk) <- colnames(uk)[-1]
colnames(Zk) <- paste0("Z",1:k)
Zk

Zk %>%
  as.data.frame %>%
  ggplot(aes(x=Z1,y=Z2,label=rownames(Zk))) +
  geom_point(size = 3) +
  geom_text(nudge_x = 50)
```

- The graph suggests that Wales and England are quite similar in terms of food consumption, and North Ireland seem to have a very different food consumption pattern.

- Also note that England is close to the origin, meaning that the food consumption pattern is close to the average pattern in the UK.

- A few questions remain:

    - How well can the 17-dimensional data be represented in a 2-dimensional subspace?
    - How can we interpret the distances (differences) between the data points in terms of the original 17 variables?

## The biplot

The biplot is a single 2-dimensional graph which displays the information in both $\mathbf{Z}_2=\mathbf{U}_2\boldsymbol{\Delta}_2$ and $\mathbf{V}_2$.

The plot of $\mathbf{Z}_2$ has been discussed previously (e.g. best approximation of distances).

The name "bi"plot refers to the plotting of two parts of the SVD ($\mathbf{Z}$ and $\mathbf{V}$) in a single graph.

From the geometrical interpretation of the SVD we know that
$\mathbf{Z}_2$ is the orthogonal projection of $\mathbf{X}$ on $\mathbf{V}_2$ the basis spanned by the first two singular values.
Thus for the $i^\text{th}$ individual we get
\[
\mathbf{z}_{2,i}= \mathbf{x}_i \left[\begin{array}{cc}\mathbf{v}_1&\mathbf{v}_2\end{array}\right]
\]

We also know that we can approximate $\mathbf{X}$ by $\mathbf{X}_2$ which equals:

\begin{align}
  \mathbf{X}_2 &= \mathbf{Z_2}\mathbf{V}_2^T\\
  &= \mathbf{Z_2}\left[\begin{array}{c} \mathbf{v}_1^T\\
      \mathbf{v}_2^T\end{array}\right]
  = \mathbf{Z_2}
    \left[
      \begin{array}{ccc}\tilde{\mathbf{v}}_{2,1} \ldots \tilde{\mathbf{v}}_{2,p}\end{array}
    \right]
\end{align}

with $\mathbf{v}_1$ and $\mathbf{v}_2$ the first two right singular vector and $\tilde{\mathbf{v}}_j$ the j$^\text{th}$ column of the matrix $\mathbf{V}_2^T$.

- This basically shows us that the orthogonal projection of $\mathbf{z}_{2,i}$ for subject/experimental unit i on $\tilde{\mathbf{v}}_{2,j}$ gives us the approximation of the value for  j$^\text{th}$ variable that was observed for the $i^\text{th}$ experimental unit, i.e. $x_{2,ij}$.
\[x_{2,ij}=\mathbf{z}_{2,i}^T\tilde{\mathbf{v}}_{2,j}\]

### UK  example

#### Biplot

```{r}
Vk <- svdUk$v[,1:k]
rownames(Vk) <- uk[[1]]
colnames(Vk) <- colnames(Zk)
scaleFactor <- mean(svdUk$d[1:k])

diyBiplot <- ggplot() +
  geom_point(
    data=Zk %>% as.data.frame,
    aes(x=Z1,y=Z2),
    size = 3) +
  geom_text(
    data=Zk %>% as.data.frame,
    aes(x=Z1,y=Z2,label=rownames(Zk)),
    nudge_x = 50) +
  geom_segment(
    data=Vk %>% as.data.frame,
    aes(x=0, y=0, xend=Z1*scaleFactor, yend=Z2*scaleFactor),
    arrow=arrow(length=unit(0.4,"cm")),
    alpha=0.25) +
  geom_text(
    data=Vk %>% as.data.frame,
    aes(x=Z1*scaleFactor, y=Z2*scaleFactor, label=rownames(Vk)),
    alpha=0.5,
    size=3)

diyBiplot

biplot(Zk,Vk)
```

The R code shows two ways of constructing a biplot: the first method starts from the SVD of $\mathbf{X}$ and plots the p vectors $\tilde{\mathbf{v}}_{2,j}$ as arrows. Note that we used a scaling factor to give the arrows a convenient length in the graph (not too small, not too large); it does not affect the interpretation. The second way of obtaining the biplot is simply by using the R built-in function biplot.

#### Illustration of projection

We project $\mathbf{z}_{2,i}$ for N.Ireland on Fresh_potatoes.

```{r echo=FALSE}
ggplot() +
  geom_point(
    data=Zk %>% as.data.frame,
    aes(x=Z1,y=Z2),
    size = 3) +
  geom_text(
    data=Zk %>% as.data.frame,
    aes(x=Z1,y=Z2,label=rownames(Zk)),
    nudge_x = 50) +
  geom_segment(
    data=rbind(Vk["Fresh_potatoes",]*scaleFactor,Zk["N.Ireland",]) %>% as.data.frame,
    aes(x=0, y=0, xend=Z1, yend=Z2),
    arrow=arrow(length=unit(0.4,"cm")),
    alpha=0.25) +
    geom_segment(
      aes(x=Zk["N.Ireland",1],
        y=Zk["N.Ireland",2],
        xend=Vk["Fresh_potatoes",]%*%Zk["N.Ireland",]*Vk["Fresh_potatoes",1],
        yend=Vk["Fresh_potatoes",]%*%Zk["N.Ireland",]*Vk["Fresh_potatoes",2]
        ),
        lty=2) +
    geom_text(
      aes(x=Vk["Fresh_potatoes",1]*scaleFactor, y=Vk["Fresh_potatoes",2]*scaleFactor, label="Fresh_potatoes"),
      alpha=0.5,
      size=3)
```


```{r}
rownames(X) <- rownames(Zk)
colnames(X) <- rownames(Vk)
X[,"Fresh_potatoes"]
Zk["N.Ireland",]%*%Vk["Fresh_potatoes",]
```

We observe that N. Ireland has a consumption in Fresh_potatoes that is `r X["N.Ireland","Fresh_potatoes"]` above the average and this is well approximated by the projection `r round(Zk["N.Ireland",]%*%Vk["Fresh_potatoes",],2)
`.

Also note that

- The origin corresponds to the sample average of the 17-dimensional observations in the data matrix

- England is close to the origin and thus one could say that the food consumption in England is as the average in the UK

- Projecting the $\mathbf{Z}_2$ coordinates for North Ireland orthogonally onto the vector $\tilde{\mathbf{v}}_{2,j}$ of fresh potatoes, we get a large and positive $x_{2,ij}$. Hence, people in N. Ireland tend to eat more fresh potatoes than on average in the UK.

- We can do the same for the other vectors.

### Further interpretation of the Biplot

From $\mathbf{x}_{k,i}^T =  \mathbf{z}_{k,i}^T\mathbf{V}_k^T$ we learnt that the original data can be (approximately) reconstructed from the $\mathbf{z}_{k,i}$.

 We now show how the dimensions of $\mathbf{Z}_k$ (columns or variables) can be interpreted. Remember,
 \[
  \mathbf{z}_{k,i}^T = \mathbf{x}_i^T\mathbf{V}_k = \left(\mathbf{x}_i^T \mathbf{v}_1 , \mathbf{x}_i^T \mathbf{v}_2 , \ldots , \mathbf{x}_i^T \mathbf{v}_k\right).
 \]

 The $j$th column of $\mathbf{Z}_k$ can be written as
 \[
   \mathbf{z}_{k,j} = \mathbf{X}\mathbf{v}_j,
 \]

 An individual element of $\mathbf{Z}_k$ can be written as
 \[
   z_{k,ij} = \mathbf{x}_i^T\mathbf{v}_j,
 \]

 which is a linear combination of the observations on the $p$ variables in $\mathbf{x}_i$, with coefficients given by the $p$ elements in $\mathbf{v}_j$. Understanding the elements in $\mathbf{v}_j$ will give us insight into the interpretation of the $j$th dimension of $\mathbf{Z}_k$.

## Illustration Uk food

```{r}
p1 <- ggplot() +
  geom_bar(
    aes( x=rownames(Vk), y=Vk[,1]),
    stat="identity") +
  xlab("V1") +
  ylab("contribution") +
  coord_flip()

p2 <-
ggplot() +
  geom_bar(
    aes( x=rownames(Vk), y=Vk[,2]),
    stat="identity") +
  xlab("V2") +
  ylab("contribution") +
  coord_flip()

grid.arrange(p1,p2,ncol=2)
```

In giving an interpretation to the elements in $\mathbf{v}_1$ and $\mathbf{v}_2$, we ignore the elements close to zero (this is a subjective decision; later we see more objective methods).

For $\mathbf{v}_1$ (1st dimension of $\mathbf{Z}_2$):

- contrast of soft drinks and fresh potatoes versus fresh fruit and alcoholic drinks

- a large value of $z_{i1}$ can result from eating many fresh potatoes and drinking a lot of soft drinks, but eating only few fresh fruit and drinking not much alcoholic drinks

- a small value of $z_{i1}$ can result from eating few fresh potatoes and drinking only few soft drinks, but eating a lot of fresh fruit and drinking much alcoholic drinks

For $\mathbf{v}_2$ (2nd dimension of $\mathbf{Z}_2$):

- contrast of soft drinks versus fresh potatoes

- a large value of $z_{i2}$ can result from eating many fresh potatoes, but drinking not much soft drinks

- a small value of $z_{i2}$ can result from eating few fresh potatoes, but drinking much soft drinks .

---

The elements in v1 and v2 are also shown in the biplot.

```{r}
grid.arrange(p1, p2, diyBiplot, ncol=2, layout_matrix = rbind(c(1,2),c(3,3)))
```

From the graph we see e.g. that N. Ireland has a high score for the first dimension. Now that we can give an interpretation to the dimension, we conclude that in Northern Ireland people eat relatively much fresh potatoes and drink many soft drinks, but they do not drink much alcoholic drinks and eat not much fresh fruit. We had already come to these findings by projecting N. Ireland on the vectors of these four food products. This comes to no surprise: both interpretations arise from the same data set and its SVD, and so no contradictory results should arise. Other conclusions can be found in a similar fashion.

We have derived the interpretation of the first dimension from the barplot of the elements of $\mathbf{v}_1$. However, there is actually no need to make a separate plot to read the elements of $\mathbf{v}_1$; they can also be read from projecting the vectors in the biplot onto the first dimension (i.e. basis vector of the first dimension). For example, fresh potatoes is in the 7th column of $\mathbf{X}$ and thus in the biplot its vector is $\tilde{\mathbf{v}}_{2,7}$; it is a 2-dimensional vector (2-dimensional biplot is shown). In the space of the biplot (i.e. the column space of $\mathbf{Z}_2$), the first basis vector is given by $(1,0)$. Projecting  $\tilde{\mathbf{v}}_{2,7}^T$ orthogonally onto $(1,0)$ gives
\[
  \tilde{\mathbf{v}}_{2,7}^T  \begin{pmatrix} 1 \\ 0 \end{pmatrix} = (v_{71} , v_{72}) \begin{pmatrix} 1 \\ 0 \end{pmatrix} = v_{71}
\]
which is the seventh element of the first right-singular vector of $\mathbf{X}$, which is thus the bar of fresh potatoes in the barplot of the first dimension.

# SVD and principal component analysis (PCA)

- PCA is basically a SVD

- PCA adds another layer of interpretation

- PCA comes with its own terminology

- One of the most widely used algorithms for dimension reduction and data exploration of multivariate and high dimensional data.

- It is motivated from the decomposition of the Variance covariance matrix of the data

## Variance covariance matrix

-   For a given centered data matrix $\mathbf{X}$ the $p\times p$ covariance matrix can be estimated by
  \[
    \boldsymbol{\Sigma}_X = \frac{1}{n-1}\mathbf{X}^T\mathbf{X} =\frac{1}{n-1}\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T,
  \]
  i.e. the $(i,j)$th element is given by (column means are zero)
  \[
    \frac{1}{n-1}\sum_{m=1}^n x_{mi}x_{mj}=\frac{1}{n-1}\sum_{m=1}^n (x_{mi}-\bar{x}_i)(x_{mj}-\bar{x}_j).
  \]

Note, that when we forget to write the factor $1/(n-1)$ all the derivations still hold. It is only a proportionality factor and it does not affect the interpretation.

---

## Conventional derivation of PCA

PCA is usually introduced as follows.

Let
\[
  y_i=\mathbf{x}_i^T\mathbf{a}
\]
with $\mathbf{a}$ a $p$-vector of constants. Hence $y_i$ is a linear combination (or transformation) of $\mathbf{x}_i$.

PCA aims at finding $\mathbf{a}$ such that the sample variance among the $n$ $y_i$'s is maximal, with
\begin{eqnarray*}
  \frac{1}{n-1}\sum_{i=1}^n (y_i - \bar{y})^2 =  \frac{1}{n-1}\sum_{i=1}^n y_i ^2 \\
    &=& \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i^T\mathbf{a})^2 = \frac{1}{n-1} \mathbf{a}^T \left(\sum_{i=1}^n \mathbf{x}_i\mathbf{x}_i^T\right) \mathbf{a} \\
    &=& \frac{1}{n-1} \mathbf{a}^T\mathbf{X}^T\mathbf{X}\mathbf{a} = \mathbf{a}^T\boldsymbol{\Sigma}_X\mathbf{a}
\end{eqnarray*}
in which $\boldsymbol{\Sigma}_X= \frac{1}{n-1}\mathbf{X}^T\mathbf{X}$ is the sample covariance matrix of $\mathbf{X}$.

### Problem for optimisation

Finding $\mathbf{a}$ that maximises
\[
  \text{var}[y]  = \mathbf{a}^T\boldsymbol{\Sigma}_X\mathbf{a}
\]
has a trivial solution (set all elements of $\mathbf{a}$ equal to $\infty$). To avoid the trivial solution, the solution must satisfy a restriction, e.g.
\[
  \Vert \mathbf{a}\Vert^2 = \mathbf{a}^T\mathbf{a}=1.
\]

The solution of the constrained maximisation problem may be formulated as
\[
  \mathbf{a} = \text{ArgMax}_{b:\Vert b\Vert^2=1} \mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}.
\]
By introducing a Lagrange multiplier $\lambda$, we get an unconstrained maximisation problem,
\[
  \mathbf{a} = \text{ArgMax}_{b} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right).
\]

---

Note that,

- If the constraint is linear, then the constrained optimisation problem (here: maximisation) may be replaced by an unconstrained optimisation problem of the same criterion but with an ``penalty term'' added. This method is due to Lagrange. The penalty term vanishes when the constraint is satisfied.

- If the constraint is satisfied, $\mathbf{b}^T\mathbf{b}=\Vert \mathbf{b}\Vert^2=1$ and thus $\mathbf{b}^T\mathbf{b}-1=0$ and the ``penalty'' term in the unconstrained criterion vanishes.

- The Lagrange multiplier $\lambda$ is to be considered as an extra parameter that may have to be estimated from the data.

---

The solution of
\[
  \mathbf{a} = \text{ArgMax}_{b} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right)
\]
is obtained by differentiating $\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)$ w.r.t. $\mathbf{b}$, equating it to zero and solving for $\mathbf{b}$.
\begin{eqnarray*}
 \frac{\partial}{\partial \mathbf{b}} \left(\mathbf{b}^T\boldsymbol{\Sigma}_X\mathbf{b}-\lambda(\mathbf{b}^T\mathbf{b}-1)\right)
    &=& 0 \\
    2\boldsymbol{\Sigma}_X\mathbf{b} -2\lambda\mathbf{b}
    &=& 0.
\end{eqnarray*}

Hence, we need the solution of
\[
  \boldsymbol{\Sigma}_X\mathbf{b} = \lambda\mathbf{b}.
\]

This equation has $r$ solutions:

-  $\mathbf{b}=\mathbf{e}_j$: the $j$th eigenvector of $\boldsymbol{\Sigma}_X$

-  $\lambda=\lambda_j$: the $j$th eigenvalue of $\boldsymbol{\Sigma}_X$.

The eigenvectors are orthonormal, i.e.
$\mathbf{e}_i^T\mathbf{e}_j=1$ if $i=j$ and $\mathbf{e}_i^T\mathbf{e}_j=0$ otherwise.

---

Consider the following calculations, with $\mathbf{b}=\mathbf{e}_j$, the $j$the eigenvector of $\boldsymbol{\Sigma}_X$.
\begin{eqnarray*}
  \text{var}[y]
    &=& \mathbf{e}_j^T\boldsymbol{\Sigma}_X\mathbf{e}_j \\
    &=& \mathbf{e}_j^T(\boldsymbol{\Sigma}_X\mathbf{e}_j) \\
    &=& \mathbf{e}_j^T \left(\lambda_j \mathbf{e}_j\right) \\
    &=& \lambda_j\mathbf{e}_j^T\mathbf{e}_j \\
    &=& \lambda_j.
\end{eqnarray*}
By convention the eigenvectors/eigenvalues are ordered so that
\[
  \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_r.
\]
Hence, the first eigenvector $\mathbf{e}_1$ gives the largest variance $\lambda_1$. Hence, this is the solution we were looking for.

---

We now switch notation and from now onwards we denote $y$ by $z$.

The observations on the first principal component (PC) are then given by
\[
    z_{i1} = \mathbf{x}_i^T\mathbf{e}_1.
\]

In PCA terminology they are referred to as the **scores** of the first PC. The elements in the eigenvector $\mathbf{e}_i$ that make up the transformation are known as the **loadings**  of the first PC.

The first PC is thus a new variable (construct) that has the largest variance among all linear transformations of the original variables.

Variability between observations is considered as informative to understand differences between the observations.

---

The equation
\[
  \boldsymbol{\Sigma}_X\mathbf{b} = \lambda\mathbf{b}
\]
has $r=\text{rank}(\boldsymbol{\Sigma}_X)$ solutions. The eigenvectors are orthonormal, i.e.
\[
  \forall i\neq j: \mathbf{e}_i^T \mathbf{e}_j = 0 \text{ and } i=j: \mathbf{e}_i^T \mathbf{e}_j = 1
\]
Hence (for $i\neq j$)
\[
  \text{cov}\left[\mathbf{x}^T\mathbf{e}_i,\mathbf{x}^T\mathbf{e}_j\right] = \mathbf{e}_i^T\text{var}[\mathbf{x}]\mathbf{e}_j = \mathbf{e}_i^T\boldsymbol{\Sigma}_X\mathbf{e}_j = \mathbf{e}_i^T\left(\lambda_j\mathbf{e}_j\right) = \lambda_j \mathbf{e}_i^T\mathbf{e}_j = 0
\]

If $z_{j}=\mathbf{x}_i^T\mathbf{e}_j$ denotes the $j$th PC, then
\[
  \text{cov}\left[z_i,z_j\right]=\text{cov}\left[\mathbf{x}^T\mathbf{e}_i,\mathbf{x}^T\mathbf{e}_j\right] =0 \text{ if } i\neq j
\]
\[
  \text{var}\left[z_j\right] = \lambda_j .
\]
We say that the $j$th PC maximises the variance among all linear transformations such that it is uncorrelated with the previous PCs.

---

### Interpretation of PCA

A PCA is a transformation of the original $p$ variables to $r$ PCs such that

- the first PC has largest variance, equal to first eigenvalue of $\boldsymbol{\Sigma}_X$

-  the next PCs have decreasing variances (decreasing information content)

- all PCs are mutually uncorrelated (no information-overlap).

---

## Link with SVD

If we write the eigen value decomposition in matrix form:
\[
\boldsymbol{\Sigma}_X\mathbf{
  B} = \mathbf{B}\boldsymbol{\Lambda}.
\]

with $\boldsymbol{\Lambda}$ is a diagonal matrix with diagonal elements $[\lambda_i]_{ii}$.

We recognise an expression that we have seen when discussing the link between the SVD and the sample covariance matrix

\begin{eqnarray}
\mathbf{X}^T\mathbf{X}\mathbf{V} &=&\mathbf{V}\boldsymbol{\Delta}^2
\end{eqnarray}

- So, the SVD can be used to solve the spectral decomposition (eigen value eigen vector) problem and the eigen vectors coincide with the right singular vectors and the eigen values are the singular values squared!

## Conservation of variance

- The total variance in a data set $\mathbf{X}$ is given by the sum of the variances of the variables,
  \[
    \sigma^2_{\text{tot}} = \sum_{j=1}^p \text{var}\left[X_j\right] = \text{trace}(\boldsymbol{\Sigma}_X)
  \]


-  For a symmetric matrix it holds that
  \[
    \text{trace}(\boldsymbol{\Sigma}_X) = \sum_{j=1}^r \lambda_r.
  \]

-  Since $\lambda_r=\text{var}\left[Z_j\right]$, we find
  \[
    \sigma^2_{\text{tot}} = \sum_{j=1}^p \text{var}\left[X_j\right]= \sum_{j=1}^r \text{var}\left[Z_j\right].
  \]

$\longrightarrow$   Thus no information is lost by the PCA transformation from the original $p$-dimensional space to the $r$-dimensional PCA space.

---

## Choosing the number of dimensions

For PCA the reasoning is usually based on the relative variance of a PC,
\[
   \frac{\text{var}\left[Z_j\right]}{\sigma^2_{\text{tot}}} = \frac{\lambda_j}{\sum_{i=1}^r \lambda_i} =\frac{\delta_j^2}{\sum_{i=1}^r \delta_i^2} .
\]
If the first $k$ PCs are selected for further use, then they represent
\[
  100 \times  \frac{\sum_{j=1}^k \lambda_j}{\sum_{i=1}^r \lambda_i} \%
  = 100 \times  \frac{\sum_{j=1}^k \delta^2_j}{\sum_{i=1}^r \delta_i^2} \%
\]
of the total variance (or information) of the original data set $\mathbf{X}$.

### UK example

A scree plot is often used to look at the eigenvalues. Here it is shown for the UK consumption data.

```{r}
n <- nrow(X)
r <- ncol(svdUk$v)
totVar <- sum(svdUk$d^2)/(n-1)
vars <- data.frame(comp=1:4,var=svdUk$d^2/(n-1)) %>%
  mutate(propVar=var/totVar,cumVar=cumsum(var/totVar))

pVar1 <- vars %>%
  ggplot(aes(x=comp:r,y=var)) +
  geom_point() +
  geom_line() +
  xlab("Component") +
  ylab("Variance")

pVar2 <- vars %>%
  ggplot(aes(x=comp:r,y=propVar)) +
  geom_point() +
  geom_line() +
  xlab("Component") +
  ylab("Proportion of Total Variance")

pVar3 <- vars %>%
  ggplot(aes(x=comp:r,y=cumVar)) +
  geom_point() +
  geom_line() +
  xlab("Component") +
  ylab("Cum. prop. of tot. var.")

grid.arrange(pVar1, pVar2, pVar3, nrow=1)
```

From these graphs we may conclude that with using k = 2 dimensions, more than 95% of the total variance is retained.

---

### Choosing the number of dimensions SVD

More generally, scree plots are also informative for the SVD.


  The motivation comes from
  \[
    \text{Min}_{A: \text{rank}(A)=k} \Vert \mathbf{X} - \mathbf{A}\Vert_F^2 = \Vert \mathbf{X} - \mathbf{X}_k\Vert_F^2 = \sum_{j=k+1}^r\delta_{j}^2 = \sum_{j=k+1}^r\lambda_{j}.
  \]
  This is known as the **approximation error** of the matrix $\mathbf{X}_k$.

  Hence,
  \[
    \frac{\sum_{j=1}^k\delta_j^2}{\sum_{j=1}^r \delta_j^2}  =\frac{\sum_{j=1}^k\lambda_{j}}{\sum_{j=1}^r\lambda_{j}}
  \]
  still makes sense as a relative quality measure for the SVD in general (including matrix approximation and MDS).

### Rules of thumb to select number of dimensions

Select k such that

- at least 80% of the total variance is retained in the PC space, or, equivalently, at
most 20% relative approximation error

- adding more dimensions does not add more information (in a relative sense); this can often be visually detected as the “knee” or “elbow” in the scree plot

- none of the dimensions has a variance smaller than the variance of one of the p original variables (this rule is typically only used when the variables are first standardised - see later).

## Covariance vs Correlation Matrix

### Direction of largest variability

 Now that we know how the PCs are constructed, we can give an additional interpretation to the first eigenvector / right-singular vector of $\boldsymbol{\Sigma}_X$.

- The first eigenvector of $\boldsymbol{\Sigma}_X$ is the direction in the original $p$-dimensional space of the largest variability.

- The second eigenvector of $\boldsymbol{\Sigma}_X$ is the direction in the original $p$-dimensional space of the second largest variability, among all directions orthogonal to the first eigenvector

#### Iris example

The iris dataset in R contains information on leaves of iris flowers from different species.
Here, we will focus on the setosa species and on the sepal length and sepal width.

```{r}
irisSetosa <- iris %>%
  filter(Species == "setosa") %>%
  dplyr::select("Sepal.Length","Sepal.Width")

nIris <- nrow(irisSetosa)
hIris <- diag(nIris) - matrix(1/nIris, nIris, nIris)
irisX <- irisSetosa %>%
  as.matrix
irisX <- hIris %*% irisX

irisSvd <- svd(irisX)

pIris <- irisX %>%
  as.data.frame %>%
  ggplot(aes(x=Sepal.Length,y=Sepal.Width)) +
  geom_point()

pIris <- pIris +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = -irisSvd$v[1,1]*irisSvd$d[1]/sqrt(nIris-1),
      yend = -irisSvd$v[2,1]*irisSvd$d[1]/sqrt(nIris-1)
      ),
      arrow = arrow(length=unit(0.4,"cm"))
    ) +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd$v[1,2]*irisSvd$d[2]/sqrt(nIris-1),
      yend = irisSvd$v[2,2]*irisSvd$d[2]/sqrt(nIris-1)
      ),
    arrow = arrow(length=unit(0.4,"cm"))
    ) +
  geom_text(
    aes(
      x = -irisSvd$v[1,1]*irisSvd$d[1]/sqrt(nIris-1)*1.2,
      y = -irisSvd$v[2,1]*irisSvd$d[1]/sqrt(nIris-1)*1.2,
      label="e1"
      )
    ) +
  geom_text(
    aes(
      x = irisSvd$v[1,2]*irisSvd$d[2]/sqrt(nIris-1)*1.2,
      y = irisSvd$v[2,2]*irisSvd$d[2]/sqrt(nIris-1)*1.2,
      label="e2"
      )
    ) +
  xlim(-1, 1) +
  ylim(-1,1)

pIris
```

### Covariance versus correlation matrix
So far we have worked with

- the column-centered data matrix $\mathbf{X}$

- the SVD of $\mathbf{X}$ and $\boldsymbol{\Sigma}_X\propto \mathbf{X}^T\mathbf{X}$ (the covariance matrix).

In some situations it is better to start from the standardised variables:

- substract from each element in $\mathbf{X}$ the  column-mean
- divide each centered element in $\mathbf{X}$ by the column-specific standard deviation

Thus each element $x_{ij}$ is replaced with
  \[
     \frac{x_{ij}-\bar{x}_j}{s_j} ,
  \]
with $\bar{x}_j$ and $s_j$ the column mean and column standard deviation.

-   In matrix notation, the standardisation, starting from the centered matrix $\mathbf{X}$ is computed as
  \[
      \mathbf{X}\mathbf{S}^{\prime-1}
  \]
  where $\mathbf{S}^\prime$ is a diagonal matrix with the column-specific standard deviations.

---

#### Iris Example
```{r}
var(irisX)
Sprime <- diag(sqrt(diag(var(irisX))))
Xs <- irisX%*%solve(Sprime)
var(Xs)
```

Much faster to use the scale function. By default center=TRUE and scale=TRUE.

```{r}
Xs <- irisSetosa %>% scale
var(Xs)
```

Show problem of different units:

- Sepal length in cm
- Sepal width  in mm

```{r}
irisX2 <- irisX
irisX2[,2] <- irisX2[,2]*10

pIris2 <- irisX2 %>%
  as.data.frame %>%
  ggplot(aes(x=Sepal.Length,y=Sepal.Width)) +
  geom_point()

irisSvd2 <- svd(irisX2)

pIris2 <- pIris2 +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd2$v[1,1]*irisSvd2$d[1]/sqrt(nIris-1),
      yend = irisSvd2$v[2,1]*irisSvd2$d[1]/sqrt(nIris-1)
      ),
      arrow = arrow(length=unit(0.4,"cm"))
    ) +
  geom_segment(
    aes(
      x = 0,
      y = 0,
      xend = irisSvd2$v[1,2]*irisSvd2$d[2]/sqrt(nIris-1),
      yend = irisSvd2$v[2,2]*irisSvd2$d[2]/sqrt(nIris-1)
      ),
    arrow = arrow(length=unit(0.4,"cm"))
    ) +
  geom_text(
    aes(
      x = irisSvd2$v[1,1]*irisSvd2$d[1]/sqrt(nIris-1)*1.2,
      y = irisSvd2$v[2,1]*irisSvd2$d[1]/sqrt(nIris-1)*1.2,
      label="e1"
      )
    ) +
  geom_text(
    aes(
      x = irisSvd2$v[1,2]*irisSvd2$d[2]/sqrt(nIris-1)*1.2,
      y = irisSvd2$v[2,2]*irisSvd2$d[2]/sqrt(nIris-1)*1.2,
      label="e2"
      )
    ) +
  xlim(-10, 10) +
  ylim(-10,10)

grid.arrange(pIris, pIris2)
```

Top: original matrix -- Bottom: second column of $\mathbf{X}$ multiplied by 10 (e.g. moving from cm to mm).

Directions of maximal variability are affected by the units of the variables.

---

### Recommendations

When to use correlation and when covariance?

- use correlation when columns of $\mathbf{X}$ are expressed in different units

- use covariance when columns of $\mathbf{X}$ are expressed in the same units.

There may be exceptions.

---

## PCA and the Multivariate Normal Distribution

The density function of a multivariate normal distribution (MVN) is given by
\[
  f(\mathbf{x}) = (2\pi)^{-p/2}\vert \boldsymbol{\Sigma}\vert^{-1/2} \exp\left( -\frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})\right) ,
\]
where

- $\boldsymbol{\mu}$ is the multivariate mean vector ($p$-dimensional). The $j$th element is $\mu_j = \text{E}\left[X_j\right]$

- $\boldsymbol{\Sigma}$ is the $p \times p$ covariance matrix. The $(i,j)$th element is $\sigma_{ij}=\text{cov}\left[X_i, X_j\right]$.

---

To get a better understanding of the MVN we focus on the exponential which has the factor
\[
    (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})
\]
This factor

- is the only factor in the density function that depends on $\mathbf{x}$

- is a $\mathbf{quadratic}$ form

- is constant in points $\mathbf{x}$ with constant density $f(\mathbf{x})$.

---

Consider $p=2$ (bivariate normal). Then, all $\mathbf{x} \in \mathbb{R}^2$ for which
\[
    (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) = \text{constant } c^2
\]
lie on an ellipse with center $\boldsymbol{\mu}$.

These ellipses are known as **constant density ellipses**.

### iris example

```{r}
pIris +
  stat_ellipse() +
  stat_ellipse(level=.68) +
  stat_ellipse(level=.1)
```

---

Now plug in the SVD of $\boldsymbol{\Sigma}=\mathbf{VDV}^T$,
\begin{eqnarray*}
   (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) &=& c^2\\
   (\mathbf{x}-\boldsymbol{\mu})^T \mathbf{VD}^{-1}\mathbf{V}^T (\mathbf{x}-\boldsymbol{\mu}) &=& c^2
\end{eqnarray*}

Note that $\mathbf{x}-\boldsymbol{\mu}$ is the centered $\mathbf{x}$. Without loss of generality, take $\boldsymbol{\mu}=\mathbf{0}$.
Hence,
\begin{eqnarray*}
   \mathbf{x}^T \mathbf{VD}^{-1}\mathbf{V}^T \mathbf{x} &=& c^2    \\
   \left(\mathbf{x}^T\mathbf{V}\right) \mathbf{D}^{-1} \left(\mathbf{x}^T\mathbf{V}\right)^T &=& c^2 \\
   \sum_{j=1}^p (\mathbf{x}^T\mathbf{v}_j)^2  / \delta_j &=& c^2\\
   \sum_{j=1}^p (z_j)^2  / (c^2\delta_j) &=& 1.
\end{eqnarray*}
The last equation is the equation of an ellipse with axes parallel to the basis of $(z_1,\ldots, z_p)$ and with half axis lengths $c\sqrt{\lambda_j}=c\delta_j$ with $\lambda_j$ the $j$th eigenvalue of $\boldsymbol{\Sigma}$.

---

```{r}
pIris +
  stat_ellipse() +
  stat_ellipse(level=.68) +
  stat_ellipse(level=.1)
```


This graphs shows $n=50$ data points on $p=2$ dimensions.  The two ellipses are constant density ellipses for three different values of $c$ (i.e. three different constant densities). The inner ellipse corresponds to the largest constant density and the outer to the smallest constant density. The arrows show the two eigenvectors /  singular vectors and they are scaled according to the sqrt of the eigen values and they form the axes of the constant density ellipses. It is clear that the first axis is pointing to the larger variance.

## Biplot

-  Because of the close connection between PCA and the SVD, the biplot as discussed before is still meaningful, with the first axis pointing into the direction of largest variance.

## Ovarian Cancer Example

The ovarian cancer data set consists of proteomics data for 216 patients, 121 of whom have ovarian cancer, and 95 of whom do not. For each subject, the expression of 4000 spectral features is assessed.
The first 121 rows consist of data for the cancer patients.

### Importing the data

```{r}
ovarian <- read_csv(
  "https://raw.githubusercontent.com/statOmics/HDA2020/data/ovarian.csv",
  col_names = FALSE,
  col_types = cols()
)
grid.arrange(
  qplot(1:4000,
    ovarian[1,] %>% unlist,
    geom="line",
    ylab="centered intensity",
    xlab="",main="cancer",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[2,] %>% unlist,
    geom="line",
    ylab="centered intensity",
    xlab="",main="cancer",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,] %>% unlist,
    geom="line",
    ylab="centered intensity",
    xlab="",main="Normal",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,] %>% unlist,
    geom="line",
    ylab="centered intensity",
    xlab="",main="Normal",
    ylim=range(ovarian[c(1,2,200,201),])),
  ncol=1
  )
```

Centering

```{r}
ovarian <-  scale(ovarian, scale=FALSE)
grid.arrange(
  qplot(1:4000,
    ovarian[1,],
    geom="line",
    ylab="centered intensity",
    xlab="",main="cancer",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[2,],
    geom="line",
    ylab="centered intensity",
    xlab="",main="cancer",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[200,],
    geom="line",
    ylab="centered intensity",
    xlab="",main="Normal",
    ylim=range(ovarian[c(1,2,200,201),])),
  qplot(1:4000,
    ovarian[201,],
    geom="line",
    ylab="centered intensity",
    xlab="",main="Normal",
    ylim=range(ovarian[c(1,2,200,201),])),
  ncol=1
  )
```


### SVD Analysis

```{r}
svdOvarian <- svd(ovarian)

nOvarian <- nrow(ovarian)
r <- ncol(svdOvarian$v)

totVar <- sum(svdOvarian$d^2)/(nOvarian-1)
vars <- data.frame(comp=1:r,var=svdOvarian$d^2/(nOvarian-1)) %>%
  mutate(propVar=var/totVar,cumVar=cumsum(var/totVar))

pVar2 <- vars %>%
  ggplot(aes(x=comp:r,y=propVar)) +
  geom_point() +
  geom_line() +
  xlab("Component") +
  ylab("Proportion of Total Variance")

pVar3 <- vars %>%
  ggplot(aes(x=comp:r,y=cumVar)) +
  geom_point() +
  geom_line() +
  xlab("Component") +
  ylab("Cum. prop. of tot. var.")

grid.arrange(pVar2, pVar3, nrow=1)
```

We see that we can explain a lot of the variability using a few PC's!

```{r}
Zk <- svdOvarian$u[,1:6]%*%diag(svdOvarian$d[1:6])
colnames(Zk) <- paste0("Z",1:6)
Vk <- svdOvarian$v[,1:6]
colnames(Vk) <- paste0("V",1:6)
reduced <- data.frame(Zk,cancer=c(rep(1,121),rep(2,95)))

pOv1 <- reduced %>%
  ggplot(aes(x=Z1,y=Z2,col=cancer)) +
  geom_point()  +
  theme(legend.position = "none")

pOv2 <- Vk %>%
  as.data.frame %>%
  ggplot(aes(x=1:4000,y=V1)) +
  geom_line() +
  xlab("")

pOv3 <- Vk %>%
  as.data.frame %>%
  ggplot(aes(x=1:4000,y=V2)) +
  geom_line() +
  xlab("")

grid.arrange(pOv1,pOv2,pOv3,layout_matrix = rbind(c(1,3),c(2,NA)))
```


Reconstruction of profiles:

```{r}
i <- 1
pOv4 <- qplot(1:4000,
  ovarian[i,],
  geom="line",
  ylab="centered intensity",
  xlab="",main="Cancer",
  ylim=range(ovarian[c(1,2,200,201),]))

pOv5 <- qplot(1:4000,
  Vk[,1:2] %*%Zk[i,1:2],
  geom="line",
  ylab="centered intensity",
  xlab="",main="Reconstructed",
  ylim=range(ovarian[c(1,2,200,201),]))



grid.arrange(
  pOv1 +
    annotate("point", x = reduced[i,1], y = reduced[i,2], colour = "red",cex=3),
  pOv2,
  pOv3,
  pOv4,
  pOv5,
  layout_matrix = rbind(c(1,2,3),c(4,5,NA)))
```



```{r}
i <- 3
pOv4 <- qplot(1:4000,
  ovarian[i,],
  geom="line",
  ylab="centered intensity",
  xlab="",main="Cancer",
  ylim=range(ovarian[c(1,2,200,201),]))

pOv5 <- qplot(1:4000,
  Vk[,1:2] %*%Zk[i,1:2],
  geom="line",
  ylab="centered intensity",
  xlab="",main="Reconstructed",
  ylim=range(ovarian[c(1,2,200,201),]))



grid.arrange(
  pOv1 +
    annotate("point", x = reduced[i,1], y = reduced[i,2], colour = "red",cex=3),
  pOv2,
  pOv3,
  pOv4,
  pOv5,
  layout_matrix = rbind(c(1,2,3),c(4,5,NA)))
```

# Acknowledgement {-}

- Olivier Thas for sharing his materials of Analysis of High Dimensional Data 2019-2020, which I used as the starting point for this chapter.

# Session info {-}

<details><summary>Session info</summary>

```{r session_info, echo=FALSE, cache=FALSE}
Sys.time()
sessioninfo::session_info()
```

</details>
</div>
<div class="footer">
    <hr>
    This work is licensed under the <a href= "https://creativecommons.org/licenses/by-nc-sa/4.0">
    CC BY-NC-SA 4.0</a> licence.
</div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("svd.Rmd");
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
